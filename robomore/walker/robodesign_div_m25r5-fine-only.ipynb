{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTWalker import GPTWalkerEnv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        api_key = \"<api_key>\"\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = \"gpt-4-turbo\"\n",
    "        \n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTWalker_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums                                                                                                                                                                                                                                                                                   \n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_walker_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = walker_design(parameter)  \n",
    "            filename = f\"GPTWalker_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_walker_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistwalker\", \"content\": json.dumps(initial_parameter)})\n",
    "\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "\n",
    "        xml_file = walker_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTWalker_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_walker_volume(diverse_parameter['parameters'])) \n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistwalker\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = walker_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTWalker_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\":reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, rewardfunc_index, morphology_index, iteration):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\"best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\" \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        # print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = walker_design(parameter)  \n",
    "        filename = f\"GPTWalker_refine2_{rewardfunc_index}_{morphology_index}_{iteration}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"parameters_refine_only.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 51\n",
    "rewardfunc_nums = 11\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewardfunc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extracting all the parameters from the provided log and storing them in a list\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrewardfunc_list\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewardfunc_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Extracting all the parameters from the provided log and storing them in a list\n",
    "\n",
    "rewardfunc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter coarse optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "morphology_list = [f'results/Div_m25_r5/assets/GPTWalker_{i}.xml' for i in range(0,51) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,11)]\n",
    "\n",
    "parameter_list =np.array([[ 1.  ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 , 0.025,  0.01 ],[1.5, 1.0, 0.6, 0.2, 0.3, -0.4, 0.05, 0.04, 0.03, 0.02], [1.0, 0.7, 0.5, 0.2, 0.25, -0.15, 0.045, 0.035, 0.025, 0.015], [1.8, 1.2, 0.9, 0.3, 0.35, -0.25, 0.06, 0.055, 0.045, 0.025], [1.0, 0.55, 0.35, 0.1, 0.1, -0.2, 0.03, 0.06, 0.07, 0.04], [1.3, 0.9, 0.6, 0.2, 0.25, -0.2, 0.04, 0.05, 0.05, 0.03], [1.2, 0.6, 0.3, 0.15, 0.2, -0.05, 0.065, 0.045, 0.03, 0.025], [0.8, 0.5, 0.2, 0.05, 0.1, -0.15, 0.04, 0.02, 0.01, 0.025], [1.1, 0.85, 0.55, 0.25, 0.3, -0.25, 0.035, 0.07, 0.08, 0.04], [2.0, 1.0, 0.5, 0.2, 0.25, -0.3, 0.02, 0.03, 0.04, 0.035], [1.2, 0.9, 0.4, 0.1, 0.15, -0.05, 0.025, 0.035, 0.045, 0.02], [1.0, 0.45, 0.25, 0.05, -0.05, -0.3, 0.02, 0.025, 0.03, 0.015], [0.9, 0.6, 0.4, 0.2, 0.25, -0.2, 0.02, 0.04, 0.06, 0.03], [0.6, 0.3, 0.15, 0.05, 0.1, -0.1, 0.04, 0.05, 0.06, 0.02], [0.5, 0.35, 0.2, 0.05, 0.15, -0.15, 0.035, 0.06, 0.075, 0.04], [0.8, 0.5, 0.3, 0.1, 0.2, -0.3, 0.04, 0.08, 0.12, 0.06], [0.6, 0.45, 0.15, 0.05, 0.1, -0.2, 0.025, 0.04, 0.055, 0.03], [1.0, 0.2, 0.1, 0.05, 0.05, -0.4, 0.03, 0.07, 0.1, 0.05], [0.8, 0.55, 0.25, 0.1, -0.05, -0.2, 0.045, 0.1, 0.15, 0.06], [0.6, 0.4, 0.3, 0.1, 0.2, -0.25, 0.055, 0.04, 0.03, 0.025], [1.2, 0.8, 0.4, 0.2, -0.1, -0.25, 0.03, 0.04, 0.06, 0.02], [1.0, 0.6, 0.4, 0.1, 0.2, -0.1, 0.02, 0.04, 0.05, 0.03], [1.5, 1.0, 0.7, 0.3, 0.4, -0.2, 0.05, 0.06, 0.07, 0.04], [1.0, 0.6, 0.25, 0.1, 0.15, -0.25, 0.03, 0.035, 0.04, 0.02], [1.2, 0.6, 0.3, 0.1, 0.12, -0.08, 0.025, 0.045, 0.065, 0.05], [1.7, 0.9, 0.45, 0.15, 0.25, -0.3, 0.025, 0.05, 0.08, 0.035], [1.5, 1.1, 0.6, 0.2, 0.35, -0.25, 0.02, 0.03, 0.04, 0.02], [1.2, 0.3, 0.15, 0.05, 0.1, -0.15, 0.02, 0.07, 0.12, 0.06], [1.0, 0.8, 0.5, 0.2, 0.25, -0.1, 0.025, 0.05, 0.1, 0.05], [1.0, 0.7, 0.4, 0.1, 0.2, -0.2, 0.03, 0.03, 0.06, 0.04], [0.9, 0.5, 0.25, 0.05, 0.1, -0.5, 0.03, 0.035, 0.07, 0.065], [1.8, 1.3, 0.6, 0.15, 0.25, -0.2, 0.025, 0.07, 0.12, 0.05], [1.0, 0.7, 0.5, 0.1, 0.2, -0.1, 0.01, 0.04, 0.08, 0.02], [1.0, 0.9, 0.6, 0.1, 0.2, -0.2, 0.02, 0.03, 0.05, 0.025], [1.2, 0.6, 0.3, 0.1, 0.25, -0.2, 0.02, 0.03, 0.08, 0.05], [0.8, 0.5, 0.3, 0.1, 0.15, -0.15, 0.015, 0.025, 0.035, 0.02], [1.0, 0.6, 0.35, 0.05, 0.1, -0.3, 0.02, 0.05, 0.1, 0.06], [1.0, 0.7, 0.3, 0.1, 0.15, -0.2, 0.025, 0.06, 0.12, 0.04], [1.2, 0.4, 0.2, 0.1, 0.2, -0.6, 0.02, 0.07, 0.15, 0.045], [0.8, 0.4, 0.2, 0.05, 0.1, -0.3, 0.015, 0.04, 0.1, 0.06], [0.7, 0.2, 0.05, 0.01, 0.2, -0.3, 0.01, 0.03, 0.08, 0.06], [1.0, 0.3, 0.15, 0.05, 0.1, -0.2, 0.02, 0.08, 0.15, 0.025], [1.5, 0.9, 0.6, 0.2, 0.25, -0.15, 0.018, 0.02, 0.025, 0.015], [0.85, 0.5, 0.25, 0.05, 0.1, -0.25, 0.015, 0.035, 0.05, 0.03], [1.0, 0.8, 0.5, 0.15, 0.2, -0.25, 0.02, 0.03, 0.035, 0.015], [1.0, 0.5, 0.3, 0.15, 0.3, -0.25, 0.04, 0.03, 0.02, 0.015], [1.2, 0.8, 0.4, 0.15, 0.3, -0.4, 0.02, 0.03, 0.04, 0.035], [0.9, 0.6, 0.25, 0.05, 0.1, -0.2, 0.02, 0.01, 0.03, 0.015], [1.0, 0.5, 0.25, 0.05, 0.25, -0.25, 0.015, 0.02, 0.025, 0.01], [1.4, 0.9, 0.6, 0.2, 0.3, -0.15, 0.025, 0.015, 0.015, 0.01], [1.45, 1.05, 0.6, 0.05, 0.1, 0, 0.2, 0.05,0.04,0.06]])\n",
    "\n",
    "material_list = [compute_walker_volume(parameter) for parameter in parameter_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 results/Div_m25_r5/env/GPTrewardfunc_10.py\n",
      "50 results/Div_m25_r5/assets/GPTWalker_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i, rewardfunc in enumerate(rewardfunc_list):\n",
    "    for j, morphology in enumerate(morphology_list):\n",
    "        # if i not in [10]:\n",
    "        #     continue\n",
    "        if j not in [50] or i not in [10]:\n",
    "            continue\n",
    "\n",
    "        print(i, rewardfunc)\n",
    "        print(j, morphology)\n",
    "        shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "        shutil.copy(rewardfunc, \"GPTrewardfunc.py\")         \n",
    "\n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTWalkerEnv._get_rew = _get_rew\n",
    "\n",
    "        # model_path = Train(j,  i, folder_name, total_timesteps=5e5)\n",
    "        model_path = \"results/Div_m25_r5/coarse/SAC_morphology50_rewardfunc0_500000.0steps\"\n",
    "        fitness, reward = Eva(model_path)\n",
    "        material = material_list[j]\n",
    "        efficiency = fitness/material\n",
    "        fitness_matrix[i][j] = fitness\n",
    "        efficiency_matrix[i][j] = efficiency\n",
    "        \n",
    "        logging.info(\"___________________finish coarse optimization_____________________\")\n",
    "        logging.info(f\"morphology: {j}, rewardfunc: {i}, material cost: {material} reward: {reward} fitness: {fitness} efficiency: {efficiency}\")\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness = fitness\n",
    "            best_morphology = morphology\n",
    "            best_efficiency = efficiency\n",
    "            best_rewardfunc = rewardfunc\n",
    "            best_material = material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, 21.924121538935957]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficiency_matrix_select = efficiency_matrix \n",
    "efficiency_matrix_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameter:[1.45 1.05 0.6  0.05 0.1  0.   0.2  0.05 0.04 0.06]\n",
      "[1.55, 1.05, 0.65, 0.07, 0.12, -0.1, 0.15, 0.045, 0.05, 0.07]\n",
      "Successfully saved GPTWalker_refine2_10_50_1.xml\n",
      "improved parameter [1.55, 1.05, 0.65, 0.07, 0.12, -0.1, 0.15, 0.045, 0.05, 0.07]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nDescription: The walker is a two-dimensional bipedal robot consisting of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs below the thighs, and two feet attached to the legs on which the entire body rests. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass Walker2dEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"walker2d_v5.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.8, 2.0),\\n        healthy_angle_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n        is_healthy = healthy_z and healthy_angle\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n\\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\nfitness: 21.924121538935957\\n\\nbest reward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\nbest fitness: 21.924121538935957\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW2pCye2uDnyno7JFtD3DPpVGB2qi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To enhance the current reward function and improve the robot\\'s forward movement while promoting coordination and efficient control, we can introduce a reward structure that not only incorporates forward velocity and healthiness but also adds incentives for stability and energy efficiency through torque usage. Below is the proposed reward function:\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward velocity reward, scaled by forward reward weight\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Healthy reward based on whether the agent is in a healthy state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost based on the exerted torque (actions)\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # New component: penalizing excessive control actions for efficiency\\n    efficient_ctrl_cost = self._ctrl_cost_weight * (np.sum(np.abs(action)) / len(action))\\n    \\n    # Total reward combining forward movement, healthiness, while penalizing inefficiencies and control efforts\\n    reward = forward_reward + healthy_reward - ctrl_cost - efficient_ctrl_cost\\n    \\n    # Optional: Add a small penalty for sudden changes in velocity to promote smooth movement\\n    velocity_diff_penalty = np.abs(x_velocity - self.data.qvel[0])\\n    reward -= 0.1 * velocity_diff_penalty  # adjust the penalty weight as needed\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_efficiency\": -efficient_ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"penalty_velocity_change\": -velocity_diff_penalty\\n    }\\n\\n    return reward, reward_info\\n```\\n\\n### Explanation:\\n1. **Forward Reward**: Maintained from the original structure to encourage forward movement.\\n2. **Healthy Reward**: Continues to contribute positively when the robot maintains a healthy posture.\\n3. **Control Cost**: Penalizes the robot\\'s actions to discourage overexertion, ensuring efficient use of energy.\\n4. **Efficient Control Cost**: A new component that penalizes the robot for applying too much torque relative to the number of joints, promoting smoother, more efficient control.\\n5. **Velocity Change Penalty**: This optional penalty discourages rapid changes in the robot\\'s velocity, promoting a smoother gait which could enhance stability and overall performance.\\n\\nThe combination of these components aims to encourage both faster forward motion and more efficient energy usage while fostering a stable gait.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746977054, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=500, prompt_tokens=1891, total_tokens=2391, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 2.6340543267640624\n",
      "[1.6, 1.1, 0.7, 0.08, 0.13, -0.12, 0.045, 0.04, 0.06, 0.08]\n",
      "Successfully saved GPTWalker_refine2_10_50_3.xml\n",
      "improved parameter [1.6, 1.1, 0.7, 0.08, 0.13, -0.12, 0.045, 0.04, 0.06, 0.08]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nDescription: The walker is a two-dimensional bipedal robot consisting of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs below the thighs, and two feet attached to the legs on which the entire body rests. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass Walker2dEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"walker2d_v5.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.8, 2.0),\\n        healthy_angle_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n        is_healthy = healthy_z and healthy_angle\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n\\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\nfitness: 21.924121538935957\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward velocity reward, scaled by forward reward weight\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Healthy reward based on whether the agent is in a healthy state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost based on the exerted torque (actions)\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # New component: penalizing excessive control actions for efficiency\\n    efficient_ctrl_cost = self._ctrl_cost_weight * (np.sum(np.abs(action)) / len(action))\\n    \\n    # Total reward combining forward movement, healthiness, while penalizing inefficiencies and control efforts\\n    reward = forward_reward + healthy_reward - ctrl_cost - efficient_ctrl_cost\\n    \\n    # Optional: Add a small penalty for sudden changes in velocity to promote smooth movement\\n    velocity_diff_penalty = np.abs(x_velocity - self.data.qvel[0])\\n    reward -= 0.1 * velocity_diff_penalty  # adjust the penalty weight as needed\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_efficiency\": -efficient_ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"penalty_velocity_change\": -velocity_diff_penalty\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 21.924121538935957\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW33YHeWNfOBjegswpgmhjyqvmmJI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward velocity reward, scaled by forward reward weight\\n    forward_reward = self._forward_reward_weight * np.clip(x_velocity, 0, None)  # Ensure no negative reward\\n\\n    # Healthy reward based on whether the agent is in a healthy state\\n    healthy_reward = self.healthy_reward\\n\\n    # Control cost based on the exerted torque (actions)\\n    ctrl_cost = self.control_cost(action)\\n\\n    # New component: penalizing excessive control output to promote efficiency\\n    penalty_excessive_torque = self._ctrl_cost_weight * np.power(np.sum(np.abs(action)), 2) / len(action)\\n\\n    # Logical pressure to maintain a consistent forward velocity\\n    desired_velocity = 1.5  # Target speed\\n    velocity_difference = np.abs(desired_velocity - x_velocity)\\n    velocity_consistency_penalty = 0.5 * velocity_difference  # Additional penalty for deviation from target velocity\\n\\n    # Combine all rewards: forward progress, healthiness, and adjust for control costs and penalties\\n    reward = forward_reward + healthy_reward - ctrl_cost - penalty_excessive_torque - velocity_consistency_penalty\\n\\n    # Normalize reward to ensure consistent training behavior\\n    normalized_reward = np.tanh(reward)  # Using tanh to normalize between -1 and 1\\n    \\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_health\": healthy_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"penalty_excessive_torque\": -penalty_excessive_torque,\\n        \"penalty_velocity_change\": -velocity_consistency_penalty\\n    }\\n\\n    return normalized_reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746977944, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_ded0d14823', usage=CompletionUsage(completion_tokens=360, prompt_tokens=2063, total_tokens=2423, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 2.9951351808132114\n",
      "[1.65, 1.2, 0.85, 0.1, 0.15, -0.1, 0.04, 0.035, 0.05, 0.07]\n",
      "Successfully saved GPTWalker_refine2_10_50_5.xml\n",
      "improved parameter [1.65, 1.2, 0.85, 0.1, 0.15, -0.1, 0.04, 0.035, 0.05, 0.07]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nDescription: The walker is a two-dimensional bipedal robot consisting of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs below the thighs, and two feet attached to the legs on which the entire body rests. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass Walker2dEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"walker2d_v5.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.8, 2.0),\\n        healthy_angle_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n        is_healthy = healthy_z and healthy_angle\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n\\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\nfitness: 21.924121538935957\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward velocity reward, scaled by forward reward weight\\n    forward_reward = self._forward_reward_weight * np.clip(x_velocity, 0, None)  # Ensure no negative reward\\n\\n    # Healthy reward based on whether the agent is in a healthy state\\n    healthy_reward = self.healthy_reward\\n\\n    # Control cost based on the exerted torque (actions)\\n    ctrl_cost = self.control_cost(action)\\n\\n    # New component: penalizing excessive control output to promote efficiency\\n    penalty_excessive_torque = self._ctrl_cost_weight * np.power(np.sum(np.abs(action)), 2) / len(action)\\n\\n    # Logical pressure to maintain a consistent forward velocity\\n    desired_velocity = 1.5  # Target speed\\n    velocity_difference = np.abs(desired_velocity - x_velocity)\\n    velocity_consistency_penalty = 0.5 * velocity_difference  # Additional penalty for deviation from target velocity\\n\\n    # Combine all rewards: forward progress, healthiness, and adjust for control costs and penalties\\n    reward = forward_reward + healthy_reward - ctrl_cost - penalty_excessive_torque - velocity_consistency_penalty\\n\\n    # Normalize reward to ensure consistent training behavior\\n    normalized_reward = np.tanh(reward)  # Using tanh to normalize between -1 and 1\\n    \\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_health\": healthy_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"penalty_excessive_torque\": -penalty_excessive_torque,\\n        \"penalty_velocity_change\": -velocity_consistency_penalty\\n    }\\n\\n    return normalized_reward, reward_info\\n\\nbest fitness: 21.924121538935957\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW3FfLxi77TJ7YH4NInCGKWwK0DVx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward velocity reward, ensuring non-negative contributions\\n    forward_reward = self._forward_reward_weight * np.clip(x_velocity, 0, None)\\n\\n    # Healthy reward based on whether the agent is in a healthy state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost based on the exerted torque\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Introduce a penalty for rapid velocity changes to encourage smooth movement\\n    velocity_change_penalty = 0.5 * np.square(np.clip(x_velocity - self.data.qvel[0], -1, 1))\\n\\n    # Encourage consistent upward movement by rewarding positive z-velocity\\n    z_velocity_reward = np.clip(self.data.qvel[1], 0, None) * 0.5\\n\\n    # Combine all rewards: forward progress, healthiness, penalize control costs, and add z-velocity reward\\n    reward = forward_reward + healthy_reward - ctrl_cost - velocity_change_penalty + z_velocity_reward\\n\\n    # Normalize reward to ensure consistent training behavior\\n    normalized_reward = np.tanh(reward)  # Using tanh to normalize between -1 and 1\\n    \\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_health\": healthy_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"penalty_velocity_change\": -velocity_change_penalty,\\n        \"reward_z_velocity\": z_velocity_reward\\n    }\\n\\n    return normalized_reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746978695, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=320, prompt_tokens=2139, total_tokens=2459, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 3.0380093859117197\n",
      "[1.6, 1.2, 0.9, 0.07, 0.14, -0.15, 0.045, 0.035, 0.06, 0.07]\n",
      "Successfully saved GPTWalker_refine2_10_50_7.xml\n",
      "improved parameter [1.6, 1.2, 0.9, 0.07, 0.14, -0.15, 0.045, 0.035, 0.06, 0.07]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nDescription: The walker is a two-dimensional bipedal robot consisting of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs below the thighs, and two feet attached to the legs on which the entire body rests. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass Walker2dEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"walker2d_v5.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.8, 2.0),\\n        healthy_angle_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n        is_healthy = healthy_z and healthy_angle\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n\\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\nfitness: 21.924121538935957\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward velocity reward, ensuring non-negative contributions\\n    forward_reward = self._forward_reward_weight * np.clip(x_velocity, 0, None)\\n\\n    # Healthy reward based on whether the agent is in a healthy state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost based on the exerted torque\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Introduce a penalty for rapid velocity changes to encourage smooth movement\\n    velocity_change_penalty = 0.5 * np.square(np.clip(x_velocity - self.data.qvel[0], -1, 1))\\n\\n    # Encourage consistent upward movement by rewarding positive z-velocity\\n    z_velocity_reward = np.clip(self.data.qvel[1], 0, None) * 0.5\\n\\n    # Combine all rewards: forward progress, healthiness, penalize control costs, and add z-velocity reward\\n    reward = forward_reward + healthy_reward - ctrl_cost - velocity_change_penalty + z_velocity_reward\\n\\n    # Normalize reward to ensure consistent training behavior\\n    normalized_reward = np.tanh(reward)  # Using tanh to normalize between -1 and 1\\n    \\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_health\": healthy_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"penalty_velocity_change\": -velocity_change_penalty,\\n        \"reward_z_velocity\": z_velocity_reward\\n    }\\n\\n    return normalized_reward, reward_info\\n\\nbest fitness: 21.924121538935957\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW3S3grH4i0x8CKafRl9Qi14TKdNG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here\\'s a new reward function tailored for the Walker2D environment that builds on your requirements and the patterns you\\'ve provided. This function aims to encourage smooth and stable forward movement while integrating additional aspects to foster a more optimized learning process.\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward velocity reward, ensuring non-negative contributions\\n    forward_reward = self._forward_reward_weight * np.clip(x_velocity, 0, None)\\n\\n    # Healthy reward based on whether the agent is in a healthy state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost for the exerted torque\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Introducing a penalty for high torques to further encourage efficient movements\\n    torque_penalty = 0.1 * np.sum(np.abs(action))\\n\\n    # Introduce a reward for smoothness in velocity by penalizing large changes in velocity\\n    velocity_change_penalty = 0.4 * np.square(np.clip(x_velocity - self.data.qvel[0], -1, 1))\\n    \\n    # Reward for a positive z-velocity to encourage upward motion\\n    z_velocity_reward = np.clip(self.data.qvel[1], 0, None) * 0.5\\n\\n    # Add a small bonus for maintaining a consistent horizontal position\\n    position_consistency_reward = 0.2 * np.clip(1 - np.abs(self.data.qpos[0] - self.init_qpos[0]), 0, None)\\n\\n    # Combine rewards and penalties\\n    reward = (\\n        forward_reward + \\n        healthy_reward - \\n        ctrl_cost - \\n        velocity_change_penalty - \\n        torque_penalty + \\n        z_velocity_reward + \\n        position_consistency_reward\\n    )\\n\\n    # Normalize reward to avoid extreme values\\n    normalized_reward = np.tanh(reward)  # Normalization step\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_health\": healthy_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"penalty_velocity_change\": -velocity_change_penalty,\\n        \"penalty_torque\": -torque_penalty,\\n        \"reward_z_velocity\": z_velocity_reward,\\n        \"reward_position_consistency\": position_consistency_reward,\\n    }\\n\\n    return normalized_reward, reward_info\\n```\\n\\n### Key Features:\\n1. **Forward Reward**: Maintains focus on moving forward while ensuring it only increases for positive velocity.\\n2. **Healthiness Reward**: Keeps the existing mechanism for healthy robot states.\\n3. **Control Cost**: Punishes excessive energy use through torque.\\n4. **Velocity Change Penalty**: Encourages consistency in speed, avoiding jerky movements.\\n5. **Z-Velocity Reward**: Smoothes overall performance by promoting upward movement.\\n6. **Position Consistency Reward**: Rewards staying close to the starting horizontal position for better stability.\\n7. **Normalization**: Ensures the reward stays within manageable limits for consistent learning.\\n\\nThis function aims to harmonize speed, control, and stability, potentially leading to improved training outcomes in the bipedal walker task.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746979463, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=640, prompt_tokens=2099, total_tokens=2739, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 2.358516759979719\n"
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "coarse_best = [(10,50)]\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_walker_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "    print(f\"Initial parameter:{parameter}\")\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        iteration +=1   \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list[morphology_index],  # 这本身已经是list结构，可以保留\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],  # 👈 用 [] 包装成列表\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        print(\"improved parameter\", improved_parameter)\n",
    "        shutil.copy(improved_morphology, \"GPTWalker.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTWalkerEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "        improved_material = compute_walker_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            # break\n",
    "            \n",
    "        iteration +=1        \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            [rewardfunc_list[rewardfunc_index]],\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        shutil.copy(best_morphology, \"GPTWalker.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTWalkerEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_walker_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "    logging.info(\"____________________________________________\")\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_refine2_10_50_3.xml',\n",
       "  'best_parameter': [1.6,\n",
       "   1.1,\n",
       "   0.7,\n",
       "   0.08,\n",
       "   0.13,\n",
       "   -0.12,\n",
       "   0.045,\n",
       "   0.04,\n",
       "   0.06,\n",
       "   0.08],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_10_50_6.py',\n",
       "  'best_fitness': 3.0380093859117197,\n",
       "  'best_material': 0.03829601444725958,\n",
       "  'best_efficiency': 79.32964904469625,\n",
       "  'best_iteration': 8}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 06:13:50,208 - Final optimized result: rewardfunc_index0 morphology_index0\n",
    "2025-04-07 06:13:50,208 -   Morphology: results/Div_m25_r5/assets/GPTWalker_refine_0_0_0.xml\n",
    "2025-04-07 06:13:50,208 -   Parameter: [1.05, 0.55, 0.3, 0.1, 0.3, -0.25, 0.015, 0.025, 0.03, 0.015]\n",
    "2025-04-07 06:13:50,208 -   Rewardfunc: results/Div_m25_r5/env/GPTWalker_0_0_1.py\n",
    "2025-04-07 06:13:50,208 -   Fitness: 6.466572426996648\n",
    "2025-04-07 06:13:50,208 -   Material: 0.003643200280612963\n",
    "2025-04-07 06:13:50,208 -   Efficiency: 1774.970336220071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_11.xml',\n",
    "  'best_parameter': array([ 1.   ,  0.45 ,  0.25 ,  0.05 , -0.05 , -0.3  ,  0.02 ,  0.025,\n",
    "          0.03 ,  0.015]),\n",
    "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_1_11_1.py',\n",
    "  'best_fitness': 14.883819809931813,\n",
    "  'best_material': 0.0033798300964870192,\n",
    "  'best_efficiency': 4403.718348269042,\n",
    "  'best_iteration': 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign best 1e6 steps train\n",
      "\n",
      "fitness:3.4504453808491076\n",
      "efficiency:1020.8931462074042\n"
     ]
    }
   ],
   "source": [
    "# Robodesign best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_11.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_refine_1_11_1.py\"\n",
    "\n",
    "morphology_index=9999\n",
    "rewardfunc_index=9999\n",
    "\n",
    "parameter = [ 1.   ,  0.45 ,  0.25 ,  0.05 , -0.05 , -0.3  ,  0.02 ,  0.025,\n",
    "          0.03 ,  0.015]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Robodesign best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_refine.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_3.py\"\n",
    "\n",
    "\n",
    "\n",
    "# morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "# rewardfunc = \"results/Div_m25_r5/env/GPTWalker_0_0_1.py\"\n",
    "\n",
    "morphology_index=9998\n",
    "rewardfunc_index=9998\n",
    "\n",
    "\n",
    "parameter = [0.95, 0.75, 0.45, 0.15, 0.2, -0.2, 0.01, 0.01, 0.01, 0.008]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009984965818806487"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter = [ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02, 0.025,  0.01 ]\n",
    "parameter = [1.1, 0.75, 0.45, 0.15, 0.2, -0.2, 0.015, 0.012, 0.012, 0.008]\n",
    "material = compute_walker_volume(parameter)\n",
    "material\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Robodesign best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_refine.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "\n",
    "\n",
    "# morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "# rewardfunc = \"results/Div_m25_r5/env/GPTWalker_0_0_1.py\"\n",
    "\n",
    "morphology_index=999\n",
    "rewardfunc_index=999\n",
    "\n",
    "# parameter = [1.05, 0.55, 0.3, 0.1, 0.3, -0.25, 0.015, 0.025, 0.03, 0.015]\n",
    "# parameter = [ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02, 0.025,  0.01 ]\n",
    "\n",
    "parameter = [0.95, 0.75, 0.45, 0.15, 0.2, -0.2, 0.01, 0.01, 0.01, 0.008]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 01:54:53,368 - Initial morphology:results/Div_m25_r5/assets/GPTWalker_11.xml\n",
    "2025-04-07 01:54:53,368 - Initial parameter:[ 1.     0.45   0.25   0.05  -0.05  -0.3    0.02   0.025  0.03   0.015]\n",
    "2025-04-07 01:54:53,368 - Initial rewardfunc:results/Div_m25_r5/env/GPTrewardfunc_3.py\n",
    "2025-04-07 01:54:53,368 - Initial fitness:5.010931913179973\n",
    "2025-04-07 01:54:53,368 - Initial efficiency:1482.5987609224244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign coarse best 1e6 steps train\n",
      "\n",
      "fitness:9.459751803686697\n",
      "efficiency:4109.826005402759\n"
     ]
    }
   ],
   "source": [
    "# coarse best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_11.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_3.py\"\n",
    "\n",
    "morphology_index=888\n",
    "rewardfunc_index=888\n",
    "\n",
    "# parameter = [ 1.    , 0.45  , 0.25 ,  0.05  ,-0.05  ,-0.3  ,  0.02  , 0.025  ,0.03  , 0.015]\n",
    "\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_3.py\"\n",
    "\n",
    "parameter =[ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 ,\n",
    "        0.025,  0.01 ]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign coarse best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign coarse best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign human 1e6 steps train\n",
      "\n",
      "fitness:4.777281062373332\n",
      "efficiency:170.29255939736836\n"
     ]
    }
   ],
   "source": [
    "# human\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "\n",
    "morphology_index=666\n",
    "rewardfunc_index=666\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign human 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign human 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 11:08:16,602 - morphology: 50, rewardfunc: 1, material cost: 0.10202845741308451 reward: 536.5836396066536 fitness: 2.282395280964088 efficiency: 22.37018317079236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Morphology Design) 1e6 steps train\n",
      "\n",
      "fitness:1.912675745085834\n",
      "efficiency:68.17987965860266\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Morphology Design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_1.py\"\n",
    "\n",
    "morphology_index=555\n",
    "rewardfunc_index=555\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign (w/o Morphology Design) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Morphology Design) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 08:36:37,117 - morphology: 0, rewardfunc: 10, material cost: 0.0023017402175301216 reward: 533.5658482194434 fitness: 3.5383881162370003 efficiency: 1537.2664948409606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 ,\n",
       "        0.025,  0.01 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      " Robodesign (w/o Reward Design) 1e6 steps train\n",
      "\n",
      "fitness:8.959412770453445\n",
      "efficiency:3892.451764199232\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Reward Design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "\n",
    "morphology_index=444\n",
    "rewardfunc_index=444\n",
    "\n",
    "parameter =[ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 ,\n",
    "        0.025,  0.01 ]\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" Robodesign (w/o Reward Design) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" Robodesign (w/o Reward Design) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "025-04-08 01:07:15,915 - Initial morphology:results/noDiv_m25_r5/assets/GPTWalker_20.xml\n",
    "2025-04-08 01:07:15,916 - Initial parameter:[ 0.6    0.45   0.3    0.15   0.1   -0.1    0.04   0.04   0.035  0.03 ]\n",
    "2025-04-08 01:07:15,916 - Initial rewardfunc:results/noDiv_m25_r5/env/GPTrewardfunc_1.py\n",
    "2025-04-08 01:07:15,916 - Initial fitness:2.539201719144284\n",
    "2025-04-08 01:07:15,916 - Initial efficiency:427.68481006358724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      " Robodesign (w/o diversity reflection) 1e6 steps train\n",
      "\n",
      "fitness:5.337485313971282\n",
      "efficiency:899.007501259998\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o diversity reflection)\n",
    "\n",
    "morphology = \"results/noDiv_m25_r5/assets/GPTWalker_20.xml\"\n",
    "rewardfunc = \"results/noDiv_m25_r5/env/GPTrewardfunc_1.py\"\n",
    "\n",
    "morphology_index=333\n",
    "rewardfunc_index=333\n",
    "\n",
    "parameter = [ 0.6 ,   0.45,   0.3    ,0.15 ,  0.1 ,  -0.1  ,  0.04 ,  0.04   ,0.035 , 0.03 ]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" Robodesign  (w/o diversity reflection) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" Robodesign (w/o diversity reflection) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "# eureka reward 1e6 steps train\n",
      "\n",
      "fitness:3.919962300766995\n",
      "efficiency:139.73228793182562\n"
     ]
    }
   ],
   "source": [
    "# eureka reward\n",
    "morphology = \"results/eureka/assets/GPTWalker_0.xml\"\n",
    "rewardfunc = \"results/eureka/env/GPTrewardfunc_2_1.py\"\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"# eureka reward 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"# eureka reward 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "eureka morphology 1e6 steps train\n",
      "\n",
      "fitness:2.9077493339127254\n",
      "efficiency:1946.5997575676536\n"
     ]
    }
   ],
   "source": [
    "# eureka morphology\n",
    "\n",
    "morphology = \"results/eureka_morphology/assets/GPTWalker_5.xml\"\n",
    "rewardfunc = \"results/eureka_morphology/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "parameter = [ 1.25   , 1.0 ,  0.75 ,   0.45  , 0.4 , -0.4  ,  0.025  , 0.018 ,  0.01 , 0.006 ]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka morphology 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka morphology 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robodesign (w/o Reward Design w/o morphology design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=123\n",
    "rewardfunc_index=123\n",
    "\n",
    "parameter = parameter_list[0]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" (w/o Reward Design w/o morphology design) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" (w/o Reward Design w/o morphology design) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
