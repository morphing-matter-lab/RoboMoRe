{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTWalker import GPTWalkerEnv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        api_key = \"<api_key>\"\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = \"gpt-4-turbo\"\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTWalker_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums                                                                                                                                                                                                                                                                                   \n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_walker_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = walker_design(parameter)  \n",
    "            filename = f\"GPTWalker_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_walker_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(initial_parameter)})\n",
    "\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "\n",
    "        xml_file = walker_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTWalker_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_walker_volume(diverse_parameter['parameters'])) \n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistant\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = walker_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTWalker_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\":reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, rewardfunc_index, morphology_index, iteration):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\"best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\" \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        # print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = walker_design(parameter)  \n",
    "        filename = f\"GPTWalker_refine2_{rewardfunc_index}_{morphology_index}_{iteration}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"parameters.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 26\n",
    "rewardfunc_nums = 6\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "designer = DGA()\n",
    "morphology_list, material_list, parameter_list = designer.generate_morphology_div(morphology_nums, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'morphology_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmorphology_list\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'morphology_list' is not defined"
     ]
    }
   ],
   "source": [
    "morphology_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "designer = DGA()\n",
    "rewardfunc_list = designer.generate_rewardfunc_div(rewardfunc_nums, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewardfunc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extracting all the parameters from the provided log and storing them in a list\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrewardfunc_list\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewardfunc_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Extracting all the parameters from the provided log and storing them in a list\n",
    "\n",
    "rewardfunc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter coarse optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "morphology_list = [f'results/Div_m25_r5/assets/GPTWalker_{i}.xml' for i in range(0,26) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,6)]\n",
    "\n",
    "parameter_list =np.array([[ 1.  ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 , 0.025,  0.01 ],[1.5, 1.0, 0.6, 0.2, 0.3, -0.4, 0.05, 0.04, 0.03, 0.02], [1.0, 0.7, 0.5, 0.2, 0.25, -0.15, 0.045, 0.035, 0.025, 0.015], [1.8, 1.2, 0.9, 0.3, 0.35, -0.25, 0.06, 0.055, 0.045, 0.025], [1.0, 0.55, 0.35, 0.1, 0.1, -0.2, 0.03, 0.06, 0.07, 0.04], [1.3, 0.9, 0.6, 0.2, 0.25, -0.2, 0.04, 0.05, 0.05, 0.03], [1.2, 0.6, 0.3, 0.15, 0.2, -0.05, 0.065, 0.045, 0.03, 0.025], [0.8, 0.5, 0.2, 0.05, 0.1, -0.15, 0.04, 0.02, 0.01, 0.025], [1.1, 0.85, 0.55, 0.25, 0.3, -0.25, 0.035, 0.07, 0.08, 0.04], [2.0, 1.0, 0.5, 0.2, 0.25, -0.3, 0.02, 0.03, 0.04, 0.035], [1.2, 0.9, 0.4, 0.1, 0.15, -0.05, 0.025, 0.035, 0.045, 0.02], [1.0, 0.45, 0.25, 0.05, -0.05, -0.3, 0.02, 0.025, 0.03, 0.015], [0.9, 0.6, 0.4, 0.2, 0.25, -0.2, 0.02, 0.04, 0.06, 0.03], [0.6, 0.3, 0.15, 0.05, 0.1, -0.1, 0.04, 0.05, 0.06, 0.02], [0.5, 0.35, 0.2, 0.05, 0.15, -0.15, 0.035, 0.06, 0.075, 0.04], [0.8, 0.5, 0.3, 0.1, 0.2, -0.3, 0.04, 0.08, 0.12, 0.06], [0.6, 0.45, 0.15, 0.05, 0.1, -0.2, 0.025, 0.04, 0.055, 0.03], [1.0, 0.2, 0.1, 0.05, 0.05, -0.4, 0.03, 0.07, 0.1, 0.05], [0.8, 0.55, 0.25, 0.1, -0.05, -0.2, 0.045, 0.1, 0.15, 0.06], [0.6, 0.4, 0.3, 0.1, 0.2, -0.25, 0.055, 0.04, 0.03, 0.025], [1.2, 0.8, 0.4, 0.2, -0.1, -0.25, 0.03, 0.04, 0.06, 0.02], [1.0, 0.6, 0.4, 0.1, 0.2, -0.1, 0.02, 0.04, 0.05, 0.03], [1.5, 1.0, 0.7, 0.3, 0.4, -0.2, 0.05, 0.06, 0.07, 0.04], [1.0, 0.6, 0.25, 0.1, 0.15, -0.25, 0.03, 0.035, 0.04, 0.02], [1.2, 0.6, 0.3, 0.1, 0.12, -0.08, 0.025, 0.045, 0.065, 0.05], [1.7, 0.9, 0.45, 0.15, 0.25, -0.3, 0.025, 0.05, 0.08, 0.035], [1.5, 1.1, 0.6, 0.2, 0.35, -0.25, 0.02, 0.03, 0.04, 0.02], [1.2, 0.3, 0.15, 0.05, 0.1, -0.15, 0.02, 0.07, 0.12, 0.06], [1.0, 0.8, 0.5, 0.2, 0.25, -0.1, 0.025, 0.05, 0.1, 0.05], [1.0, 0.7, 0.4, 0.1, 0.2, -0.2, 0.03, 0.03, 0.06, 0.04], [0.9, 0.5, 0.25, 0.05, 0.1, -0.5, 0.03, 0.035, 0.07, 0.065], [1.8, 1.3, 0.6, 0.15, 0.25, -0.2, 0.025, 0.07, 0.12, 0.05], [1.0, 0.7, 0.5, 0.1, 0.2, -0.1, 0.01, 0.04, 0.08, 0.02], [1.0, 0.9, 0.6, 0.1, 0.2, -0.2, 0.02, 0.03, 0.05, 0.025], [1.2, 0.6, 0.3, 0.1, 0.25, -0.2, 0.02, 0.03, 0.08, 0.05], [0.8, 0.5, 0.3, 0.1, 0.15, -0.15, 0.015, 0.025, 0.035, 0.02], [1.0, 0.6, 0.35, 0.05, 0.1, -0.3, 0.02, 0.05, 0.1, 0.06], [1.0, 0.7, 0.3, 0.1, 0.15, -0.2, 0.025, 0.06, 0.12, 0.04], [1.2, 0.4, 0.2, 0.1, 0.2, -0.6, 0.02, 0.07, 0.15, 0.045], [0.8, 0.4, 0.2, 0.05, 0.1, -0.3, 0.015, 0.04, 0.1, 0.06], [0.7, 0.2, 0.05, 0.01, 0.2, -0.3, 0.01, 0.03, 0.08, 0.06], [1.0, 0.3, 0.15, 0.05, 0.1, -0.2, 0.02, 0.08, 0.15, 0.025], [1.5, 0.9, 0.6, 0.2, 0.25, -0.15, 0.018, 0.02, 0.025, 0.015], [0.85, 0.5, 0.25, 0.05, 0.1, -0.25, 0.015, 0.035, 0.05, 0.03], [1.0, 0.8, 0.5, 0.15, 0.2, -0.25, 0.02, 0.03, 0.035, 0.015], [1.0, 0.5, 0.3, 0.15, 0.3, -0.25, 0.04, 0.03, 0.02, 0.015], [1.2, 0.8, 0.4, 0.15, 0.3, -0.4, 0.02, 0.03, 0.04, 0.035], [0.9, 0.6, 0.25, 0.05, 0.1, -0.2, 0.02, 0.01, 0.03, 0.015], [1.0, 0.5, 0.25, 0.05, 0.25, -0.25, 0.015, 0.02, 0.025, 0.01], [1.4, 0.9, 0.6, 0.2, 0.3, -0.15, 0.025, 0.015, 0.015, 0.01], [1.45, 1.05, 0.6, 0.05, 0.1, 0, 0.2, 0.05,0.04,0.06]])\n",
    "\n",
    "material_list = [compute_walker_volume(parameter) for parameter in parameter_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'morphology_nums:{morphology_nums}')\n",
    "logging.info(f'rewardfunc_nums:{rewardfunc_nums}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'morphology_list:{morphology_list}')\n",
    "logging.info(f'material_list:{material_list}')\n",
    "logging.info(f'_________________________________enter coarse optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 results/Div_m25_r5/env/GPTrewardfunc_0.py\n",
      "50 results/Div_m25_r5/assets/GPTWalker_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "1 results/Div_m25_r5/env/GPTrewardfunc_1.py\n",
      "50 results/Div_m25_r5/assets/GPTWalker_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "2 results/Div_m25_r5/env/GPTrewardfunc_2.py\n",
      "50 results/Div_m25_r5/assets/GPTWalker_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "3 results/Div_m25_r5/env/GPTrewardfunc_3.py\n",
      "50 results/Div_m25_r5/assets/GPTWalker_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "4 results/Div_m25_r5/env/GPTrewardfunc_4.py\n",
      "50 results/Div_m25_r5/assets/GPTWalker_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "5 results/Div_m25_r5/env/GPTrewardfunc_5.py\n",
      "50 results/Div_m25_r5/assets/GPTWalker_50.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-524:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-517:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-520:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-528:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-516:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-514:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-519:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-513:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-521:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-527:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-525:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-515:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-523:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-518:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-526:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-522:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGPTrewardfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_rew\n\u001b[1;32m     16\u001b[0m GPTWalkerEnv\u001b[38;5;241m.\u001b[39m_get_rew \u001b[38;5;241m=\u001b[39m _get_rew\n\u001b[0;32m---> 18\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# model_path = \"results/Div_m50_r10/coarse/SAC_morphology50_rewardfunc0_500000.0steps\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m fitness, reward \u001b[38;5;241m=\u001b[39m Eva(model_path)\n",
      "File \u001b[0;32m~/MML/Walker2D/utils.py:82\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(morphology, rewardfunc, folder_name, total_timesteps, stage)\u001b[0m\n\u001b[1;32m     64\u001b[0m envs \u001b[38;5;241m=\u001b[39m VecMonitor(envs)  \n\u001b[1;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC(\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m     envs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39mfolder_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/sac_morphology\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmorphology\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rewardfunc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewardfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m )\n\u001b[0;32m---> 82\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 训练 100 万步   \u001b[39;00m\n\u001b[1;32m     83\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_name, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC_morphology\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmorphology\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rewardfunc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewardfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# print(model_path)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:557\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mreset_noise(env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# Select action randomly or according to policy\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m    560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:390\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._sample_action\u001b[0;34m(self, learning_starts, action_noise, n_envs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Note: when using continuous actions,\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;66;03m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself._last_obs was not set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 390\u001b[0m     unscaled_action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:353\u001b[0m, in \u001b[0;36mSACPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:168\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 168\u001b[0m     mean_actions, log_std, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_dist_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Note: the action is squashed\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mactions_from_params(mean_actions, log_std, deterministic\u001b[38;5;241m=\u001b[39mdeterministic, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:157\u001b[0m, in \u001b[0;36mActor.get_action_dist_params\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    155\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[1;32m    156\u001b[0m latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_pi(features)\n\u001b[0;32m--> 157\u001b[0m mean_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std, \u001b[38;5;28mdict\u001b[39m(latent_sde\u001b[38;5;241m=\u001b[39mlatent_pi)\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, rewardfunc in enumerate(rewardfunc_list):\n",
    "    for j, morphology in enumerate(morphology_list):\n",
    "        # if i not in [10]:\n",
    "        #     continue\n",
    "        if j not in [50]:\n",
    "            continue\n",
    "\n",
    "        print(i, rewardfunc)\n",
    "        print(j, morphology)\n",
    "        shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "        shutil.copy(rewardfunc, \"GPTrewardfunc.py\")         \n",
    "\n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTWalkerEnv._get_rew = _get_rew\n",
    "\n",
    "        model_path = Train(j,  i, folder_name, total_timesteps=5e5)\n",
    "        # model_path = \"results/Div_m50_r10/coarse/SAC_morphology50_rewardfunc0_500000.0steps\"\n",
    "        fitness, reward = Eva(model_path)\n",
    "        material = material_list[j]\n",
    "        efficiency = fitness/material\n",
    "        fitness_matrix[i][j] = fitness\n",
    "        efficiency_matrix[i][j] = efficiency\n",
    "        \n",
    "        logging.info(\"___________________finish coarse optimization_____________________\")\n",
    "        logging.info(f\"morphology: {j}, rewardfunc: {i}, material cost: {material} reward: {reward} fitness: {fitness} efficiency: {efficiency}\")\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness = fitness\n",
    "            best_morphology = morphology\n",
    "            best_efficiency = efficiency\n",
    "            best_rewardfunc = rewardfunc\n",
    "            best_material = material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, 22.03956807368918],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, 22.37018317079236],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, 15.529767079216246],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, 23.501167815424633],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, 19.732093740316223],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None],\n",
       "       [1537.2664948409606, 73.62787496720499, 258.31611527007493,\n",
       "        64.75118441231163, 74.93995824807168, 43.06018130259688,\n",
       "        180.12343612293145, 605.7474322480911, 69.81440125269484,\n",
       "        274.4314895365642, 99.31721019153916, 1051.4346756536547,\n",
       "        209.580908175287, 11.668751855861618, 95.42955882334901,\n",
       "        38.376050927029645, 13.319130630659009, 81.9971585906027,\n",
       "        7.666078623617548, 221.65121426974633, 152.97853018372314,\n",
       "        89.40667676349767, 39.4001538721706, 365.6535291363097,\n",
       "        60.27997202659875, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficiency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitness_matrix = np.array([[2.1684423634862577, 2.260962152299912, 2.7830358346106645,\n",
    "        2.267754423199486, 1.2019426754496374, 2.2180311735828018,\n",
    "        1.1420885490570625, 1.361656197547038, 1.6480213650841316,\n",
    "        3.583995803620787, 0.9911623374110732, 2.832058260689889,\n",
    "        2.0765715894062553, 0.5379509086343696, 1.3115382453304014,\n",
    "        1.8274472777097637, 2.74449354132574, 3.656782967997985,\n",
    "        0.7861817564663918, 1.9263857740896508, 1.3509708122407755,\n",
    "        1.2926124369864613, 1.5461287579259553, 0.8030222253603531,\n",
    "        1.118556567616124],\n",
    "       [3.296117704275014, 2.1808387002338883, 1.4803630351537576,\n",
    "        2.3965327714544897, 1.1166557585148793, 1.5370766857209004,\n",
    "        2.2791414846352063, 0.6935612371162847, 3.115774124010825,\n",
    "        1.8580872496255996, 1.3124392280655628, 3.551990245232875,\n",
    "        1.9878199356831245, 0.18095867926972256, 0.9974099576625679,\n",
    "        2.4471040225094973, 0.13097317573228934, 2.2469147585163824,\n",
    "        2.3464868387191493, 1.6976871702661034, 1.3822707157233114,\n",
    "        0.9038684847770183, 1.7621550661800363, 2.813059184445691,\n",
    "        0.2771921904725525],\n",
    "       [3.150755544052107, 0.4936148243852119, 1.6894076122096988,\n",
    "        1.8781770282697574, 1.4165204655434278, 1.5587308446578891,\n",
    "        2.0146879015880352, 1.3121857207121383, 1.192973647433868,\n",
    "        1.5749288784877309, 1.1600410291788124, 1.3149447091526922,\n",
    "        2.811299860369774, 0.5491428089460723, 1.9408819004445945,\n",
    "        2.058863057231706, 0.5674005974561951, 5.5429768680638,\n",
    "        0.9168869472706261, 2.8074231351843446, 1.8073516899339324,\n",
    "        -0.1910151615774878, 1.088694608983779, 2.6067062564529886,\n",
    "        0.17939723865521628],\n",
    "       [3.9743075022649754, 1.3000726519486097, 1.0079781782462414,\n",
    "        2.516884441478525, 1.8344365120705952, 0.8175545429630111,\n",
    "        3.568250871354235, 1.8108210732761214, 1.4402524772215386,\n",
    "        3.5355060497471067, 0.9842685911195281, 5.010931913179973,\n",
    "        1.7366540219688487, 1.3110460430385134, 1.8258827896409806,\n",
    "        1.4331772987480949, 0.2306285658101946, 3.6890675113195948,\n",
    "        0.4510896438230694, 1.985574528281275, 1.3732822375978082,\n",
    "        0.7316284386825173, 1.9013945832827728, 0.2864017371052294,\n",
    "        1.8282309234707705],\n",
    "       [1.5493358592468198, 1.0287715538818076, 0.7361911461574775,\n",
    "        2.3924977865989403, 1.4234553627228812, 2.5699812500455765,\n",
    "        2.503516634235497, 1.2124805677454231, -0.23619038873382994,\n",
    "        2.0331253927381554, 0.3087242005045853, 2.9831742056642696,\n",
    "        1.3648269575619312, 0.475993552248324, 1.6747878244121595,\n",
    "        3.053773634638773, -0.2254332359696839, 1.98517413335603,\n",
    "        0.7770595857596254, 1.623537335568082, 1.2941700643303151,\n",
    "        1.5346291542977477, 1.4308150135371362, 0.9605436791852171,\n",
    "        0.616100189409229]], dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均值： 149.5382972569018\n",
      "标准差： 199.40039305172402\n"
     ]
    }
   ],
   "source": [
    "# efficiency_matrix_select = efficiency_matrix[:10, :50]\n",
    "efficiency_matrix = np.array([[50.57913839160387, 169.71072284965652, 456.8683959568497,\n",
    "        88.33915138387243, 55.03047939135277, 122.2759352987948,\n",
    "        71.91179138774183, 357.17233560993066, 46.20201188838541,\n",
    "        286.98703526067237, 98.9949076909252, 837.9291798228314,\n",
    "        172.16355259847157, 54.777700735723904, 71.48452612208305,\n",
    "        30.559743711894424, 300.62783175872613, 130.7756164716796,\n",
    "        9.372491393851774, 260.52441714907957, 107.39944192856754,\n",
    "        120.0733417403442, 44.42571483669403, 108.31693473727309,\n",
    "        63.47225487619574],\n",
    "       [76.88228026107649, 163.696553637004, 243.01917959282287,\n",
    "        93.3556425370078, 51.125650966008386, 84.73618027148908,\n",
    "        143.50651455314286, 181.9261627094962, 87.3502226178499,\n",
    "        148.78559581096238, 131.08326994310994, 1050.9375157422257,\n",
    "        164.80536659519854, 18.426403263696244, 54.363171205117176,\n",
    "        40.92204063909154, 14.346611222095596, 80.35523717864157,\n",
    "        27.973719207794705, 229.59521736712296, 109.88772082844008,\n",
    "        83.96214236804126, 50.63291014207957, 379.4439785984215,\n",
    "        15.729211979740409],\n",
    "       [73.49169310846413, 37.05136265571969, 277.33633045925677,\n",
    "        73.16337392121522, 64.85484031703581, 85.92993379879053,\n",
    "        126.85515165178973, 344.1958693134537, 33.44482287003379,\n",
    "        126.11180211957438, 115.86210479022125, 389.0564530209492,\n",
    "        233.07810520476025, 55.917333657796334, 105.78648652160183,\n",
    "        34.42962658038504, 62.15223639020261, 198.230582277669,\n",
    "        10.930697579467848, 379.67591217827623, 143.68079688235557,\n",
    "        -17.74377850421455, 31.28202356693746, 351.60973450361996,\n",
    "        10.179858208044966],\n",
    "       [92.7011261874951, 97.5851229065605, 165.4715931889148,\n",
    "        98.04387698108626, 83.9889644774293, 45.07026212670541,\n",
    "        224.67549691467332, 474.99155313863355, 40.377244788612956,\n",
    "        283.10423754906, 98.30637691042892, 1482.5987609224244,\n",
    "        143.98180519366034, 133.49933357047288, 99.51853591510897,\n",
    "        23.96650862526436, 25.262717742688395, 131.93019170689013,\n",
    "        5.3776798683169575, 268.52910442146145, 109.17315503196295,\n",
    "        67.96242170598926, 54.63375098350023, 38.63175549437341,\n",
    "        103.74257548224753],\n",
    "       [36.13841629340406, 77.22091406034578, 120.8545229205266,\n",
    "        93.19846187655378, 65.17235189567437, 141.6782887420304,\n",
    "        157.63433236899536, 318.042481680349, -6.621559270650414,\n",
    "        162.80170534397476, 30.834629785000413, 882.6402867898502,\n",
    "        113.1545181946799, 48.46879508649463, 91.28320459536242,\n",
    "        51.06715841655201, -24.693628866469112, 70.99474411394986,\n",
    "        9.263741138914932, 219.5671935277486, 102.8838975722906,\n",
    "        142.5547562564717, 41.11234555958338, 129.5644674889495,\n",
    "        34.96047440389759]], dtype=object)\n",
    "efficiency_matrix_select = efficiency_matrix\n",
    "# parameter_list[48]\n",
    "# material_list[48]\n",
    "efficiency_matrix_select\n",
    "mean = np.mean(efficiency_matrix)\n",
    "\n",
    "std = np.std(efficiency_matrix)\n",
    "\n",
    "print(\"平均值：\", mean)\n",
    "print(\"标准差：\", std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'_________________________________end coarse optimization stage_________________________________')\n",
    "logging.info(f\"Stage1: Final best morphology: {best_morphology}, Fitness: {best_fitness}, best_efficiency: {best_efficiency}, best reward function: {best_rewardfunc}, Material cost: {best_material}, Reward: {best_reward}\")\n",
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'fitness_matrix:{fitness_matrix}')\n",
    "logging.info(f'efficiency_matrix:{efficiency_matrix}')\n",
    "logging.info(f'_________________________________enter fine optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 获取矩阵中所有非 None 的值和它们的坐标\n",
    "all_values_with_coords = []\n",
    "for i in range(len(efficiency_matrix_select)):\n",
    "    for j in range(len(efficiency_matrix_select[0])):\n",
    "        value = efficiency_matrix_select[i][j]\n",
    "        if value is not None:\n",
    "            all_values_with_coords.append(((i, j), value))\n",
    "\n",
    "# 按值降序排序\n",
    "sorted_values = sorted(all_values_with_coords, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 计算前 20% 的数量（至少选1个）\n",
    "top_k = max(1, int(len(sorted_values) * 0.05))\n",
    "# 取前 20% 个坐标\n",
    "efficiency_coarse_best = [coord for coord, val in sorted_values[:top_k]]\n",
    "\n",
    "logging.info(f\"fitness_coarse_best {efficiency_coarse_best}\")\n",
    "logging.info(f\"fitness_coarse_best values {sorted_values[:top_k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 11), (1, 11), (4, 11), (0, 11), (3, 7), (0, 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_best = efficiency_coarse_best\n",
    "coarse_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameter:[ 1.     0.45   0.25   0.05  -0.05  -0.3    0.02   0.025  0.03   0.015]\n",
      "[2.2, 1.0, 0.7, 0.25, 0.3, -0.4, 0.08, 0.04, 0.03, 0.02]\n",
      "Successfully saved GPTWalker_refine2_3_11_1.xml\n",
      "improved_morphology:[2.2, 1.0, 0.7, 0.25, 0.3, -0.4, 0.08, 0.04, 0.03, 0.02]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 1.6387538435548066\n",
      "improved_efficiency:484.86278800171647\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nDescription: The walker is a two-dimensional bipedal robot consisting of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs below the thighs, and two feet attached to the legs on which the entire body rests. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass Walker2dEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"walker2d_v5.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.8, 2.0),\\n        healthy_angle_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n        is_healthy = healthy_z and healthy_angle\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n\\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for moving forward, the primary objective\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Penalty for using too much control or torque, which promotes efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Checking if the walker is within the healthy range of operation\\n    health_bonus = self.healthy_reward\\n\\n    # Total reward calculation\\n    reward = forward_reward + health_bonus - control_penalty\\n\\n    # Reward breakdown for debugging and analysis\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'control_penalty\\': control_penalty,\\n        \\'health_bonus\\': health_bonus\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 837.9291798228314\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for moving forward, emphasizing faster and smoother motions\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Encourage alternating leg movement by rewarding sinusoidal pattern in hinge angles\\n    # This promotes a more natural walking gait\\n    qpos = self.data.qpos\\n    sinusoidal_reward = np.sin(qpos[2]) * np.cos(qpos[3]) + np.sin(qpos[4]) * np.cos(qpos[5])\\n\\n    # Penalize excessive control inputs to maintain efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Reward for staying in a healthy configuration\\n    health_bonus = self.healthy_reward\\n\\n    # Introduce a bonus for alternating velocity patterns (imitating dynamic steps)\\n    dynamic_vel_bonus = np.sin(self.data.qvel[2]) * np.cos(self.data.qvel[3]) \\\\\\n                        + np.sin(self.data.qvel[4]) * np.cos(self.data.qvel[5])\\n\\n    # Total reward calculation\\n    reward = forward_reward + 0.5 * sinusoidal_reward + 0.3 * dynamic_vel_bonus \\\\\\n             + health_bonus - control_penalty\\n\\n    # Reward breakdown for debugging and analysis\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'sinusoidal_reward\\': sinusoidal_reward,\\n        \\'control_penalty\\': control_penalty,\\n        \\'health_bonus\\': health_bonus,\\n        \\'dynamic_vel_bonus\\': dynamic_vel_bonus\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 1050.9375157422257\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for maintaining a specific effective speed range to encourage optimal pacing\\n    optimal_velocity_reward = -np.square(x_velocity - 2.0)  # Optimal speed considered to be 2.0\\n\\n    # Penalize excessive control inputs to maintain energy efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Reward for staying upright and healthy, ensuring the walker does not fall\\n    health_bonus = self.healthy_reward\\n\\n    # Encourage dynamism by penalizing very low velocities to avoid stagnation\\n    dynamic_bonus = -np.exp(-x_velocity)\\n\\n    # Assembling the total reward\\n    reward = optimal_velocity_reward + health_bonus - control_penalty + dynamic_bonus\\n\\n    # Break down the rewards to allow tracking of what influences the overall reward\\n    reward_info = {\\n        \\'optimal_velocity_reward\\': optimal_velocity_reward,\\n        \\'control_penalty\\': control_penalty,\\n        \\'health_bonus\\': health_bonus,\\n        \\'dynamic_bonus\\': dynamic_bonus\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 389.0564530209492\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward motion, scaled exponentially to favor higher speeds but with diminishing returns\\n    forward_reward = np.exp(0.3 * x_velocity) - 1\\n\\n    # Penalty to discourage excessive use of actuator torques, promoting energy efficiency\\n    control_penalty = self.control_cost(action)\\n    \\n    # Bonus for maintaining a healthy mechanics of motion, which includes staying upright    \\n    health_bonus = self.healthy_reward\\n    \\n    # Additional reward for synchronous and rhythmic leg movements\\n    # We can leverage the sine of the sum of relevant joint angles to promote a smooth cyclic locomotion\\n    angles = self.data.qpos[2:7]  # Assuming indices 2-6 are joint angles\\n    rhythmic_movement_bonus = np.sum(np.sin(angles))\\n\\n    # Compute total reward considering all the components\\n    reward = forward_reward + rhythmic_movement_bonus + health_bonus - control_penalty\\n    \\n    # Organize detailed reward components for diagnostic purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'rhythmic_movement_bonus\\': rhythmic_movement_bonus,\\n        \\'health_bonus\\': health_bonus,\\n        \\'control_penalty\\': control_penalty\\n    }\\n    \\n    return reward, reward_info\\n\\nfitness: 1482.5987609224244\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward motion, with an additional focus on accelerating from a standstill which encourages burst speeds\\n    if x_velocity < 1:\\n        acceleration_reward = 5 * x_velocity  # Significantly encourage starting from rest\\n    else:\\n        acceleration_reward = np.log1p(x_velocity)  # Gradually increase the reward but with diminishing returns after reaching a stable pace\\n    \\n    # Penalizing the actions squared to preserve energy and penalize large and frequent changes in the joints\\n    control_penalty = self.control_cost(action)\\n    \\n    # Health bonus that encourages maintaining a natural, upright healthy pose\\n    health_bonus = self.healthy_reward\\n    \\n    # Bonus for smoothness in motion which can be conceptualized by minimizing the fluctuation of velocities\\n    # A softer change in acceleration or velocity minimizes the jerky motion and promotes fluid walking\\n    prev_vel_diff = np.abs(self.data.qvel[:-1] - self.data.qvel[1:])\\n    smoothness_bonus = -np.sum(np.square(prev_vel_diff))\\n    \\n    # Total reward synthesis\\n    reward = acceleration_reward + smoothness_bonus + health_bonus - control_penalty\\n    \\n    # Structuring the reward information for possible debugging and understanding the contributions of different parts\\n    reward_info = {\\n        \\'acceleration_reward\\': acceleration_reward,\\n        \\'smoothness_bonus\\': smoothness_bonus,\\n        \\'health_bonus\\': health_bonus,\\n        \\'control_penalty\\': control_penalty\\n    }\\n    \\n    return reward, reward_info\\n\\nfitness: 882.6402867898502\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward motion, scaled exponentially to favor higher speeds but with diminishing returns\\n    forward_reward = np.exp(0.3 * x_velocity) - 1\\n\\n    # Penalty to discourage excessive use of actuator torques, promoting energy efficiency\\n    control_penalty = self.control_cost(action)\\n    \\n    # Bonus for maintaining a healthy mechanics of motion, which includes staying upright    \\n    health_bonus = self.healthy_reward\\n    \\n    # Additional reward for synchronous and rhythmic leg movements\\n    # We can leverage the sine of the sum of relevant joint angles to promote a smooth cyclic locomotion\\n    angles = self.data.qpos[2:7]  # Assuming indices 2-6 are joint angles\\n    rhythmic_movement_bonus = np.sum(np.sin(angles))\\n\\n    # Compute total reward considering all the components\\n    reward = forward_reward + rhythmic_movement_bonus + health_bonus - control_penalty\\n    \\n    # Organize detailed reward components for diagnostic purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'rhythmic_movement_bonus\\': rhythmic_movement_bonus,\\n        \\'health_bonus\\': health_bonus,\\n        \\'control_penalty\\': control_penalty\\n    }\\n    \\n    return reward, reward_info\\n\\nbest fitness: 1482.5987609224244\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVvSq0ODBz6EoR4zQhxPXa8kjl09U', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Base reward for forward motion, encouraging maximal speed\\n    forward_reward = np.log1p(x_velocity)  # Encouraging all velocities with diminishing returns for higher speeds\\n\\n    # Control penalty to discourage excessive actuator usage while promoting efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Health bonus to ensure that the walker remains in a healthy position\\n    health_bonus = self.healthy_reward\\n\\n    # Penalty for abrupt changes in joint angles, promoting smoother movements\\n    joint_angle_diff = np.sum(np.abs(np.diff(self.data.qpos[2:7])))  # Indexes for joint angles\\n    smoothness_penalty = -0.5 * joint_angle_diff  # Encouraging less fluctuation in angles\\n\\n    # Reward for maintaining a consistent forward velocity, discouraging erratic movements\\n    velocity_consistency_bonus = -np.sum(np.square(np.diff(self.data.qvel)))  # Penalizing rapid changes in velocity\\n\\n    # Total reward synthesis\\n    reward = forward_reward + health_bonus - control_penalty + smoothness_penalty + velocity_consistency_bonus\\n\\n    # Structure the reward information for clarity in analysis\\n    reward_info = {\\n        'forward_reward': forward_reward,\\n        'control_penalty': control_penalty,\\n        'health_bonus': health_bonus,\\n        'smoothness_penalty': smoothness_penalty,\\n        'velocity_consistency_bonus': velocity_consistency_bonus\\n    }\\n\\n    return reward, reward_info\\n```\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746948760, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=316, prompt_tokens=3307, total_tokens=3623, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 3.51637815963908\n",
      "Initial parameter:[ 1.     0.45   0.25   0.05  -0.05  -0.3    0.02   0.025  0.03   0.015]\n",
      "[1.5, 1.0, 0.7, 0.3, 0.05, -0.4, 0.04, 0.03, 0.02, 0.01]\n",
      "Successfully saved GPTWalker_refine2_1_11_1.xml\n",
      "improved_morphology:[1.5, 1.0, 0.7, 0.3, 0.05, -0.4, 0.04, 0.03, 0.02, 0.01]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 1.8934089576800408\n",
      "improved_efficiency:560.208325160499\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nDescription: The walker is a two-dimensional bipedal robot consisting of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs below the thighs, and two feet attached to the legs on which the entire body rests. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass Walker2dEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"walker2d_v5.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.8, 2.0),\\n        healthy_angle_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n        is_healthy = healthy_z and healthy_angle\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n\\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for moving forward, the primary objective\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Penalty for using too much control or torque, which promotes efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Checking if the walker is within the healthy range of operation\\n    health_bonus = self.healthy_reward\\n\\n    # Total reward calculation\\n    reward = forward_reward + health_bonus - control_penalty\\n\\n    # Reward breakdown for debugging and analysis\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'control_penalty\\': control_penalty,\\n        \\'health_bonus\\': health_bonus\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 837.9291798228314\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for moving forward, emphasizing faster and smoother motions\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Encourage alternating leg movement by rewarding sinusoidal pattern in hinge angles\\n    # This promotes a more natural walking gait\\n    qpos = self.data.qpos\\n    sinusoidal_reward = np.sin(qpos[2]) * np.cos(qpos[3]) + np.sin(qpos[4]) * np.cos(qpos[5])\\n\\n    # Penalize excessive control inputs to maintain efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Reward for staying in a healthy configuration\\n    health_bonus = self.healthy_reward\\n\\n    # Introduce a bonus for alternating velocity patterns (imitating dynamic steps)\\n    dynamic_vel_bonus = np.sin(self.data.qvel[2]) * np.cos(self.data.qvel[3]) \\\\\\n                        + np.sin(self.data.qvel[4]) * np.cos(self.data.qvel[5])\\n\\n    # Total reward calculation\\n    reward = forward_reward + 0.5 * sinusoidal_reward + 0.3 * dynamic_vel_bonus \\\\\\n             + health_bonus - control_penalty\\n\\n    # Reward breakdown for debugging and analysis\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'sinusoidal_reward\\': sinusoidal_reward,\\n        \\'control_penalty\\': control_penalty,\\n        \\'health_bonus\\': health_bonus,\\n        \\'dynamic_vel_bonus\\': dynamic_vel_bonus\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 1050.9375157422257\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for maintaining a specific effective speed range to encourage optimal pacing\\n    optimal_velocity_reward = -np.square(x_velocity - 2.0)  # Optimal speed considered to be 2.0\\n\\n    # Penalize excessive control inputs to maintain energy efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Reward for staying upright and healthy, ensuring the walker does not fall\\n    health_bonus = self.healthy_reward\\n\\n    # Encourage dynamism by penalizing very low velocities to avoid stagnation\\n    dynamic_bonus = -np.exp(-x_velocity)\\n\\n    # Assembling the total reward\\n    reward = optimal_velocity_reward + health_bonus - control_penalty + dynamic_bonus\\n\\n    # Break down the rewards to allow tracking of what influences the overall reward\\n    reward_info = {\\n        \\'optimal_velocity_reward\\': optimal_velocity_reward,\\n        \\'control_penalty\\': control_penalty,\\n        \\'health_bonus\\': health_bonus,\\n        \\'dynamic_bonus\\': dynamic_bonus\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 389.0564530209492\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward motion, scaled exponentially to favor higher speeds but with diminishing returns\\n    forward_reward = np.exp(0.3 * x_velocity) - 1\\n\\n    # Penalty to discourage excessive use of actuator torques, promoting energy efficiency\\n    control_penalty = self.control_cost(action)\\n    \\n    # Bonus for maintaining a healthy mechanics of motion, which includes staying upright    \\n    health_bonus = self.healthy_reward\\n    \\n    # Additional reward for synchronous and rhythmic leg movements\\n    # We can leverage the sine of the sum of relevant joint angles to promote a smooth cyclic locomotion\\n    angles = self.data.qpos[2:7]  # Assuming indices 2-6 are joint angles\\n    rhythmic_movement_bonus = np.sum(np.sin(angles))\\n\\n    # Compute total reward considering all the components\\n    reward = forward_reward + rhythmic_movement_bonus + health_bonus - control_penalty\\n    \\n    # Organize detailed reward components for diagnostic purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'rhythmic_movement_bonus\\': rhythmic_movement_bonus,\\n        \\'health_bonus\\': health_bonus,\\n        \\'control_penalty\\': control_penalty\\n    }\\n    \\n    return reward, reward_info\\n\\nfitness: 1482.5987609224244\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward motion, with an additional focus on accelerating from a standstill which encourages burst speeds\\n    if x_velocity < 1:\\n        acceleration_reward = 5 * x_velocity  # Significantly encourage starting from rest\\n    else:\\n        acceleration_reward = np.log1p(x_velocity)  # Gradually increase the reward but with diminishing returns after reaching a stable pace\\n    \\n    # Penalizing the actions squared to preserve energy and penalize large and frequent changes in the joints\\n    control_penalty = self.control_cost(action)\\n    \\n    # Health bonus that encourages maintaining a natural, upright healthy pose\\n    health_bonus = self.healthy_reward\\n    \\n    # Bonus for smoothness in motion which can be conceptualized by minimizing the fluctuation of velocities\\n    # A softer change in acceleration or velocity minimizes the jerky motion and promotes fluid walking\\n    prev_vel_diff = np.abs(self.data.qvel[:-1] - self.data.qvel[1:])\\n    smoothness_bonus = -np.sum(np.square(prev_vel_diff))\\n    \\n    # Total reward synthesis\\n    reward = acceleration_reward + smoothness_bonus + health_bonus - control_penalty\\n    \\n    # Structuring the reward information for possible debugging and understanding the contributions of different parts\\n    reward_info = {\\n        \\'acceleration_reward\\': acceleration_reward,\\n        \\'smoothness_bonus\\': smoothness_bonus,\\n        \\'health_bonus\\': health_bonus,\\n        \\'control_penalty\\': control_penalty\\n    }\\n    \\n    return reward, reward_info\\n\\nfitness: 882.6402867898502\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for moving forward, emphasizing faster and smoother motions\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Encourage alternating leg movement by rewarding sinusoidal pattern in hinge angles\\n    # This promotes a more natural walking gait\\n    qpos = self.data.qpos\\n    sinusoidal_reward = np.sin(qpos[2]) * np.cos(qpos[3]) + np.sin(qpos[4]) * np.cos(qpos[5])\\n\\n    # Penalize excessive control inputs to maintain efficiency\\n    control_penalty = self.control_cost(action)\\n\\n    # Reward for staying in a healthy configuration\\n    health_bonus = self.healthy_reward\\n\\n    # Introduce a bonus for alternating velocity patterns (imitating dynamic steps)\\n    dynamic_vel_bonus = np.sin(self.data.qvel[2]) * np.cos(self.data.qvel[3]) \\\\\\n                        + np.sin(self.data.qvel[4]) * np.cos(self.data.qvel[5])\\n\\n    # Total reward calculation\\n    reward = forward_reward + 0.5 * sinusoidal_reward + 0.3 * dynamic_vel_bonus \\\\\\n             + health_bonus - control_penalty\\n\\n    # Reward breakdown for debugging and analysis\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'sinusoidal_reward\\': sinusoidal_reward,\\n        \\'control_penalty\\': control_penalty,\\n        \\'health_bonus\\': health_bonus,\\n        \\'dynamic_vel_bonus\\': dynamic_vel_bonus\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 1482.5987609224244\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVvbHbgN2ITZRcRr76EXmg8uYNrJP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encouraging forward motion with an exponential scaling to prioritize high velocities\\n    forward_reward = np.exp(self._forward_reward_weight * x_velocity) - 1\\n    \\n    # Bonus for maintaining a rhythmic walking pattern by analyzing the torque applied to joints\\n    torque_magnitude = np.sum(np.abs(action))  # Total absolute torque applied\\n    rhythmic_bonus = -np.abs(torque_magnitude - np.mean(np.abs(action)))  # Favor smooth and controlled torque\\n    \\n    # Health bonus rewarding upright and balanced configurations\\n    health_bonus = self.healthy_reward\\n    \\n    # Penalizing rapid changes in velocity which could lead to jerky movements\\n    velocity_change = np.abs(x_velocity - self.data.qvel[0])  # Current and previous velocity\\n    smoothness_penalty = -np.square(velocity_change)  # Reward for smooth transitions\\n\\n    # Total reward synthesis\\n    reward = forward_reward + 0.5 * rhythmic_bonus + health_bonus + smoothness_penalty\\n    \\n    # Structuring the reward information for debugging\\n    reward_info = {\\n        'forward_reward': forward_reward,\\n        'rhythmic_bonus': rhythmic_bonus,\\n        'health_bonus': health_bonus,\\n        'smoothness_penalty': smoothness_penalty,\\n    }\\n    \\n    return reward, reward_info\\n```\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746949283, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_ded0d14823', usage=CompletionUsage(completion_tokens=283, prompt_tokens=3361, total_tokens=3644, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 3.0270111901406\n",
      "Initial parameter:[ 1.     0.45   0.25   0.05  -0.05  -0.3    0.02   0.025  0.03   0.015]\n",
      "[1.5, 1.0, 0.7, 0.3, 0.4, -0.4, 0.05, 0.05, 0.04, 0.03]\n",
      "Successfully saved GPTWalker_refine2_4_11_1.xml\n",
      "improved_morphology:[1.5, 1.0, 0.7, 0.3, 0.4, -0.4, 0.05, 0.05, 0.04, 0.03]\n"
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_walker_volume(parameter)\n",
    "    \n",
    "    print(f\"Initial parameter:{parameter}\")\n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        iteration +=1\n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list,\n",
    "            efficiency_matrix_select[rewardfunc_index, :],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "            \n",
    "        )\n",
    "        print(f\"improved_morphology:{improved_parameter}\")\n",
    "        \n",
    "        shutil.copy(improved_morphology, \"GPTWalker.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "        # model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_walker_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "        print(f\"improved_efficiency:{improved_efficiency}\")\n",
    "        \n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            \n",
    "            \n",
    "        # -------- 优化 reward function --------\n",
    "        iteration +=1\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            rewardfunc_list,\n",
    "            efficiency_matrix_select[:, morphology_index],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "\n",
    "        shutil.copy(best_morphology, \"GPTWalker.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_walker_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_11.xml',\n",
       "  'best_parameter': array([ 1.   ,  0.45 ,  0.25 ,  0.05 , -0.05 , -0.3  ,  0.02 ,  0.025,\n",
       "          0.03 ,  0.015]),\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_3.py',\n",
       "  'best_fitness': 5.010931913179973,\n",
       "  'best_material': 0.0033798300964870192,\n",
       "  'best_efficiency': 1482.5987609224244,\n",
       "  'best_iteration': 2},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_11.xml',\n",
       "  'best_parameter': array([ 1.   ,  0.45 ,  0.25 ,  0.05 , -0.05 , -0.3  ,  0.02 ,  0.025,\n",
       "          0.03 ,  0.015]),\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_1.py',\n",
       "  'best_fitness': 3.551990245232875,\n",
       "  'best_material': 0.0033798300964870192,\n",
       "  'best_efficiency': 1050.9375157422257,\n",
       "  'best_iteration': 2},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_11.xml',\n",
       "  'best_parameter': array([ 1.   ,  0.45 ,  0.25 ,  0.05 , -0.05 , -0.3  ,  0.02 ,  0.025,\n",
       "          0.03 ,  0.015]),\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_4.py',\n",
       "  'best_fitness': 2.9831742056642696,\n",
       "  'best_material': 0.0033798300964870192,\n",
       "  'best_efficiency': 882.6402867898502,\n",
       "  'best_iteration': 2},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_refine2_0_11_1.xml',\n",
       "  'best_parameter': [2.0, 1.5, 1.0, 0.4, 0.5, -0.5, 0.07, 0.06, 0.05, 0.03],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_0.py',\n",
       "  'best_fitness': 2.8382854859709084,\n",
       "  'best_material': 0.0033798300964870192,\n",
       "  'best_efficiency': 839.7716467821889,\n",
       "  'best_iteration': 4},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_refine2_3_7_1.xml',\n",
       "  'best_parameter': [2.0, 1.5, 1.0, 0.4, 0.3, -0.5, 0.06, 0.05, 0.04, 0.03],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_3.py',\n",
       "  'best_fitness': 1.88729369678452,\n",
       "  'best_material': 0.0038123226851312143,\n",
       "  'best_efficiency': 495.05087912555916,\n",
       "  'best_iteration': 4},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_2.xml',\n",
       "  'best_parameter': array([ 1.   ,  0.7  ,  0.5  ,  0.2  ,  0.25 , -0.15 ,  0.045,  0.035,\n",
       "          0.025,  0.015]),\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_0.py',\n",
       "  'best_fitness': 2.7830358346106645,\n",
       "  'best_material': 0.006091548155310609,\n",
       "  'best_efficiency': 456.8683959568497,\n",
       "  'best_iteration': 2}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 06:13:50,208 - Final optimized result: rewardfunc_index0 morphology_index0\n",
    "2025-04-07 06:13:50,208 -   Morphology: results/Div_m25_r5/assets/GPTWalker_refine_0_0_0.xml\n",
    "2025-04-07 06:13:50,208 -   Parameter: [1.05, 0.55, 0.3, 0.1, 0.3, -0.25, 0.015, 0.025, 0.03, 0.015]\n",
    "2025-04-07 06:13:50,208 -   Rewardfunc: results/Div_m25_r5/env/GPTWalker_0_0_1.py\n",
    "2025-04-07 06:13:50,208 -   Fitness: 6.466572426996648\n",
    "2025-04-07 06:13:50,208 -   Material: 0.003643200280612963\n",
    "2025-04-07 06:13:50,208 -   Efficiency: 1774.970336220071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'best_morphology': 'results/Div_m25_r5/assets/GPTWalker_11.xml',\n",
    "  'best_parameter': array([ 1.   ,  0.45 ,  0.25 ,  0.05 , -0.05 , -0.3  ,  0.02 ,  0.025,\n",
    "          0.03 ,  0.015]),\n",
    "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_1_11_1.py',\n",
    "  'best_fitness': 14.883819809931813,\n",
    "  'best_material': 0.0033798300964870192,\n",
    "  'best_efficiency': 4403.718348269042,\n",
    "  'best_iteration': 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign best 1e6 steps train\n",
      "\n",
      "fitness:3.4504453808491076\n",
      "efficiency:1020.8931462074042\n"
     ]
    }
   ],
   "source": [
    "# Robodesign best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_11.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_refine_1_11_1.py\"\n",
    "\n",
    "morphology_index=9999\n",
    "rewardfunc_index=9999\n",
    "\n",
    "parameter = [ 1.   ,  0.45 ,  0.25 ,  0.05 , -0.05 , -0.3  ,  0.02 ,  0.025,\n",
    "          0.03 ,  0.015]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Robodesign best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_refine.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_3.py\"\n",
    "\n",
    "\n",
    "\n",
    "# morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "# rewardfunc = \"results/Div_m25_r5/env/GPTWalker_0_0_1.py\"\n",
    "\n",
    "morphology_index=9998\n",
    "rewardfunc_index=9998\n",
    "\n",
    "\n",
    "parameter = [0.95, 0.75, 0.45, 0.15, 0.2, -0.2, 0.01, 0.01, 0.01, 0.008]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009984965818806487"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter = [ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02, 0.025,  0.01 ]\n",
    "parameter = [1.1, 0.75, 0.45, 0.15, 0.2, -0.2, 0.015, 0.012, 0.012, 0.008]\n",
    "material = compute_walker_volume(parameter)\n",
    "material\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Robodesign best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_refine.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "\n",
    "\n",
    "# morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "# rewardfunc = \"results/Div_m25_r5/env/GPTWalker_0_0_1.py\"\n",
    "\n",
    "morphology_index=999\n",
    "rewardfunc_index=999\n",
    "\n",
    "# parameter = [1.05, 0.55, 0.3, 0.1, 0.3, -0.25, 0.015, 0.025, 0.03, 0.015]\n",
    "# parameter = [ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02, 0.025,  0.01 ]\n",
    "\n",
    "parameter = [0.95, 0.75, 0.45, 0.15, 0.2, -0.2, 0.01, 0.01, 0.01, 0.008]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "# fitness, _ = Eva(model_path)\n",
    "fitness, _ = Eva_with_qpos_logging(model_path, run_steps=10, video = True, rewardfunc_index=rewardfunc_index, morphology_index=morphology_index)\n",
    "\n",
    "\n",
    "\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 01:54:53,368 - Initial morphology:results/Div_m25_r5/assets/GPTWalker_11.xml\n",
    "2025-04-07 01:54:53,368 - Initial parameter:[ 1.     0.45   0.25   0.05  -0.05  -0.3    0.02   0.025  0.03   0.015]\n",
    "2025-04-07 01:54:53,368 - Initial rewardfunc:results/Div_m25_r5/env/GPTrewardfunc_3.py\n",
    "2025-04-07 01:54:53,368 - Initial fitness:5.010931913179973\n",
    "2025-04-07 01:54:53,368 - Initial efficiency:1482.5987609224244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign coarse best 1e6 steps train\n",
      "\n",
      "fitness:3.819145605463566\n",
      "efficiency:1129.9815364781707\n"
     ]
    }
   ],
   "source": [
    "# coarse best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_11.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_3.py\"\n",
    "\n",
    "morphology_index=888\n",
    "rewardfunc_index=888\n",
    "\n",
    "parameter = [ 1.    , 0.45  , 0.25 ,  0.05  ,-0.05  ,-0.3  ,  0.02  , 0.025  ,0.03  , 0.015]\n",
    "\n",
    "morphology_index=11\n",
    "rewardfunc_index=3\n",
    "\n",
    "# morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "# rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_3.py\"\n",
    "# morphology_index=0\n",
    "# rewardfunc_index=3\n",
    "# parameter =[ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 ,\n",
    "#         0.025,  0.01 ]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/coarse/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign coarse best best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign coarse best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "fitness 8.74718176148099\n",
      "Saved qpos log to /root/MML/Walker2D/qpos.txt\n",
      "Average Fitness: 4.4272, Average Reward: 995.8090\n",
      "Robodesign human 1e6 steps train\n",
      "\n",
      "fitness:4.427216305766132\n",
      "efficiency:157.8140339392407\n"
     ]
    }
   ],
   "source": [
    "# human\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "\n",
    "morphology_index=666\n",
    "rewardfunc_index=666\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "# fitness, _ = Eva(model_path)\n",
    "fitness, _ = Eva_with_qpos_logging(model_path, run_steps=10, video = True, rewardfunc_index=rewardfunc_index, morphology_index=morphology_index)\n",
    "\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign human 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign human 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 11:08:16,602 - morphology: 50, rewardfunc: 1, material cost: 0.10202845741308451 reward: 536.5836396066536 fitness: 2.282395280964088 efficiency: 22.37018317079236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Morphology Design) 1e6 steps train\n",
      "\n",
      "fitness:1.912675745085834\n",
      "efficiency:68.17987965860266\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Morphology Design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_1.py\"\n",
    "\n",
    "morphology_index=555\n",
    "rewardfunc_index=555\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign (w/o Morphology Design) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Morphology Design) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 08:36:37,117 - morphology: 0, rewardfunc: 10, material cost: 0.0023017402175301216 reward: 533.5658482194434 fitness: 3.5383881162370003 efficiency: 1537.2664948409606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 ,\n",
       "        0.025,  0.01 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      " Robodesign (w/o Reward Design) 1e6 steps train\n",
      "\n",
      "fitness:3.273572012442703\n",
      "efficiency:1422.2161074091166\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Reward Design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "\n",
    "morphology_index=444\n",
    "rewardfunc_index=444\n",
    "\n",
    "morphology_index=0\n",
    "rewardfunc_index=10\n",
    "parameter =[ 1.   ,  0.5  ,  0.25 ,  0.05 ,  0.25 , -0.25 ,  0.015,  0.02 ,\n",
    "        0.025,  0.01 ]\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "model_path = f\"results/Div_m25_r5/coarse/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" Robodesign (w/o Reward Design) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" Robodesign (w/o Reward Design) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "025-04-08 01:07:15,915 - Initial morphology:results/noDiv_m25_r5/assets/GPTWalker_20.xml\n",
    "2025-04-08 01:07:15,916 - Initial parameter:[ 0.6    0.45   0.3    0.15   0.1   -0.1    0.04   0.04   0.035  0.03 ]\n",
    "2025-04-08 01:07:15,916 - Initial rewardfunc:results/noDiv_m25_r5/env/GPTrewardfunc_1.py\n",
    "2025-04-08 01:07:15,916 - Initial fitness:2.539201719144284\n",
    "2025-04-08 01:07:15,916 - Initial efficiency:427.68481006358724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      " Robodesign (w/o diversity reflection) 1e6 steps train\n",
      "\n",
      "fitness:5.337485313971282\n",
      "efficiency:899.007501259998\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o diversity reflection)\n",
    "\n",
    "morphology = \"results/noDiv_m25_r5/assets/GPTWalker_20.xml\"\n",
    "rewardfunc = \"results/noDiv_m25_r5/env/GPTrewardfunc_1.py\"\n",
    "\n",
    "morphology_index=333\n",
    "rewardfunc_index=333\n",
    "\n",
    "parameter = [ 0.6 ,   0.45,   0.3    ,0.15 ,  0.1 ,  -0.1  ,  0.04 ,  0.04   ,0.035 , 0.03 ]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" Robodesign  (w/o diversity reflection) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" Robodesign (w/o diversity reflection) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "fitness 1.1383605759929971\n",
      "Saved qpos log to /root/MML/Walker2D/qpos.txt\n",
      "Average Fitness: 0.1414, Average Reward: 444.0662\n",
      "# eureka reward 1e6 steps train\n",
      "\n",
      "fitness:0.1414111423563164\n",
      "efficiency:5.040788901626663\n"
     ]
    }
   ],
   "source": [
    "# eureka reward\n",
    "morphology = \"results/eureka/assets/GPTWalker_0.xml\"\n",
    "rewardfunc = \"results/eureka/env/GPTrewardfunc_2_1.py\"\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "# fitness, _ = Eva(model_path)\n",
    "fitness, _ = Eva_with_qpos_logging(model_path, run_steps=10, video = True, rewardfunc_index=rewardfunc_index, morphology_index=morphology_index)\n",
    "\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"# eureka reward 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"# eureka reward 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Saved qpos log to /root/MML/Walker2D/qpos.txt\n",
      "Average Fitness: 3.6028, Average Reward: 718.2425\n",
      "eureka morphology 1e6 steps train\n",
      "\n",
      "fitness:3.602824393752411\n",
      "efficiency:2411.9194215410907\n"
     ]
    }
   ],
   "source": [
    "# eureka morphology\n",
    "\n",
    "morphology = \"results/eureka_morphology/assets/GPTWalker_5.xml\"\n",
    "rewardfunc = \"results/eureka_morphology/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "parameter = [ 1.25   , 1.0 ,  0.75 ,   0.45  , 0.4 , -0.4  ,  0.025  , 0.018 ,  0.01 , 0.006 ]\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "# fitness, _ = Eva(model_path)\n",
    "fitness, _ = Eva_with_qpos_logging(model_path, run_steps=10, video = True, rewardfunc_index=rewardfunc_index, morphology_index=morphology_index)\n",
    "\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka morphology 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka morphology 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robodesign (w/o Reward Design w/o morphology design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTWalker_0.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=123\n",
    "rewardfunc_index=123\n",
    "\n",
    "parameter = parameter_list[0]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTWalker.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTWalkerEnv._get_rew = _get_rew\n",
    "        \n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_walker_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" (w/o Reward Design w/o morphology design) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" (w/o Reward Design w/o morphology design) 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
