{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTAnt import GPTAntEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=\"api_key\")\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "        \n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTAnt_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        # env_path = os.path.join(os.path.dirname(__file__), \"env\", \"ant_v5.py\")\n",
    "        # with open(env_path, \"r\") as f:\n",
    "        #     env_content = f.read().rstrip()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums\n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_ant_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = ant_design(parameter)  \n",
    "            filename = f\"GPTAnt_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_ant_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(initial_parameter)})\n",
    "\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "\n",
    "        xml_file = ant_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTAnt_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_ant_volume(diverse_parameter['parameters']))\n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistant\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = ant_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTAnt_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\" \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\":reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine2_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "    \n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\"best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\" + \"this is helpful [0.045, 0.22, 0.11,0.31,0.055, 0.055,0.055,0.012, 0.012,0.012]\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = ant_design(parameter)  \n",
    "        filename = f\"GPTAnt_refine2_{step}_{rewardfunc_index}_{morphology_index}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"parameters_refine_only.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 26\n",
    "rewardfunc_nums = 6\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n",
    "\n",
    "\n",
    "\n",
    "# return file list of morphology and reward function: [GPTAnt_{i}.xml] and [GPTAnt_{j}.py]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.2, 0.3, 0.06, 0.2, 0.15, 0.1, 0.05, 0.02, 0.02, 0.015]\n",
      "params: [0.25, 0.35, 0.08, 0.25, 0.2, 0.15, 0.1, 0.03, 0.025, 0.02]\n",
      "params: [0.15, 0.2, 0.05, 0.15, 0.1, 0.08, 0.04, 0.015, 0.015, 0.01]\n",
      "params: [0.3, 0.45, 0.1, 0.4, 0.3, 0.25, 0.15, 0.05, 0.04, 0.03]\n",
      "params: [0.18, 0.25, 0.04, 0.12, 0.09, 0.06, 0.03, 0.02, 0.018, 0.015]\n",
      "params: [0.35, 0.5, 0.1, 0.25, 0.05, 0.15, 0.02, 0.04, 0.03, 0.025]\n",
      "params: [0.45, 0.6, 0.15, 0.35, 0.2, 0.25, 0.1, 0.07, 0.06, 0.05]\n",
      "params: [0.2, 0.1, 0.3, 0.2, 0.4, 0.15, 0.35, 0.015, 0.025, 0.02]\n",
      "params: [0.22, 0.18, 0.07, 0.1, 0.05, 0.12, 0.06, 0.025, 0.02, 0.015]\n",
      "params: [0.1, 0.15, 0.02, 0.3, 0.1, 0.05, 0.02, 0.01, 0.015, 0.01]\n",
      "params: [0.4, 0.25, 0.06, 0.2, 0.05, 0.3, 0.15, 0.03, 0.025, 0.02]\n",
      "params: [0.25, 0.1, 0.2, 0.15, 0.3, 0.1, 0.2, 0.02, 0.03, 0.02]\n",
      "params: [0.3, 0.15, 0.1, 0.05, 0.2, 0.08, 0.04, 0.02, 0.015, 0.01]\n",
      "params: [0.5, 0.2, 0.12, 0.25, 0.08, 0.3, 0.1, 0.04, 0.03, 0.025]\n",
      "params: [0.15, 0.25, 0.08, 0.12, 0.07, 0.15, 0.09, 0.015, 0.018, 0.012]\n",
      "params: [0.35, 0.05, 0.3, 0.1, 0.15, 0.05, 0.1, 0.02, 0.02, 0.015]\n",
      "params: [0.12, 0.4, 0.15, 0.45, 0.3, 0.1, 0.05, 0.02, 0.01, 0.01]\n",
      "params: [0.5, 0.05, 0.05, 0.3, 0.02, 0.25, 0.02, 0.03, 0.04, 0.03]\n",
      "params: [0.55, 0.1, 0.1, 0.3, 0.02, 0.2, 0.02, 0.04, 0.04, 0.04]\n",
      "params: [0.3, 0.4, 0.1, 0.2, 0.2, 0.15, 0.15, 0.04, 0.03, 0.03]\n",
      "params: [0.2, 0.15, 0.05, 0.1, 0.08, 0.1, 0.08, 0.015, 0.015, 0.01]\n",
      "params: [0.1, 0.05, 0.25, 0.4, 0.05, 0.2, 0.1, 0.02, 0.02, 0.015]\n",
      "params: [0.3, 0.1, 0.2, 0.35, 0.15, 0.1, 0.05, 0.025, 0.02, 0.015]\n",
      "params: [0.15, 0.05, 0.1, 0.4, 0.15, 0.4, 0.2, 0.01, 0.03, 0.02]\n",
      "params: [0.1, 0.2, 0.2, 0.2, 0.05, 0.3, 0.1, 0.02, 0.02, 0.01]\n",
      "params: [0.08, 0.2, 0.05, 0.05, 0.25, 0.3, 0.15, 0.012, 0.015, 0.012]\n",
      "params: [0.15, 0.25, 0.08, 0.35, 0.2, 0.25, 0.15, 0.02, 0.025, 0.02]\n",
      "params: [0.5, 0.35, 0.25, 0.15, 0.1, 0.05, 0.03, 0.025, 0.02, 0.015]\n",
      "params: [0.12, 0.08, 0.04, 0.3, 0.02, 0.25, 0.1, 0.01, 0.015, 0.015]\n",
      "params: [0.2, 0.1, 0.3, 0.05, 0.15, 0.08, 0.04, 0.015, 0.01, 0.01]\n",
      "params: [0.4, 0.45, 0.2, 0.5, 0.3, 0.6, 0.4, 0.05, 0.04, 0.035]\n",
      "params: [0.15, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0.02, 0.015, 0.015]\n",
      "params: [0.2, 0.3, 0.4, 0.2, 0.1, 0.15, 0.05, 0.025, 0.02, 0.015]\n",
      "params: [0.25, 0.1, 0.15, 0.12, 0.08, 0.05, 0.03, 0.02, 0.015, 0.012]\n",
      "params: [0.18, 0.05, 0.12, 0.03, 0.2, 0.075, 0.03, 0.01, 0.015, 0.01]\n",
      "params: [0.1, 0.35, 0.25, 0.08, 0.12, 0.2, 0.15, 0.015, 0.02, 0.018]\n",
      "params: [0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.005, 0.005, 0.005]\n",
      "params: [0.3, 0.6, 0.15, 0.1, 0.05, 0.25, 0.1, 0.03, 0.02, 0.015]\n",
      "params: [0.4, 0.1, 0.05, 0.2, 0.2, 0.1, 0.08, 0.02, 0.03, 0.02]\n",
      "params: [0.5, 0.18, 0.12, 0.25, 0.15, 0.3, 0.18, 0.04, 0.035, 0.03]\n",
      "params: [0.12, 0.2, 0.08, 0.35, 0.1, 0.3, 0.2, 0.015, 0.02, 0.015]\n",
      "params: [0.08, 0.4, 0.12, 0.6, 0.25, 0.05, 0.02, 0.01, 0.015, 0.012]\n",
      "params: [0.3, 0.2, 0.4, 0.1, 0.35, 0.2, 0.25, 0.03, 0.04, 0.03]\n",
      "params: [0.3, 0.2, 0.12, 0.15, 0.1, 0.1, 0.05, 0.025, 0.02, 0.015]\n",
      "params: [0.15, 0.35, 0.1, 0.2, 0.05, 0.2, 0.1, 0.02, 0.018, 0.01]\n",
      "params: [0.2, 0.1, 0.05, 0.3, 0.15, 0.05, 0.02, 0.015, 0.01, 0.005]\n",
      "params: [0.05, 0.1, 0.025, 0.1, 0.025, 0.08, 0.02, 0.01, 0.008, 0.006]\n",
      "params: [0.45, 0.2, 0.08, 0.22, 0.12, 0.36, 0.18, 0.045, 0.03, 0.025]\n",
      "params: [0.25, 0.5, 0.75, 0.3, 0.4, 0.2, 0.35, 0.05, 0.06, 0.045]\n",
      "params: [0.35, 0.1, 0.3, 0.2, 0.45, 0.6, 0.05, 0.04, 0.03, 0.025]\n"
     ]
    }
   ],
   "source": [
    "designer = DGA()\n",
    "morphology_list, material_list, parameter_list = designer.generate_morphology_div(morphology_nums, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_0.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_1.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_2.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_3.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_4.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_5.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_6.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_7.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_8.py\n",
      "Saved: results/div2025-03-21_15-03-44/env/GPTrewardfunc_9.py\n"
     ]
    }
   ],
   "source": [
    "designer = DGA()\n",
    "rewardfunc_list = designer.generate_rewardfunc_div(rewardfunc_nums, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficiency_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter coarse optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.1, 0.15, 0.05, 0.15, 0.05, 0.1, 0.05, 0.02, 0.02, 0.02]\n",
      "params: [0.12, 0.2, 0.08, 0.25, 0.08, 0.15, 0.06, 0.025, 0.025, 0.025]\n",
      "params: [0.2, 0.1, 0.15, 0.1, 0.15, 0.05, 0.1, 0.03, 0.03, 0.03]\n",
      "params: [0.25, 0.3, 0.05, 0.3, 0.05, 0.15, 0.05, 0.01, 0.01, 0.01]\n",
      "params: [0.15, 0.25, 0.1, 0.2, 0.07, 0.12, 0.04, 0.035, 0.035, 0.035]\n",
      "params: [0.08, 0.18, 0.06, 0.12, 0.04, 0.1, 0.03, 0.015, 0.015, 0.015]\n",
      "params: [0.1, 0.05, 0.1, 0.05, 0.1, 0.2, 0.1, 0.04, 0.04, 0.04]\n",
      "params: [0.05, 0.1, 0.02, 0.07, 0.02, 0.02, 0.01, 0.015, 0.015, 0.015]\n",
      "params: [0.2, 0.12, 0.2, 0.25, 0.15, 0.18, 0.12, 0.05, 0.05, 0.05]\n",
      "params: [0.05, 0.07, 0.02, 0.04, 0.02, 0.03, 0.01, 0.01, 0.01, 0.01]\n",
      "params: [0.3, 0.05, 0.25, 0.05, 0.25, 0.1, 0.05, 0.02, 0.02, 0.02]\n",
      "params: [0.15, 0.2, 0.08, 0.1, 0.04, 0.15, 0.05, 0.04, 0.03, 0.02]\n",
      "params: [0.2, 0.15, 0.04, 0.18, 0.06, 0.12, 0.03, 0.025, 0.025, 0.025]\n",
      "params: [0.12, 0.18, 0.12, 0.22, 0.08, 0.18, 0.08, 0.02, 0.02, 0.02]\n",
      "params: [0.25, 0.3, 0.15, 0.35, 0.1, 0.25, 0.08, 0.05, 0.04, 0.04]\n",
      "params: [0.05, 0.2, 0.1, 0.3, 0.05, 0.05, 0.05, 0.015, 0.015, 0.015]\n",
      "params: [0.3, 0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.05, 0.05, 0.05]\n",
      "params: [0.1, 0.05, 0.1, 0.05, 0.1, 0.15, 0.15, 0.03, 0.03, 0.03]\n",
      "params: [0.2, 0.08, 0.2, 0.08, 0.2, 0.1, 0.1, 0.01, 0.01, 0.01]\n",
      "params: [0.2, 0.3, 0.05, 0.15, 0.03, 0.2, 0.04, 0.04, 0.02, 0.02]\n",
      "params: [0.15, 0.1, 0.15, 0.08, 0.12, 0.12, 0.08, 0.015, 0.015, 0.015]\n",
      "params: [0.18, 0.07, 0.22, 0.07, 0.22, 0.09, 0.09, 0.03, 0.03, 0.03]\n",
      "params: [0.05, 0.3, 0.1, 0.05, 0.2, 0.05, 0.2, 0.02, 0.02, 0.02]\n",
      "params: [0.13, 0.08, 0.13, 0.12, 0.18, 0.2, 0.24, 0.02, 0.04, 0.04]\n",
      "params: [0.2, 0.12, 0.08, 0.06, 0.04, 0.15, 0.1, 0.01, 0.015, 0.02]\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n"
     ]
    }
   ],
   "source": [
    "morphology_list = [f'results/Div_m25_r5/assets/GPTAnt_{i}.xml' for i in range(0,26) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,6)]\n",
    "\n",
    "parameter_list =[[0.1, 0.15, 0.05, 0.15, 0.05, 0.1, 0.05, 0.02, 0.02, 0.02],\n",
    " [0.12, 0.2, 0.08, 0.25, 0.08, 0.15, 0.06, 0.025, 0.025, 0.025],\n",
    " [0.2, 0.1, 0.15, 0.1, 0.15, 0.05, 0.1, 0.03, 0.03, 0.03],\n",
    " [0.25, 0.3, 0.05, 0.3, 0.05, 0.15, 0.05, 0.01, 0.01, 0.01],\n",
    " [0.15, 0.25, 0.1, 0.2, 0.07, 0.12, 0.04, 0.035, 0.035, 0.035],\n",
    " [0.08, 0.18, 0.06, 0.12, 0.04, 0.1, 0.03, 0.015, 0.015, 0.015],\n",
    " [0.1, 0.05, 0.1, 0.05, 0.1, 0.2, 0.1, 0.04, 0.04, 0.04],\n",
    " [0.05, 0.1, 0.02, 0.07, 0.02, 0.02, 0.01, 0.015, 0.015, 0.015],\n",
    " [0.2, 0.12, 0.2, 0.25, 0.15, 0.18, 0.12, 0.05, 0.05, 0.05],\n",
    " [0.05, 0.07, 0.02, 0.04, 0.02, 0.03, 0.01, 0.01, 0.01, 0.01],\n",
    " [0.3, 0.05, 0.25, 0.05, 0.25, 0.1, 0.05, 0.02, 0.02, 0.02],\n",
    " [0.15, 0.2, 0.08, 0.1, 0.04, 0.15, 0.05, 0.04, 0.03, 0.02],\n",
    " [0.2, 0.15, 0.04, 0.18, 0.06, 0.12, 0.03, 0.025, 0.025, 0.025],\n",
    " [0.12, 0.18, 0.12, 0.22, 0.08, 0.18, 0.08, 0.02, 0.02, 0.02],\n",
    " [0.25, 0.3, 0.15, 0.35, 0.1, 0.25, 0.08, 0.05, 0.04, 0.04],\n",
    " [0.05, 0.2, 0.1, 0.3, 0.05, 0.05, 0.05, 0.015, 0.015, 0.015],\n",
    " [0.3, 0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.05, 0.05, 0.05],\n",
    " [0.1, 0.05, 0.1, 0.05, 0.1, 0.15, 0.15, 0.03, 0.03, 0.03],\n",
    " [0.2, 0.08, 0.2, 0.08, 0.2, 0.1, 0.1, 0.01, 0.01, 0.01],\n",
    " [0.2, 0.3, 0.05, 0.15, 0.03, 0.2, 0.04, 0.04, 0.02, 0.02],\n",
    " [0.15, 0.1, 0.15, 0.08, 0.12, 0.12, 0.08, 0.015, 0.015, 0.015],\n",
    " [0.18, 0.07, 0.22, 0.07, 0.22, 0.09, 0.09, 0.03, 0.03, 0.03],\n",
    " [0.05, 0.3, 0.1, 0.05, 0.2, 0.05, 0.2, 0.02, 0.02, 0.02],\n",
    " [0.13, 0.08, 0.13, 0.12, 0.18, 0.2, 0.24, 0.02, 0.04, 0.04],\n",
    " [0.2, 0.12, 0.08, 0.06, 0.04, 0.15, 0.1, 0.01, 0.015, 0.02],\n",
    "[0.25, 0.2, 0.2, 0.2, 0.2,0.4,0.4, 0.08, 0.08, 0.08 ]]\n",
    "\n",
    "\n",
    "material_list = [compute_ant_volume(parameter) for parameter in parameter_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'morphology_nums:{morphology_nums}')\n",
    "logging.info(f'rewardfunc_nums:{rewardfunc_nums}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'morphology_list:{morphology_list}')\n",
    "logging.info(f'material_list:{material_list}')\n",
    "logging.info(f'_________________________________enter coarse optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 results/Div_m25_r5/env/GPTrewardfunc_5.py\n",
      "25 results/Div_m25_r5/assets/GPTAnt_25.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i, rewardfunc in enumerate(rewardfunc_list):\n",
    "    for j, morphology in enumerate(morphology_list):\n",
    "        # if i not in [0] or j not in [12]:\n",
    "        #     continue\n",
    "        if i not in [5] or j not in [25]:\n",
    "            continue\n",
    "        print(i, rewardfunc)\n",
    "        print(j, morphology)\n",
    "        shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(rewardfunc, \"GPTrewardfunc.py\")         \n",
    "\n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "        env_name = \"GPTAntEnv\"\n",
    "        # model_path = Train(j,  i, folder_name, total_timesteps=5e5)\n",
    "        model_path = folder_name + f\"/coarse/SAC_morphology{j}_rewardfunc{i}_500000.0steps\"\n",
    "        fitness, reward = Eva(model_path)\n",
    "        material = material_list[j]\n",
    "        efficiency = fitness/material\n",
    "        fitness_matrix[i][j] = fitness\n",
    "        efficiency_matrix[i][j] = efficiency\n",
    "        \n",
    "        logging.info(\"___________________finish coarse optimization_____________________\")\n",
    "        logging.info(f\"morphology: {j}, rewardfunc: {i}, material cost: {material} reward: {reward} fitness: {fitness} efficiency: {efficiency}\")\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness = fitness\n",
    "            best_morphology = morphology\n",
    "            best_efficiency = efficiency\n",
    "            best_rewardfunc = rewardfunc\n",
    "            best_material = material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'_________________________________end coarse optimization stage_________________________________')\n",
    "logging.info(f\"Stage1: Final best morphology: {best_morphology}, Fitness: {best_fitness}, best_efficiency: {best_efficiency}, best reward function: {best_rewardfunc}, Material cost: {best_material}, Reward: {best_reward}\")\n",
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'fitness_matrix:{fitness_matrix}')\n",
    "logging.info(f'efficiency_matrix:{efficiency_matrix}')\n",
    "logging.info(f'_________________________________enter fine optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix_select = efficiency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "Initial parameter:[0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "ChatCompletion(id='chatcmpl-BWEA3jxznpYwWtGONbjrsvR5gjqjE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.2, 0.25, 0.15, 0.25, 0.25, 0.3, 0.2, 0.04, 0.06, 0.06],\\n  \"description\": \"Reduced torso size and ankle dimensions while optimizing leg and hip attachment points for efficient movement, targeting improved fitness.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747020631, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=83, prompt_tokens=2943, total_tokens=3026, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.2, 0.25, 0.15, 0.25, 0.25, 0.3, 0.2, 0.04, 0.06, 0.06]\n",
      "Successfully saved GPTAnt_refine2_5_25_1.xml\n",
      "improved parameter [0.2, 0.25, 0.15, 0.25, 0.25, 0.3, 0.2, 0.04, 0.06, 0.06]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.2, 0.25, 0.15, 0.25, 0.25, 0.3, 0.2, 0.04, 0.06, 0.06]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nThe terrain environemnt is a desert terrain with friction=\"0.4 0.005 0.0001\".\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nfitness: 22.35357564162838\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nbest fitness: 22.35357564162838\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BWEGuffsf5brij8wpKWT4IXFIz1xJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for moving forward with emphasis on higher speeds\\n    forward_reward = np.clip(x_velocity, 0, None) * self._forward_reward_weight\\n    \\n    # Reward for being healthy (staying within the healthy z range)\\n    healthy_reward = self.healthy_reward\\n    \\n    # Penalty for applying control (torque) to the joints\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Penalty for contact forces indicating potential instability\\n    contact_cost = self.contact_cost\\n    \\n    # Total rewards combined\\n    total_rewards = forward_reward + healthy_reward\\n\\n    # Reduce total rewards by costs to encourage more efficient movement\\n    costs = ctrl_cost + contact_cost\\n    reward = total_rewards - costs\\n    \\n    # Reward information for logging and analysis\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    # Normalizing the reward to encourage exploration\\n    normalized_reward = np.tanh(reward)\\n\\n    return normalized_reward, reward_info    \\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747021056, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=245, prompt_tokens=2207, total_tokens=2452, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "improved_fitness 4.248257455498679\n",
      "ChatCompletion(id='chatcmpl-BWEOZPS4lNuE7jXxtIVGdkQvnh6VM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.25, 0.15, 0.15, 0.15, 0.15, 0.3, 0.05, 0.06, 0.06, 0.06],\\n  \"description\": \"Reduced sizes of torso, upper attachment points, and hip while optimizing leg and ankle lengths for better fitness performance.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747021531, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=84, prompt_tokens=2943, total_tokens=3027, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.25, 0.15, 0.15, 0.15, 0.15, 0.3, 0.05, 0.06, 0.06, 0.06]\n",
      "Successfully saved GPTAnt_refine2_5_25_3.xml\n",
      "improved parameter [0.25, 0.15, 0.15, 0.15, 0.15, 0.3, 0.05, 0.06, 0.06, 0.06]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.15, 0.15, 0.15, 0.15, 0.3, 0.05, 0.06, 0.06, 0.06]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nThe terrain environemnt is a desert terrain with friction=\"0.4 0.005 0.0001\".\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nfitness: 22.35357564162838\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for moving forward with emphasis on higher speeds\\n    forward_reward = np.clip(x_velocity, 0, None) * self._forward_reward_weight\\n    \\n    # Reward for being healthy (staying within the healthy z range)\\n    healthy_reward = self.healthy_reward\\n    \\n    # Penalty for applying control (torque) to the joints\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Penalty for contact forces indicating potential instability\\n    contact_cost = self.contact_cost\\n    \\n    # Total rewards combined\\n    total_rewards = forward_reward + healthy_reward\\n\\n    # Reduce total rewards by costs to encourage more efficient movement\\n    costs = ctrl_cost + contact_cost\\n    reward = total_rewards - costs\\n    \\n    # Reward information for logging and analysis\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    # Normalizing the reward to encourage exploration\\n    normalized_reward = np.tanh(reward)\\n\\n    return normalized_reward, reward_info\\n\\nbest fitness: 22.35357564162838\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BWEVNo1kxS8uu8jbqRAyylmX4DOAc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward velocity, using an exponential function to increase the importance of higher speeds\\n    forward_reward = np.exp(x_velocity) * self._forward_reward_weight if x_velocity > 0 else 0\\n    \\n    # Reward for being healthy (punctual based on staying within the z range)\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost for applying torque to the joints, to encourage a balance of effort\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Contact cost penalty, to avoid instabilities during movement\\n    contact_cost = self.contact_cost\\n    \\n    # Calculate total rewards \\n    total_rewards = forward_reward + healthy_reward\\n\\n    # Total costs considering control and contact costs\\n    costs = ctrl_cost + contact_cost\\n    reward = total_rewards - costs\\n    \\n    # Apply a logistic function to normalize the reward values, encouraging exploration\\n    normalized_reward = 1 / (1 + np.exp(-reward))\\n    \\n    # Reward information for logging and analysis\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return normalized_reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747021953, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=272, prompt_tokens=2320, total_tokens=2592, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "improved_fitness 4.45820498797138\n",
      "ChatCompletion(id='chatcmpl-BWEchKQwuHMmMUw6b5j6sBcRuzazS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.22, 0.18, 0.18, 0.18, 0.18, 0.35, 0.35, 0.07, 0.07, 0.07],\\n  \"desciption\": \"Optimized design with reduced torso and leg sizes to lower material costs while enhancing leg and ankle dimensions for better locomotion control.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747022407, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=89, prompt_tokens=2943, total_tokens=3032, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.22, 0.18, 0.18, 0.18, 0.18, 0.35, 0.35, 0.07, 0.07, 0.07]\n",
      "Successfully saved GPTAnt_refine2_5_25_5.xml\n",
      "improved parameter [0.22, 0.18, 0.18, 0.18, 0.18, 0.35, 0.35, 0.07, 0.07, 0.07]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.22, 0.18, 0.18, 0.18, 0.18, 0.35, 0.35, 0.07, 0.07, 0.07]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nThe terrain environemnt is a desert terrain with friction=\"0.4 0.005 0.0001\".\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nfitness: 22.35357564162838\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward velocity, using an exponential function to increase the importance of higher speeds\\n    forward_reward = np.exp(x_velocity) * self._forward_reward_weight if x_velocity > 0 else 0\\n    \\n    # Reward for being healthy (punctual based on staying within the z range)\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost for applying torque to the joints, to encourage a balance of effort\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Contact cost penalty, to avoid instabilities during movement\\n    contact_cost = self.contact_cost\\n    \\n    # Calculate total rewards \\n    total_rewards = forward_reward + healthy_reward\\n\\n    # Total costs considering control and contact costs\\n    costs = ctrl_cost + contact_cost\\n    reward = total_rewards - costs\\n    \\n    # Apply a logistic function to normalize the reward values, encouraging exploration\\n    normalized_reward = 1 / (1 + np.exp(-reward))\\n    \\n    # Reward information for logging and analysis\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return normalized_reward, reward_info\\n\\nbest fitness: 22.35357564162838\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BWEjbOMs9qaIhfkz6irSc7sEqZbvP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward velocity, using a quadratic function to emphasize acceleration\\n    forward_reward = (x_velocity ** 2) * self._forward_reward_weight if x_velocity > 0 else 0\\n    \\n    # Healthy reward for being within a specific range in the z-axis\\n    healthy_reward = self.healthy_reward\\n    \\n    # Control cost to manage torque application on joints\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Contact cost penalty to encourage stable movement\\n    contact_cost = self.contact_cost\\n    \\n    # Total rewards combining forward movement and health\\n    total_rewards = forward_reward + healthy_reward\\n    \\n    # Aggregate costs including control and contact penalties\\n    total_costs = ctrl_cost + contact_cost\\n    \\n    # Calculate the overall reward\\n    reward = total_rewards - total_costs\\n    \\n    # Apply a normalized logistic transformation to reward\\n    normalized_reward = (reward + 1) / (1 + np.exp(-reward + 1))  # Shifted for stability\\n    \\n    # Reward information for tracking and analysis purposes\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return normalized_reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747022835, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=281, prompt_tokens=2347, total_tokens=2628, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "coarse_best = [(5,25)]\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_ant_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "    print(f\"Initial parameter:{parameter}\")\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        iteration +=1   \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list[morphology_index],  # 这本身已经是list结构，可以保留\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],  # 👈 用 [] 包装成列表\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        print(\"improved parameter\", improved_parameter)\n",
    "        shutil.copy(improved_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "        improved_material = compute_ant_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            # break\n",
    "            \n",
    "        iteration +=1        \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            [rewardfunc_list[rewardfunc_index]],\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        shutil.copy(best_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_ant_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "    logging.info(\"____________________________________________\")\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5272.058355242536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_refine2_5_15_7.xml',\n",
    "  'best_parameter': [0.04, 0.2, 0.1, 0.3, 0.05, 0.05, 0.05, 0.01, 0.01, 0.01],\n",
    "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine2_5_15_6.py',\n",
    "  'best_fitness': 16.902433015902933,\n",
    "  'best_material': 0.001070389545142885,\n",
    "  'best_efficiency': 15790.917514657383,\n",
    "  'best_iteration': 10},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material=0.0012196922555950196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.04, 0.25, 0.12, 0.35, 0.06, 0.06, 0.06, 0.01, 0.01, 0.01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0012196922555950196"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.04, 0.2, 0.1, 0.3, 0.05, 0.05, 0.05, 0.01, 0.01, 0.01]\n",
      "best 1e6 steps train\n",
      "\n",
      "fitness:39.5998887416585\n",
      "efficiency:36995.7731008783\n"
     ]
    }
   ],
   "source": [
    "# best new\n",
    "# 5e5\n",
    "# fitness:20.08678694564774\n",
    "# efficiency:18765.866162273083\n",
    "\n",
    "# fitness:39.5998887416585\n",
    "# efficiency:36995.7731008783\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTAnt_refine2_5_15_7.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_refine2_5_15_6.py\"\n",
    "\n",
    "morphology_index=9999\n",
    "rewardfunc_index=9999\n",
    "\n",
    "parameter = [0.04, 0.2, 0.1, 0.3, 0.05, 0.05, 0.05, 0.01, 0.01, 0.01]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'best_morphology': 'results/noDiv_m25_r5/assets/GPTAnt_refine_0_1_0.xml',\n",
    "  'best_parameter': [0.1, 0.18, 0.1, 0.15, 0.1, 0.1, 0.05, 0.02, 0.02, 0.02],\n",
    "  'best_rewardfunc': 'results/noDiv_m25_r5/env/GPTrewardfunc_0.py',\n",
    "  'best_fitness': 28.671216806088314,\n",
    "  'best_material': 0.007094102747340339,\n",
    "  'best_efficiency': 4041.5564627728977,\n",
    "  'best_iteration': 1},"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-05-02 03:52:58,144 - iteration:1, morphology: 0, rewardfunc: 0, material cost: 0.1821760165414783 reward: 1988.0968899033003 fitness: 6.343393664629946 efficiency: 34.820135960024494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "eureka 1e6 steps train\n",
      "\n",
      "fitness:9.076895316354705\n",
      "efficiency:49.82486437389004\n"
     ]
    }
   ],
   "source": [
    "# eureka\n",
    "\n",
    "\n",
    "morphology = \"results/Eureka/assets/GPTAnt_25.xml\"\n",
    "rewardfunc = \"results/Eureka/env/GPTAnt_1_1.py\"\n",
    "\n",
    "morphology_index=222\n",
    "rewardfunc_index=222\n",
    "\n",
    "parameter = [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-05-02 11:37:20,508 - iter 2, morphology: 0, rewardfunc: 0, material cost: 0.030716026555633116 reward: 942.9674856487128 fitness: 0.05740483585471345 efficiency: 1.868888729821266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.18, 0.17, 0.15, 0.07, 0.07, 0.07, 0.07, 0.035, 0.025, 0.025]\n",
      "eureka morphlogy 1e6 steps train\n",
      "\n",
      "fitness:0.0772372877729353\n",
      "efficiency:2.514559870985998\n"
     ]
    }
   ],
   "source": [
    "# eureka morphology\n",
    "\n",
    "\n",
    "\n",
    "morphology = \"results/Eureka_morphology/assets/GPTAnt_0_iter2.xml\"\n",
    "rewardfunc = \"results/Eureka_morphology/env/GPTrewardfunc_5.py\"\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "parameter = [0.18, 0.17, 0.15, 0.07, 0.07, 0.07, 0.07, 0.035, 0.025, 0.025]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka morphology 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka morphlogy 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
