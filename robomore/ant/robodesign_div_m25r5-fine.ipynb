{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTAnt import GPTAntEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        # api_key = \"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\"\n",
    "        api_key = \"sk-FdAAYf3ZI8C4uY1R1GVAk8jDuuc0qyZ3XfDfF4ijqM5gTqFk\"\n",
    "        self.client = OpenAI(api_key=api_key, base_url = \"http://chatapi.littlewheat.com/v1\")\n",
    "        # self.model = \"gpt-3.5-turbo\"\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTAnt_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        # env_path = os.path.join(os.path.dirname(__file__), \"env\", \"ant_v5.py\")\n",
    "        # with open(env_path, \"r\") as f:\n",
    "        #     env_content = f.read().rstrip()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums\n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_ant_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = ant_design(parameter)  \n",
    "            filename = f\"GPTAnt_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_ant_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(initial_parameter)})\n",
    "\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "\n",
    "        xml_file = ant_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTAnt_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_ant_volume(diverse_parameter['parameters']))\n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistant\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = ant_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTAnt_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "    \n",
    "    \n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\"best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\" \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = ant_design(parameter)  \n",
    "        filename = f\"GPTAnt_refine_{step}_{rewardfunc_index}_{morphology_index}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"parameters_fineonly.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 26\n",
    "rewardfunc_nums = 6\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n",
    "\n",
    "\n",
    "\n",
    "# return file list of morphology and reward function: [GPTAnt_{i}.xml] and [GPTAnt_{j}.py]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter coarse optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.15, 0.2, 0.05, 0.3, 0.1, 0.25, 0.1, 0.05, 0.045, 0.045]\n",
      "params: [0.2, 0.15, 0.1, 0.25, 0.15, 0.2, 0.15, 0.04, 0.03, 0.03]\n",
      "params: [0.12, 0.18, 0.08, 0.35, 0.12, 0.3, 0.12, 0.035, 0.025, 0.025]\n",
      "params: [0.25, 0.1, 0.05, 0.2, 0.15, 0.15, 0.1, 0.02, 0.02, 0.02]\n",
      "params: [0.1, 0.25, 0.2, 0.4, 0.2, 0.5, 0.25, 0.06, 0.05, 0.05]\n",
      "params: [0.3, 0.05, 0.2, 0.1, 0.3, 0.05, 0.2, 0.01, 0.01, 0.01]\n",
      "params: [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02]\n",
      "params: [0.18, 0.12, 0.08, 0.22, 0.11, 0.18, 0.09, 0.04, 0.025, 0.025]\n",
      "params: [0.35, 0.1, 0.05, 0.1, 0.2, 0.05, 0.1, 0.07, 0.06, 0.06]\n",
      "params: [0.05, 0.4, 0.15, 0.1, 0.05, 0.2, 0.1, 0.02, 0.015, 0.015]\n",
      "params: [0.15, 0.05, 0.1, 0.08, 0.2, 0.05, 0.15, 0.03, 0.02, 0.02]\n",
      "params: [0.2, 0.25, 0.2, 0.5, 0.25, 0.4, 0.2, 0.08, 0.06, 0.06]\n",
      "params: [0.12, 0.2, 0.3, 0.18, 0.15, 0.25, 0.18, 0.045, 0.035, 0.035]\n",
      "params: [0.4, 0.12, 0.05, 0.12, 0.3, 0.08, 0.3, 0.04, 0.03, 0.03]\n",
      "params: [0.07, 0.18, 0.18, 0.35, 0.35, 0.18, 0.18, 0.03, 0.045, 0.045]\n",
      "params: [0.25, 0.1, 0.2, 0.25, 0.1, 0.3, 0.1, 0.02, 0.015, 0.015]\n",
      "params: [0.1, 0.35, 0.15, 0.45, 0.1, 0.6, 0.2, 0.01, 0.02, 0.02]\n",
      "params: [0.3, 0.1, 0.05, 0.2, 0.15, 0.05, 0.15, 0.07, 0.06, 0.06]\n",
      "params: [0.2, 0.05, 0.2, 0.1, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02]\n",
      "params: [0.18, 0.12, 0.25, 0.4, 0.2, 0.45, 0.15, 0.05, 0.035, 0.035]\n",
      "params: [0.05, 0.4, 0.1, 0.6, 0.05, 0.8, 0.1, 0.015, 0.01, 0.01]\n",
      "params: [0.2, 0.05, 0.1, 0.05, 0.25, 0.05, 0.25, 0.01, 0.02, 0.02]\n",
      "params: [0.5, 0.08, 0.02, 0.12, 0.08, 0.15, 0.08, 0.04, 0.025, 0.025]\n",
      "params: [0.15, 0.12, 0.08, 0.25, 0.1, 0.3, 0.1, 0.03, 0.02, 0.02]\n",
      "params: [0.25, 0.05, 0.3, 0.1, 0.4, 0.15, 0.35, 0.03, 0.03, 0.02]\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n"
     ]
    }
   ],
   "source": [
    "morphology_list = [f'results/Div_m25_r5/assets/GPTAnt_{i}.xml' for i in range(0,26) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,6)]\n",
    "\n",
    "parameter_list = [[0.15, 0.2, 0.05, 0.3, 0.1, 0.25, 0.1, 0.05, 0.045, 0.045],\n",
    " [0.2, 0.15, 0.1, 0.25, 0.15, 0.2, 0.15, 0.04, 0.03, 0.03],\n",
    " [0.12, 0.18, 0.08, 0.35, 0.12, 0.3, 0.12, 0.035, 0.025, 0.025],\n",
    " [0.25, 0.1, 0.05, 0.2, 0.15, 0.15, 0.1, 0.02, 0.02, 0.02],\n",
    " [0.1, 0.25, 0.2, 0.4, 0.2, 0.5, 0.25, 0.06, 0.05, 0.05],\n",
    " [0.3, 0.05, 0.2, 0.1, 0.3, 0.05, 0.2, 0.01, 0.01, 0.01],\n",
    " [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02],\n",
    " [0.18, 0.12, 0.08, 0.22, 0.11, 0.18, 0.09, 0.04, 0.025, 0.025],\n",
    " [0.35, 0.1, 0.05, 0.1, 0.2, 0.05, 0.1, 0.07, 0.06, 0.06],\n",
    " [0.05, 0.4, 0.15, 0.1, 0.05, 0.2, 0.1, 0.02, 0.015, 0.015],\n",
    " [0.15, 0.05, 0.1, 0.08, 0.2, 0.05, 0.15, 0.03, 0.02, 0.02],\n",
    " [0.2, 0.25, 0.2, 0.5, 0.25, 0.4, 0.2, 0.08, 0.06, 0.06],\n",
    " [0.12, 0.2, 0.3, 0.18, 0.15, 0.25, 0.18, 0.045, 0.035, 0.035],\n",
    " [0.4, 0.12, 0.05, 0.12, 0.3, 0.08, 0.3, 0.04, 0.03, 0.03],\n",
    " [0.07, 0.18, 0.18, 0.35, 0.35, 0.18, 0.18, 0.03, 0.045, 0.045],\n",
    " [0.25, 0.1, 0.2, 0.25, 0.1, 0.3, 0.1, 0.02, 0.015, 0.015],\n",
    " [0.1, 0.35, 0.15, 0.45, 0.1, 0.6, 0.2, 0.01, 0.02, 0.02],\n",
    " [0.3, 0.1, 0.05, 0.2, 0.15, 0.05, 0.15, 0.07, 0.06, 0.06],\n",
    " [0.2, 0.05, 0.2, 0.1, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02],\n",
    " [0.18, 0.12, 0.25, 0.4, 0.2, 0.45, 0.15, 0.05, 0.035, 0.035],\n",
    " [0.05, 0.4, 0.1, 0.6, 0.05, 0.8, 0.1, 0.015, 0.01, 0.01],\n",
    " [0.2, 0.05, 0.1, 0.05, 0.25, 0.05, 0.25, 0.01, 0.02, 0.02],\n",
    " [0.5, 0.08, 0.02, 0.12, 0.08, 0.15, 0.08, 0.04, 0.025, 0.025],\n",
    " [0.15, 0.12, 0.08, 0.25, 0.1, 0.3, 0.1, 0.03, 0.02, 0.02],\n",
    " [0.25, 0.05, 0.3, 0.1, 0.4, 0.15, 0.35, 0.03, 0.03, 0.02],\n",
    "[0.25, 0.2, 0.2, 0.2, 0.2,0.4,0.4, 0.08, 0.08, 0.08 ]\n",
    "]\n",
    "\n",
    "\n",
    "material_list = [compute_ant_volume(parameter) for parameter in parameter_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 results/Div_m25_r5/env/GPTrewardfunc_5.py\n",
      "25 results/Div_m25_r5/assets/GPTAnt_25.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i, rewardfunc in enumerate(rewardfunc_list):\n",
    "    for j, morphology in enumerate(morphology_list):\n",
    "        # if i not in [0] or j not in [12]:\n",
    "        #     continue\n",
    "        if i not in [5] or j not in [25]:\n",
    "            continue\n",
    "        print(i, rewardfunc)\n",
    "        print(j, morphology)\n",
    "        shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(rewardfunc, \"GPTrewardfunc.py\")         \n",
    "\n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "        env_name = \"GPTAntEnv\"\n",
    "        # model_path = Train(j,  i, folder_name, total_timesteps=5e5)\n",
    "        model_path = folder_name + f\"/coarse/SAC_morphology{j}_rewardfunc{i}_500000.0steps\"\n",
    "        fitness, reward = Eva(model_path)\n",
    "        material = material_list[j]\n",
    "        efficiency = fitness/material\n",
    "        fitness_matrix[i][j] = fitness\n",
    "        efficiency_matrix[i][j] = efficiency\n",
    "        \n",
    "        logging.info(\"___________________finish coarse optimization_____________________\")\n",
    "        logging.info(f\"morphology: {j}, rewardfunc: {i}, material cost: {material} reward: {reward} fitness: {fitness} efficiency: {efficiency}\")\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness = fitness\n",
    "            best_morphology = morphology\n",
    "            best_efficiency = efficiency\n",
    "            best_rewardfunc = rewardfunc\n",
    "            best_material = material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None],\n",
       "       [None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None, None, None, None, None, None, None, None,\n",
       "        None, None, None, None],\n",
       "       [25.976004433052275, 36.58525898984436, 31.567318151268104,\n",
       "        13.436917179667597, 36.199257782034444, 17.52818671148676,\n",
       "        6190.388972915846, 210.68507833990023, 75.04880074633242,\n",
       "        114.71596198696973, 377.16730624483864, 35.74591642271765,\n",
       "        225.15876856567405, 21.53321351597596, 1738.3087280500133,\n",
       "        29.316838818767042, 148.74909546705013, 11.004440550548004,\n",
       "        19.20661299136772, 32.16299235790535, 377.3616748405897,\n",
       "        65.48818959763025, -2.430992278812614, 19.186624914278536,\n",
       "        18.690754933815192, None]], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficiency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'efficiency_coarse_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m coarse_best \u001b[38;5;241m=\u001b[39m \u001b[43mefficiency_coarse_best\u001b[49m\n\u001b[1;32m      2\u001b[0m coarse_best\n",
      "\u001b[0;31mNameError\u001b[0m: name 'efficiency_coarse_best' is not defined"
     ]
    }
   ],
   "source": [
    "coarse_best = efficiency_coarse_best\n",
    "coarse_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix_select = efficiency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 这里写一个reward func改进版本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "Initial parameter:[0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "ChatCompletion(id='chatcmpl-BVviGYR4WOTqckw2RDKqD76Up8GEe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.3, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.1, 0.1, 0.1],\\n  \"description\": \"Increased leg and ankle lengths for improved stride and stability while slightly reducing torso and geom sizes to cut down on material costs.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746949716, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=87, prompt_tokens=2512, total_tokens=2599, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.3, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.1, 0.1, 0.1]\n",
      "Successfully saved GPTAnt_refine_5_25_1.xml\n",
      "improved parameter [0.3, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.1, 0.1, 0.1]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.3, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.1, 0.1, 0.1]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 35.81691401075135\\n\\nbest reward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 35.81691401075135\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVvqniKIBNMxWy2XnPBFb66vWg0XJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here\\'s a revised reward function designed to improve the performance of the ant robot. This version includes an enhanced emphasis on obtaining higher velocities, while also penalizing excessive control actions to promote efficient movement.\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward reward weighted by the x_velocity\\n    forward_reward = x_velocity * self._forward_reward_weight\\n\\n    # Healthy reward for maintaining a valid state\\n    healthy_reward = self.healthy_reward\\n\\n    # Enhance forward reward with exponential scaling for better feedback at higher speeds\\n    scaled_forward_reward = np.exp(forward_reward) - 1  # Normalizing to a range\\n\\n    # Aggregate rewards\\n    rewards = scaled_forward_reward + healthy_reward\\n\\n    # Control cost penalizes high torque actions\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Contact cost penalizes excessive forces on contacts\\n    contact_cost = self.contact_cost\\n\\n    # Total costs calculation\\n    costs = ctrl_cost + contact_cost\\n\\n    # Compute the total reward\\n    reward = rewards - costs\\n\\n    # Create reward information for analysis\\n    reward_info = {\\n        \"reward_forward\": scaled_forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n``` \\n\\nThis reward function introduces an exponential scaling factor to the forward reward, allowing the agent to receive more significant positive feedback as it accelerates, thus incentivizing it to move faster. Balancing efficiency and health remains critical, as indicated by the existing penalties for control and contact costs.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746950245, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=332, prompt_tokens=2182, total_tokens=2514, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "improved_fitness 4.205243289377263\n"
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "coarse_best = [(5,25)]\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_ant_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "    print(f\"Initial parameter:{parameter}\")\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        iteration +=1   \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list[morphology_index],  # 这本身已经是list结构，可以保留\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],  # 👈 用 [] 包装成列表\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        print(\"improved parameter\", improved_parameter)\n",
    "        shutil.copy(improved_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "        improved_material = compute_ant_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            # break\n",
    "            \n",
    "        iteration +=1        \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            [rewardfunc_list[rewardfunc_index]],\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        shutil.copy(best_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_ant_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "    logging.info(\"____________________________________________\")\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02]\n",
      "ChatCompletion(id='chatcmpl-BVsD5iPfyYt2OiXB5eBXb06xRGlAN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.07, 0.28, 0.22, 0.18, 0.1, 0.12, 0.08, 0.03, 0.02, 0.02],\\n  \"desciption\": \"Optimized design with slightly reduced torso size and proportional increases in leg lengths and ankle sizes to enhance walking stability and distance while minimizing material costs.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746936251, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=92, prompt_tokens=4071, total_tokens=4163, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.07, 0.28, 0.22, 0.18, 0.1, 0.12, 0.08, 0.03, 0.02, 0.02]\n",
      "Successfully saved GPTAnt_refine_4_6_0.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.07, 0.28, 0.22, 0.18, 0.1, 0.12, 0.08, 0.03, 0.02, 0.02]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\n\\nfitness: 9744.070064730184\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for efficient forward motion with minimization of sideways drift (y_velocity close to 0)\\n    forward_efficiency_reward = self._forward_reward_weight * x_velocity\\n    sideways_drift_penalty = -self._forward_reward_weight * abs(self.data.qvel[1])  # Penalty for y_velocity\\n\\n    # Penalize for oscillations in movement - Oscillations can be inferred by monitoring large changes in x velocity\\n    velocity_smoothness_penalty = -self._forward_reward_weight * np.abs(self.data.qvel[3])  # Change in x velocity over time\\n\\n    # Control cost: Encourage the agent to use less energy in movements\\n    control_cost = self.control_cost(action)\\n\\n    # Health reward: Encourage the agent to maintain a physically feasible posture\\n    health_reward = self.healthy_reward\\n\\n    # Total reward computation\\n    reward = (forward_efficiency_reward + sideways_drift_penalty + velocity_smoothness_penalty\\n              - control_cost + health_reward)\\n\\n    # Dictionary of all reward components for debugging and analysis in training logs\\n    reward_info = {\\n        \\'forward_efficiency_reward\\': forward_efficiency_reward,\\n        \\'sideways_drift_penalty\\': sideways_drift_penalty,\\n        \\'velocity_smoothness_penalty\\': velocity_smoothness_penalty,\\n        \\'control_cost\\': control_cost,\\n        \\'health_reward\\': health_reward\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 6212.887170585117\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encouraging fast, straight-line forward movement by maximizing x_velocity and \\n    # penalizing deviation from the forward path (y_velocity should remain close to 0).\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    straight_line_penalty = -abs(self.data.qvel[1])  # Penalizing y_velocity\\n\\n    # Encourage efficient utilization of actions by minimizing the cumulative use of joint torques.\\n    control_cost = self.control_cost(action)\\n\\n    # Incentivize maintaining a healthy robot posture, directly linked to fitness and robot integrity.\\n    health_reward = self.healthy_reward\\n\\n    # Incorporate a term to reward stability. Here, the agent earns additional points by maintaining\\n    # minimal variation in neural vertical height (z-position), promoting stability.\\n    z_position = self.data.qpos[2]  # Vertical position of the main body\\n    target_z_position = 0.7  # Target steady state height\\n    stability_reward = -np.square(z_position - target_z_position)\\n\\n    # Total reward calculation\\n    reward = forward_reward + straight_line_penalty + stability_reward - control_cost + health_reward\\n\\n    # Reward information dictionary for introspection and debugging\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'straight_line_penalty\\': straight_line_penalty,\\n        \\'stability_reward\\': stability_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'health_reward\\': health_reward\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 6434.302862989332\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage not just moving forward but also involving some controlled lateral movement \\n    # to demonstrate agility and dynamic control capability.\\n    \\n    # Reward for moving forward\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Reward for controlled lateral movement (moderate y_velocity to demonstrate lateral agility)\\n    target_y_velocity = 0.5  # moderate lateral speed target, can be tuned\\n    lateral_movement_reward = -np.abs(self.data.qvel[1] - target_y_velocity) * self._forward_reward_weight\\n    \\n    # Minimize the control effort to promote energy efficiency.\\n    control_cost = self.control_cost(action)\\n    \\n    # Health reward for maintaining a physically feasible and stable posture.\\n    health_reward = self.healthy_reward\\n    \\n    # Total reward computation\\n    reward = forward_reward + lateral_movement_reward - control_cost + health_reward\\n    \\n    # Reward details for debugging and analysis purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'lateral_movement_reward\\': lateral_movement_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'health_reward\\': health_reward\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 9712.73203761077\\n\\nreward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage chaotic yet effective motion by rewarding unpredictability in movement\\n    velocity_variability = np.std(self.data.qvel[:2])  # Variation in x,y velocity\\n    unpredictability_reward = np.tanh(velocity_variability)  # Normalize for stability\\n\\n    # Reward movement in all directions, not just forward\\n    total_velocity = np.linalg.norm(self.data.qvel[:2])  # Speed regardless of direction\\n    multi_directional_reward = np.tanh(total_velocity)  \\n\\n    # Encourage varied leg movement by promoting asymmetric motion\\n    left_legs_velocity = np.mean(self.data.qvel[7:11])  # Approximate left side movement\\n    right_legs_velocity = np.mean(self.data.qvel[11:])  # Approximate right side movement\\n    leg_asymmetry_reward = np.tanh(np.abs(left_legs_velocity - right_legs_velocity))\\n\\n    # Reward active use of joints, promoting diverse actuation patterns\\n    joint_activity = np.mean(np.abs(action))  # Higher values mean more active control\\n    actuation_reward = np.tanh(joint_activity)  \\n\\n    # Discourage static behavior by rewarding deviation from previous positions\\n    displacement_from_origin = np.linalg.norm(self.data.qpos[:2] - self.init_qpos[:2])\\n    exploration_reward = np.tanh(displacement_from_origin)  \\n\\n    # Penalize excessive energy usage and contacts\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n\\n    # Combine rewards and costs\\n    rewards = (\\n        unpredictability_reward \\n        + multi_directional_reward \\n        + leg_asymmetry_reward \\n        + actuation_reward \\n        + exploration_reward\\n    )\\n    costs = ctrl_cost + contact_cost\\n\\n    # Total reward\\n    reward = rewards - costs\\n\\n    # Reward breakdown\\n    reward_info = {\\n        \"reward_unpredictability\": unpredictability_reward,\\n        \"reward_multi_directional\": multi_directional_reward,\\n        \"reward_leg_asymmetry\": leg_asymmetry_reward,\\n        \"reward_actuation\": actuation_reward,\\n        \"reward_exploration\": exploration_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 10464.171685319596\\n\\nbest reward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage chaotic yet effective motion by rewarding unpredictability in movement\\n    velocity_variability = np.std(self.data.qvel[:2])  # Variation in x,y velocity\\n    unpredictability_reward = np.tanh(velocity_variability)  # Normalize for stability\\n\\n    # Reward movement in all directions, not just forward\\n    total_velocity = np.linalg.norm(self.data.qvel[:2])  # Speed regardless of direction\\n    multi_directional_reward = np.tanh(total_velocity)  \\n\\n    # Encourage varied leg movement by promoting asymmetric motion\\n    left_legs_velocity = np.mean(self.data.qvel[7:11])  # Approximate left side movement\\n    right_legs_velocity = np.mean(self.data.qvel[11:])  # Approximate right side movement\\n    leg_asymmetry_reward = np.tanh(np.abs(left_legs_velocity - right_legs_velocity))\\n\\n    # Reward active use of joints, promoting diverse actuation patterns\\n    joint_activity = np.mean(np.abs(action))  # Higher values mean more active control\\n    actuation_reward = np.tanh(joint_activity)  \\n\\n    # Discourage static behavior by rewarding deviation from previous positions\\n    displacement_from_origin = np.linalg.norm(self.data.qpos[:2] - self.init_qpos[:2])\\n    exploration_reward = np.tanh(displacement_from_origin)  \\n\\n    # Penalize excessive energy usage and contacts\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n\\n    # Combine rewards and costs\\n    rewards = (\\n        unpredictability_reward \\n        + multi_directional_reward \\n        + leg_asymmetry_reward \\n        + actuation_reward \\n        + exploration_reward\\n    )\\n    costs = ctrl_cost + contact_cost\\n\\n    # Total reward\\n    reward = rewards - costs\\n\\n    # Reward breakdown\\n    reward_info = {\\n        \"reward_unpredictability\": unpredictability_reward,\\n        \"reward_multi_directional\": multi_directional_reward,\\n        \"reward_leg_asymmetry\": leg_asymmetry_reward,\\n        \"reward_actuation\": actuation_reward,\\n        \"reward_exploration\": exploration_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 10464.171685319596\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVsLJPyfERr2B0ce9SEPo4AldRnmX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage efficient forward motion\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Penalize sideways drift to keep the movement in a straight line\\n    sideways_drift_penalty = -self._forward_reward_weight * abs(self.data.qvel[1])\\n\\n    # Promote velocity stability by penalizing abrupt changes in forward velocity\\n    velocity_change = np.abs(self.data.qvel[3])\\n    velocity_smoothness_penalty = -self._forward_reward_weight * velocity_change\\n\\n    # Encourage controlled leg movement efficiency\\n    control_cost = self.control_cost(action)\\n\\n    # Maintain overall robot health by rewarding stable postures\\n    health_reward = self.healthy_reward\\n\\n    # Introduce a reward for maintaining an average target speed\\n    target_velocity = 1.0  # Target average forward speed\\n    speed_reward = self._forward_reward_weight * np.tanh(x_velocity / target_velocity)\\n\\n    # Total reward calculation\\n    reward = (forward_reward + sideways_drift_penalty + velocity_smoothness_penalty\\n              - control_cost + health_reward + speed_reward)\\n\\n    # Reward information for debugging and analysis\\n    reward_info = {\\n        'forward_reward': forward_reward,\\n        'sideways_drift_penalty': sideways_drift_penalty,\\n        'velocity_smoothness_penalty': velocity_smoothness_penalty,\\n        'control_cost': control_cost,\\n        'health_reward': health_reward,\\n        'speed_reward': speed_reward\\n    }\\n\\n    return reward, reward_info\\n```\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746936761, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=329, prompt_tokens=3836, total_tokens=4165, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02]\n",
      "params: [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02]\n",
      "ChatCompletion(id='chatcmpl-BVsStaWDrR6cM2FIP035ILIrV5tgl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.1, 0.25, 0.2, 0.4, 0.2, 0.5, 0.25, 0.05, 0.04, 0.04],\\n  \"description\": \"This design focuses on increasing leg length and reducing geom size, optimizing material cost while enhancing the robot\\'s ability to walk further.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746937231, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=87, prompt_tokens=4070, total_tokens=4157, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.1, 0.25, 0.2, 0.4, 0.2, 0.5, 0.25, 0.05, 0.04, 0.04]\n",
      "Successfully saved GPTAnt_refine_0_6_0.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.1, 0.25, 0.2, 0.4, 0.2, 0.5, 0.25, 0.05, 0.04, 0.04]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\n\\nfitness: 9744.070064730184\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for efficient forward motion with minimization of sideways drift (y_velocity close to 0)\\n    forward_efficiency_reward = self._forward_reward_weight * x_velocity\\n    sideways_drift_penalty = -self._forward_reward_weight * abs(self.data.qvel[1])  # Penalty for y_velocity\\n\\n    # Penalize for oscillations in movement - Oscillations can be inferred by monitoring large changes in x velocity\\n    velocity_smoothness_penalty = -self._forward_reward_weight * np.abs(self.data.qvel[3])  # Change in x velocity over time\\n\\n    # Control cost: Encourage the agent to use less energy in movements\\n    control_cost = self.control_cost(action)\\n\\n    # Health reward: Encourage the agent to maintain a physically feasible posture\\n    health_reward = self.healthy_reward\\n\\n    # Total reward computation\\n    reward = (forward_efficiency_reward + sideways_drift_penalty + velocity_smoothness_penalty\\n              - control_cost + health_reward)\\n\\n    # Dictionary of all reward components for debugging and analysis in training logs\\n    reward_info = {\\n        \\'forward_efficiency_reward\\': forward_efficiency_reward,\\n        \\'sideways_drift_penalty\\': sideways_drift_penalty,\\n        \\'velocity_smoothness_penalty\\': velocity_smoothness_penalty,\\n        \\'control_cost\\': control_cost,\\n        \\'health_reward\\': health_reward\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 6212.887170585117\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encouraging fast, straight-line forward movement by maximizing x_velocity and \\n    # penalizing deviation from the forward path (y_velocity should remain close to 0).\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    straight_line_penalty = -abs(self.data.qvel[1])  # Penalizing y_velocity\\n\\n    # Encourage efficient utilization of actions by minimizing the cumulative use of joint torques.\\n    control_cost = self.control_cost(action)\\n\\n    # Incentivize maintaining a healthy robot posture, directly linked to fitness and robot integrity.\\n    health_reward = self.healthy_reward\\n\\n    # Incorporate a term to reward stability. Here, the agent earns additional points by maintaining\\n    # minimal variation in neural vertical height (z-position), promoting stability.\\n    z_position = self.data.qpos[2]  # Vertical position of the main body\\n    target_z_position = 0.7  # Target steady state height\\n    stability_reward = -np.square(z_position - target_z_position)\\n\\n    # Total reward calculation\\n    reward = forward_reward + straight_line_penalty + stability_reward - control_cost + health_reward\\n\\n    # Reward information dictionary for introspection and debugging\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'straight_line_penalty\\': straight_line_penalty,\\n        \\'stability_reward\\': stability_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'health_reward\\': health_reward\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 6434.302862989332\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage not just moving forward but also involving some controlled lateral movement \\n    # to demonstrate agility and dynamic control capability.\\n    \\n    # Reward for moving forward\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Reward for controlled lateral movement (moderate y_velocity to demonstrate lateral agility)\\n    target_y_velocity = 0.5  # moderate lateral speed target, can be tuned\\n    lateral_movement_reward = -np.abs(self.data.qvel[1] - target_y_velocity) * self._forward_reward_weight\\n    \\n    # Minimize the control effort to promote energy efficiency.\\n    control_cost = self.control_cost(action)\\n    \\n    # Health reward for maintaining a physically feasible and stable posture.\\n    health_reward = self.healthy_reward\\n    \\n    # Total reward computation\\n    reward = forward_reward + lateral_movement_reward - control_cost + health_reward\\n    \\n    # Reward details for debugging and analysis purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'lateral_movement_reward\\': lateral_movement_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'health_reward\\': health_reward\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 9712.73203761077\\n\\nreward function:\\nimport numpy as np\\n\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage chaotic yet effective motion by rewarding unpredictability in movement\\n    velocity_variability = np.std(self.data.qvel[:2])  # Variation in x,y velocity\\n    unpredictability_reward = np.tanh(velocity_variability)  # Normalize for stability\\n\\n    # Reward movement in all directions, not just forward\\n    total_velocity = np.linalg.norm(self.data.qvel[:2])  # Speed regardless of direction\\n    multi_directional_reward = np.tanh(total_velocity)  \\n\\n    # Encourage varied leg movement by promoting asymmetric motion\\n    left_legs_velocity = np.mean(self.data.qvel[7:11])  # Approximate left side movement\\n    right_legs_velocity = np.mean(self.data.qvel[11:])  # Approximate right side movement\\n    leg_asymmetry_reward = np.tanh(np.abs(left_legs_velocity - right_legs_velocity))\\n\\n    # Reward active use of joints, promoting diverse actuation patterns\\n    joint_activity = np.mean(np.abs(action))  # Higher values mean more active control\\n    actuation_reward = np.tanh(joint_activity)  \\n\\n    # Discourage static behavior by rewarding deviation from previous positions\\n    displacement_from_origin = np.linalg.norm(self.data.qpos[:2] - self.init_qpos[:2])\\n    exploration_reward = np.tanh(displacement_from_origin)  \\n\\n    # Penalize excessive energy usage and contacts\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n\\n    # Combine rewards and costs\\n    rewards = (\\n        unpredictability_reward \\n        + multi_directional_reward \\n        + leg_asymmetry_reward \\n        + actuation_reward \\n        + exploration_reward\\n    )\\n    costs = ctrl_cost + contact_cost\\n\\n    # Total reward\\n    reward = rewards - costs\\n\\n    # Reward breakdown\\n    reward_info = {\\n        \"reward_unpredictability\": unpredictability_reward,\\n        \"reward_multi_directional\": multi_directional_reward,\\n        \"reward_leg_asymmetry\": leg_asymmetry_reward,\\n        \"reward_actuation\": actuation_reward,\\n        \"reward_exploration\": exploration_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 10464.171685319596\\n\\nbest reward function:\\n\\nbest fitness: 10464.171685319596\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVsbdouDGmd6DCnb8cnE6TQS6ejuw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here’s a reward function designed to improve the fitness of the ant robot in the given environment. This function emphasizes efficient forward movement while managing lateral stability and energy consumption more effectively.\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward movement\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Penalize uncontrolled lateral movement to encourage straight path movement\\n    sideways_drift_penalty = -self._forward_reward_weight * np.abs(self.data.qvel[1])  \\n    \\n    # Encourage smooth acceleration by penalizing rapid changes in x velocity\\n    velocity_smoothness_penalty = -self._forward_reward_weight * np.abs(self.data.qvel[3])  \\n    \\n    # Control cost to minimize high energy usage\\n    control_cost = self.control_cost(action)\\n    \\n    # Health reward to maintain robot integrity\\n    health_reward = self.healthy_reward\\n    \\n    # Encourage a proper height range to promote stability\\n    z_position = self.data.qpos[2]  # Vertical position of the main body\\n    ideal_height = 0.7  # Target height for stability\\n    stability_reward = -np.square(z_position - ideal_height)  # Penalize deviation from ideal height\\n\\n    # Total reward calculation\\n    reward = (forward_reward + sideways_drift_penalty + velocity_smoothness_penalty \\n              - control_cost + health_reward + stability_reward)\\n\\n    # Reward information for detailed analysis\\n    reward_info = {\\n        'forward_reward': forward_reward,\\n        'sideways_drift_penalty': sideways_drift_penalty,\\n        'velocity_smoothness_penalty': velocity_smoothness_penalty,\\n        'control_cost': control_cost,\\n        'health_reward': health_reward,\\n        'stability_reward': stability_reward\\n    }\\n\\n    return reward, reward_info\\n``` \\n\\nThis reward function integrates multiple aspects that contribute to the effective learning of the agent, focusing on maintaining efficiency while encouraging stability and health.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746937773, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=406, prompt_tokens=3353, total_tokens=3759, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGPTrewardfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_rew\n\u001b[1;32m     90\u001b[0m GPTAntEnv\u001b[38;5;241m.\u001b[39m_get_rew \u001b[38;5;241m=\u001b[39m _get_rew\n\u001b[0;32m---> 92\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmorphology_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewardfunc_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m improved_fitness, _ \u001b[38;5;241m=\u001b[39m Eva(model_path)\n\u001b[1;32m     94\u001b[0m improved_material \u001b[38;5;241m=\u001b[39m compute_ant_volume(best_parameter)\n",
      "File \u001b[0;32m~/autodl-tmp/Ant/utils.py:114\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(morphology, rewardfunc, folder_name, total_timesteps, stage, callback, iter)\u001b[0m\n\u001b[1;32m     96\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     envs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39mtensorboard_name\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback:\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 训练 100 万步\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     callback \u001b[38;5;241m=\u001b[39m DynamicRewardLoggingCallback()\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:76\u001b[0m, in \u001b[0;36mVecMonitor.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 76\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/autodl-tmp/Ant/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/autodl-tmp/Ant/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/Ant/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/Ant/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/Ant/gymnasium/envs/robodesign/GPTAnt.py:364\u001b[0m, in \u001b[0;36mGPTAntEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    359\u001b[0m reward, reward_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rew(x_velocity, action)\n\u001b[1;32m    360\u001b[0m terminated \u001b[38;5;241m=\u001b[39m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_healthy) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminate_when_unhealthy\n\u001b[1;32m    361\u001b[0m info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_from_origin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqpos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mord\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_velocity\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_velocity,\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_velocity\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_velocity,\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreward_info,\n\u001b[1;32m    368\u001b[0m }\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_ant_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list,\n",
    "            efficiency_matrix_select[rewardfunc_index, :],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "            \n",
    "        )\n",
    "\n",
    "        shutil.copy(improved_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        \n",
    "        model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "        improved_fitness, _ = Eva(model_path)\n",
    "        improved_material = compute_ant_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            iteration +=1\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            # break\n",
    "            \n",
    "            \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            rewardfunc_list,\n",
    "            efficiency_matrix_select[:, morphology_index],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "\n",
    "        shutil.copy(best_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        \n",
    "        model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "        improved_fitness, _ = Eva(model_path)\n",
    "        improved_material = compute_ant_volume(best_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            iteration +=1\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_25.xml',\n",
       "  'best_parameter': [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_5.py',\n",
       "  'best_fitness': 6.524982719287343,\n",
       "  'best_material': 0.1821760165414783,\n",
       "  'best_efficiency': 35.81691401075135,\n",
       "  'best_iteration': 2}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_6.xml',\n",
       "  'best_parameter': [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_4.py',\n",
       "  'best_fitness': 90.39450517864243,\n",
       "  'best_material': 0.008638476880636311,\n",
       "  'best_efficiency': 10464.171685319596,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_6.xml',\n",
       "  'best_parameter': [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_0.py',\n",
       "  'best_fitness': 84.17392397747206,\n",
       "  'best_material': 0.008638476880636311,\n",
       "  'best_efficiency': 9744.070064730184,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_refine_3_6_0.xml',\n",
       "  'best_parameter': [0.08,\n",
       "   0.32,\n",
       "   0.25,\n",
       "   0.17,\n",
       "   0.05,\n",
       "   0.12,\n",
       "   0.05,\n",
       "   0.03,\n",
       "   0.02,\n",
       "   0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_3.py',\n",
       "  'best_fitness': 121.56424277326022,\n",
       "  'best_material': 0.00900193353403425,\n",
       "  'best_efficiency': 13504.236874628506,\n",
       "  'best_iteration': 1},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_refine_2_6_2.xml',\n",
       "  'best_parameter': [0.08,\n",
       "   0.32,\n",
       "   0.27,\n",
       "   0.16,\n",
       "   0.04,\n",
       "   0.14,\n",
       "   0.045,\n",
       "   0.02,\n",
       "   0.015,\n",
       "   0.015],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTAnt_refine_2_6_1.py',\n",
       "  'best_fitness': 112.76682141066868,\n",
       "  'best_material': 0.0053784556056496475,\n",
       "  'best_efficiency': 20966.394385075142,\n",
       "  'best_iteration': 3},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_6.xml',\n",
       "  'best_parameter': [0.08, 0.3, 0.25, 0.15, 0.05, 0.1, 0.05, 0.03, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_1.py',\n",
       "  'best_fitness': 53.66988218510148,\n",
       "  'best_material': 0.008638476880636311,\n",
       "  'best_efficiency': 6212.887170585117,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_refine_1_14_1.xml',\n",
       "  'best_parameter': [0.07, 0.25, 0.2, 0.4, 0.35, 0.2, 0.18, 0.02, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_1.py',\n",
       "  'best_fitness': 26.145256032557008,\n",
       "  'best_material': 0.007472312391452871,\n",
       "  'best_efficiency': 3498.951149641842,\n",
       "  'best_iteration': 2},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_refine_2_14_0.xml',\n",
       "  'best_parameter': [0.06, 0.3, 0.3, 0.28, 0.28, 0.2, 0.2, 0.02, 0.03, 0.03],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTAnt_refine_2_14_1.py',\n",
       "  'best_fitness': 73.18167822133022,\n",
       "  'best_material': 0.011753484182474558,\n",
       "  'best_efficiency': 6226.381648639159,\n",
       "  'best_iteration': 2},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_14.xml',\n",
       "  'best_parameter': [0.07,\n",
       "   0.18,\n",
       "   0.18,\n",
       "   0.35,\n",
       "   0.35,\n",
       "   0.18,\n",
       "   0.18,\n",
       "   0.03,\n",
       "   0.045,\n",
       "   0.045],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_3.py',\n",
       "  'best_fitness': 44.781686547876895,\n",
       "  'best_material': 0.026895057039088423,\n",
       "  'best_efficiency': 1665.0526705629443,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_12.xml',\n",
       "  'best_parameter': [0.12,\n",
       "   0.2,\n",
       "   0.3,\n",
       "   0.18,\n",
       "   0.15,\n",
       "   0.25,\n",
       "   0.18,\n",
       "   0.045,\n",
       "   0.035,\n",
       "   0.035],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_4.py',\n",
       "  'best_fitness': 34.253350209103345,\n",
       "  'best_material': 0.027725883764084917,\n",
       "  'best_efficiency': 1235.428616110476,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_refine_4_14_3.xml',\n",
       "  'best_parameter': [0.07,\n",
       "   0.32,\n",
       "   0.26,\n",
       "   0.16,\n",
       "   0.05,\n",
       "   0.11,\n",
       "   0.05,\n",
       "   0.03,\n",
       "   0.02,\n",
       "   0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTAnt_refine_4_14_4.py',\n",
       "  'best_fitness': 96.70899899969896,\n",
       "  'best_material': 0.008270312722157724,\n",
       "  'best_efficiency': 11693.511750843152,\n",
       "  'best_iteration': 5},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_20.xml',\n",
       "  'best_parameter': [0.05, 0.4, 0.1, 0.6, 0.05, 0.8, 0.1, 0.015, 0.01, 0.01],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_0.py',\n",
       "  'best_fitness': 2.465729029898626,\n",
       "  'best_material': 0.003549167305769469,\n",
       "  'best_efficiency': 694.7345158652782,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_20.xml',\n",
       "  'best_parameter': [0.05, 0.4, 0.1, 0.6, 0.05, 0.8, 0.1, 0.015, 0.01, 0.01],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_2.py',\n",
       "  'best_fitness': 2.4289366138009667,\n",
       "  'best_material': 0.003549167305769469,\n",
       "  'best_efficiency': 684.3680234100339,\n",
       "  'best_iteration': 0}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10464.171685319596"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1821760165414783"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015]\n",
      "best 1e6 steps train\n",
      "\n",
      "fitness:-6.25737631816727\n",
      "efficiency:-1163.4150724595338\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter =   [0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015]\n",
    "xml_file = ant_design(parameter)  \n",
    "filename = r\"results/Div_m25_r5/assets/GPTAnt_refine_2_6_2.xml\"\n",
    "with open(filename, \"w\") as fp:\n",
    "    fp.write(xml_file)\n",
    "\n",
    "# best new\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTAnt_refine_2_6_2.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_4.py\"\n",
    "morphology_index=9999\n",
    "rewardfunc_index=9999\n",
    "\n",
    "parameter = [0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology6_rewardfunc2_500000.0steps\"\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 08:11:31,331 - Final optimized result: rewardfunc_index2 morphology_index6\n",
    "2025-04-07 08:11:31,331 -   Morphology: results/Div_m25_r5/assets/GPTAnt_refine_2_6_2.xml\n",
    "2025-04-07 08:11:31,331 -   Parameter: [0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015]\n",
    "2025-04-07 08:11:31,331 -   Rewardfunc: results/Div_m25_r5/env/GPTAnt_refine_2_6_1.py\n",
    "2025-04-07 08:11:31,331 -   Fitness: 112.76682141066868\n",
    "2025-04-07 08:11:31,331 -   Material: 0.0053784556056496475\n",
    "2025-04-07 08:11:31,331 -   Efficiency: 20966.394385075142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015]\n",
      "best 1e6 steps train\n",
      "\n",
      "fitness:145.9361529796732\n",
      "efficiency:27133.46798407682\n"
     ]
    }
   ],
   "source": [
    "# best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTAnt_refine_2_6_2.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTAnt_refine_2_6_1.py\"\n",
    "morphology_index=999\n",
    "rewardfunc_index=999\n",
    "\n",
    "parameter = [0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology6_rewardfunc2_500000.0steps\"\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"best 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"best 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "human 1e6 steps train\n",
      "\n",
      "fitness:26.74654625816157\n",
      "efficiency:146.81705509831392\n"
     ]
    }
   ],
   "source": [
    "# human\n",
    "morphology = \"results/Div_m25_r5/assets/GPTAnt_25.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_5.py\"\n",
    "morphology_index=888\n",
    "rewardfunc_index=888\n",
    "\n",
    "parameter = [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "# model_path = \"results/Div_m25_r5/fine/SAC_morphology6_rewardfunc2_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"human 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"human 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "eureka 1e6 steps train\n",
      "\n",
      "fitness:22.142035533056177\n",
      "efficiency:121.54198973834089\n"
     ]
    }
   ],
   "source": [
    "# eureka\n",
    "\n",
    "morphology = \"results/Eureka/assets/GPTAnt_0.xml\"\n",
    "rewardfunc = \"results/Eureka/env/GPTAnt_1_1.py\"\n",
    "morphology_index=888\n",
    "rewardfunc_index=888\n",
    "\n",
    "parameter = [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-05-02 12:38:52,500 - iter 2, morphology: 4, rewardfunc: 0, material cost: 0.0274933964772566 reward: 1456.4783091535148 fitness: 61.410184357942725 efficiency: 2233.6339712971862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.08, 0.6, 0.6, 0.5, 0.5, 0.4, 0.4, 0.03, 0.03, 0.03]\n",
      "eureka morphology 1e6 steps train\n",
      "\n",
      "fitness:143.07060077843667\n",
      "efficiency:5203.816883693841\n"
     ]
    }
   ],
   "source": [
    "# eureka morphology\n",
    "# 有点奇怪，为了保险，尽量重新运行吧\n",
    "morphology = \"results/Eureka_morphology/assets/GPTAnt_4_iter2.xml\"\n",
    "rewardfunc = \"results/Eureka_morphology/env/GPTrewardfunc_0.py\"\n",
    "morphology_index=777\n",
    "rewardfunc_index=777\n",
    "\n",
    "parameter = [0.08, 0.6, 0.6, 0.5, 0.5, 0.4, 0.4, 0.03, 0.03, 0.03]\n",
    "\n",
    "shutil.copy(morphology, \"GPTAnt.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTAntEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_ant_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka morphology 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka morphology 1e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coarse best\n",
    "2025-04-07 04:50:51,889 - morphology: 6, rewardfunc: 4, material cost: 0.008638476880636311 reward: 2999.8658541850805 fitness: 90.39450517864243 efficiency: 10464.171685319596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 11:24:45,590 - morphology: 6, rewardfunc: 5, material cost: 0.008638476880636311 reward: 1668.3157174084183 fitness: 53.4755320246795 efficiency: 6190.388972915846"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-06 16:07:31,309 - morphology: 8, rewardfunc: 0, material cost: 0.21463754792052214 reward: 1213.152772878221 fitness: 21.023052079911725 efficiency: 97.9467585405715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eureka morphology\n",
    "2025-04-06 12:49:27,537 - morphology: 3, rewardfunc: 0, material cost: 1.4778471060217129 reward: 1985.5329028127207 fitness: 118.0055746056207 efficiency: 79.84965029520917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 89\u001b[0m\n\u001b[1;32m     85\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/Div_m25_r5/fine/SAC_morphology\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmorphology_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rewardfunc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewardfunc_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_1000000.0steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# model_path = f\"results/Div_m25_r5/fine/SAC_morphology6_rewardfunc2_500000.0steps\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m fitness, _ \u001b[38;5;241m=\u001b[39m \u001b[43mEva_with_qpos_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewardfunc_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewardfunc_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmorphology_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmorphology_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m material \u001b[38;5;241m=\u001b[39m compute_ant_volume(parameter)\n\u001b[1;32m     91\u001b[0m efficiency \u001b[38;5;241m=\u001b[39m fitness \u001b[38;5;241m/\u001b[39m material\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mEva_with_qpos_logging\u001b[0;34m(model_path, run_steps, folder_name, video, rewardfunc_index, morphology_index)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAC\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m----> 9\u001b[0m current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[1;32m     10\u001b[0m xml_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTAnt.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m env \u001b[38;5;241m=\u001b[39m FitnessWrapper(gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTAntEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m, xml_file\u001b[38;5;241m=\u001b[39mxml_path, healthy_z_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m4.0\u001b[39m),\n\u001b[1;32m     13\u001b[0m                                render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m720\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
