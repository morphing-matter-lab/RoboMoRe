{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTHopper import GPTHopperEnv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        # self.model = \"gpt-3.5-turbo\"\n",
    "        self.model = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTHopper_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "        messages.append({\"role\": \"assistant\", \"content\": initial_code})\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "            # print(diverse_messages)\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            messages.append({\"role\": \"assistant\", \"content\": diverse_code})\n",
    "\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums                                                                                                                                                                                                                                                                                   \n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_hopper_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = hopper_design(parameter)  \n",
    "            filename = f\"GPTHopper_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_hopper_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(initial_parameter)})\n",
    "\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "\n",
    "        xml_file = hopper_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTHopper_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_hopper_volume(diverse_parameter['parameters'])) \n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistant\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = hopper_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTHopper_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\":reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, rewardfunc_index, morphology_index, iteration):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\"best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\"  \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        # print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = hopper_design(parameter)  \n",
    "        filename = f\"GPTHopper_refine_{rewardfunc_index}_{morphology_index}_{iteration}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"new_parameters.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 51\n",
    "rewardfunc_nums = 11\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_list = [[1.2, 0.9, 0.6, 0.3, 0.3, -0.3, 0.05, 0.04, 0.05, 0.05],\n",
    " [1.5, 1.2, 0.8, 0.5, 0.5, -0.5, 0.03, 0.03, 0.03, 0.04],\n",
    " [1.0, 0.7, 0.5, 0.2, 0.2, -0.2, 0.04, 0.05, 0.06, 0.07],\n",
    " [1.0, 0.6, 0.4, 0.1, 0.1, -0.1, 0.05, 0.04, 0.03, 0.02],\n",
    " [1.3, 0.9, 0.4, 0.1, 0.1, -0.1, 0.02, 0.02, 0.04, 0.06],\n",
    " [1.8, 1.3, 0.7, 0.2, 0.2, -0.2, 0.01, 0.03, 0.05, 0.06],\n",
    " [1.0, 0.8, 0.5, 0.2, 0.18, -0.2, 0.03, 0.02, 0.02, 0.03],\n",
    " [1.0, 0.9, 0.6, 0.2, 0.2, -0.2, 0.02, 0.04, 0.06, 0.08],\n",
    " [1.5, 1.2, 0.8, 0.4, 0.35, -0.35, 0.02, 0.02, 0.01, 0.05],\n",
    " [1.2, 0.8, 0.5, 0.3, 0.25, -0.25, 0.03, 0.05, 0.06, 0.02],\n",
    " [1.6, 1.1, 0.7, 0.4, 0.35, -0.35, 0.03, 0.03, 0.03, 0.04],\n",
    " [1.4, 0.9, 0.6, 0.2, 0.2, -0.2, 0.01, 0.01, 0.02, 0.04],\n",
    " [1.8, 1.1, 0.9, 0.6, 0.55, -0.6, 0.015, 0.015, 0.015, 0.025],\n",
    " [1.7, 1.2, 0.8, 0.3, 0.25, -0.25, 0.01, 0.03, 0.04, 0.02],\n",
    " [1.0, 0.6, 0.3, 0.1, 0.08, -0.08, 0.015, 0.025, 0.035, 0.045],\n",
    " [1.2, 0.5, 0.2, 0.1, 0.12, -0.12, 0.02, 0.015, 0.02, 0.05],\n",
    " [1.0, 0.8, 0.2, 0.05, 0.05, -0.05, 0.01, 0.02, 0.03, 0.06],\n",
    " [1.8, 0.6, 0.3, 0.05, 0.05, -0.05, 0.025, 0.035, 0.045, 0.02],\n",
    " [0.9, 0.4, 0.15, 0.05, 0.05, -0.05, 0.02, 0.03, 0.04, 0.01],\n",
    " [1.5, 0.8, 0.5, 0.2, 0.2, -0.2, 0.01, 0.01, 0.01, 0.04],\n",
    " [1.4, 0.6, 0.3, 0.1, 0.12, -0.12, 0.015, 0.04, 0.06, 0.02],\n",
    " [2.0, 1.0, 0.6, 0.1, 0.15, -0.15, 0.01, 0.02, 0.03, 0.05],\n",
    " [0.8, 0.5, 0.4, 0.2, 0.25, -0.25, 0.01, 0.015, 0.02, 0.03],\n",
    " [1.1, 0.8, 0.6, 0.2, 0.19, -0.19, 0.015, 0.03, 0.05, 0.02],\n",
    " [1.2, 0.4, 0.3, 0.15, 0.15, -0.15, 0.02, 0.04, 0.05, 0.01],\n",
    " [1.5, 1.2, 0.9, 0.5, 0.5, -0.5, 0.015, 0.02, 0.025, 0.03],\n",
    " [1.0, 0.8, 0.5, 0.1, 0.1, -0.1, 0.02, 0.015, 0.01, 0.06],\n",
    " [1.5, 0.7, 0.4, 0.1, 0.1, -0.1, 0.015, 0.025, 0.03, 0.05],\n",
    " [1.5, 1.0, 0.4, 0.15, 0.2, -0.2, 0.015, 0.01, 0.035, 0.05],\n",
    " [1.5, 1.0, 0.6, 0.3, 0.3, -0.3, 0.015, 0.02, 0.025, 0.03],\n",
    " [1.6, 1.2, 0.7, 0.2, 0.25, -0.25, 0.01, 0.04, 0.06, 0.02],\n",
    " [1.4, 0.6, 0.4, 0.2, 0.2, -0.35, 0.012, 0.015, 0.018, 0.06],\n",
    " [1.0, 0.6, 0.3, 0.05, 0.06, -0.06, 0.01, 0.02, 0.03, 0.05],\n",
    " [1.8, 1.0, 0.8, 0.15, 0.15, -0.15, 0.02, 0.02, 0.05, 0.07],\n",
    " [1.0, 0.6, 0.25, 0.05, 0.1, -0.1, 0.01, 0.01, 0.04, 0.05],\n",
    " [1.0, 0.3, 0.15, 0.05, 0.05, -0.05, 0.015, 0.025, 0.05, 0.1],\n",
    " [1.8, 1.2, 0.8, 0.2, 0.15, -0.15, 0.015, 0.04, 0.06, 0.08],\n",
    " [1.5, 0.7, 0.3, 0.05, 0.05, -0.05, 0.015, 0.03, 0.07, 0.1],\n",
    " [1.8, 1.4, 0.9, 0.3, 0.3, -0.3, 0.012, 0.013, 0.018, 0.065],\n",
    " [1.5, 1.0, 0.6, 0.05, 0.05, -0.05, 0.01, 0.02, 0.03, 0.06],\n",
    " [1.8, 0.5, 0.2, 0.05, 0.05, -0.05, 0.01, 0.05, 0.1, 0.2],\n",
    " [1.2, 0.9, 0.5, 0.1, 0.07, -0.12, 0.02, 0.01, 0.02, 0.08],\n",
    " [1.5, 1.0, 0.6, 0.3, 0.35, -0.35, 0.014, 0.018, 0.1, 0.03],\n",
    " [1.0, 0.8, 0.5, 0.1, 0.1, -0.1, 0.05, 0.02, 0.01, 0.1],\n",
    " [1.2, 0.6, 0.4, 0.15, 0.15, -0.15, 0.015, 0.02, 0.03, 0.04],\n",
    " [1.8, 1.4, 1.0, 0.5, 0.6, -0.7, 0.015, 0.02, 0.025, 0.15],\n",
    " [1.5, 0.5, 0.2, 0.1, 0.1, -0.15, 0.012, 0.02, 0.04, 0.08],\n",
    " [1.4, 0.85, 0.4, 0.05, 0.05, -0.05, 0.015, 0.01, 0.02, 0.03],\n",
    " [1.6, 0.5, 0.25, 0.05, 0.05, -0.05, 0.008, 0.015, 0.03, 0.1],\n",
    " [1.5, 1.2, 0.6, 0.1, 0.1, -0.3, 0.01, 0.03, 0.05, 0.07],\n",
    "[1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]]\n",
    "\n",
    "morphology_list = [f'results/Div_m25_r5/assets/GPTHopper_{i}.xml' for i in range(0,51) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,11)]\n",
    "\n",
    "material_list = [compute_hopper_volume(parameter) for parameter in parameter_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter coarse optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix = np.array([[None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 416.00392985310305],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 380.17372338336355],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 561.6139087406524],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 381.10650937501947],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 495.97501954740835],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [48.64317013054387, 203.81506929121974, 75.46462999365069,\n",
    "        564.2513466097603, 16.057820351949477, 906.2202450244702,\n",
    "        -11.725614708265494, 41.82336731444111, 151.2091580265195,\n",
    "        269.03077069812673, 263.77577467841803, 335.3823298069768,\n",
    "        462.14178803383237, 844.139122389248, 223.39054046172868,\n",
    "        248.7655359599275, -4.435027776773188, 280.797706653486,\n",
    "        1653.5922168599618, 428.12462169225836, 580.3848884995529,\n",
    "        808.3142418156576, 381.8550909407698, 852.4473279544258,\n",
    "        397.168803236307, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 94.19255287549959]],\n",
    "      dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix_select = np.array([[None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 416.00392985310305],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 380.17372338336355],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 561.6139087406524],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 381.10650937501947],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 495.97501954740835],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None],\n",
    "       [48.64317013054387, 203.81506929121974, 75.46462999365069,\n",
    "        564.2513466097603, 16.057820351949477, 906.2202450244702,\n",
    "        -11.725614708265494, 41.82336731444111, 151.2091580265195,\n",
    "        269.03077069812673, 263.77577467841803, 335.3823298069768,\n",
    "        462.14178803383237, 844.139122389248, 223.39054046172868,\n",
    "        248.7655359599275, -4.435027776773188, 280.797706653486,\n",
    "        1653.5922168599618, 428.12462169225836, 580.3848884995529,\n",
    "        808.3142418156576, 381.8550909407698, 852.4473279544258,\n",
    "        397.168803236307, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, None, None, None, None, None,\n",
    "        None, None, None, None, None, None, 94.19255287549959]],\n",
    "      dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewardfunc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/Div_m25_r5/env/GPTrewardfunc_10.py'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rewardfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameter:[1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
      "[1.5, 1.1, 0.65, 0.15, -0.12, 0.3, 0.04, 0.04, 0.03, 0.05]\n",
      "Successfully saved GPTHopper_refine_10_50_1.xml\n",
      "improved parameter [1.5, 1.1, 0.65, 0.15, -0.12, 0.3, 0.04, 0.04, 0.03, 0.05]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The hopper is a two-dimensional one-legged figure consisting of four main body parts - the torso at the top, the thigh in the middle, the leg at the bottom, and a single foot on which the entire body rests.\\nThe goal is to make hops that move in the forward (right) direction by applying torque to the three hinges that connect the four body parts, you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass HopperEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"hopper.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_state_range: Tuple[float, float] = (-100.0, 100.0),\\n        healthy_z_range: Tuple[float, float] = (0.7, float(\"inf\")),\\n        healthy_angle_range: Tuple[float, float] = (-0.2, 0.2),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_state_range,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_state_range = healthy_state_range\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n        state = self.state_vector()[2:]\\n\\n        min_state, max_state = self._healthy_state_range\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n\\n        is_healthy = all((healthy_state, healthy_z, healthy_angle))\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n        \\n        \\n        \\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 94.19255287549959\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 94.19255287549959\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVtUaubFkj9ZQbMtELKCOsjYCxhze', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward speed, amplified to enhance its weight in total reward\\n    forward_reward = self._forward_reward_weight * (x_velocity ** 2)  # Squaring velocity to emphasize faster movements\\n    healthy_reward = self.healthy_reward\\n    \\n    # Introducing a penalty for excessive control actions to encourage energy-efficient movements\\n    ctrl_cost = self.control_cost(action)\\n    control_penalty = self._ctrl_cost_weight * np.sum(np.square(action - np.mean(action)))  # Encouraging consistent control efforts\\n\\n    # Total reward calculation\\n    total_reward = forward_reward + healthy_reward - ctrl_cost - control_penalty\\n\\n    # Normalize rewards for stability\\n    normalized_reward = np.tanh(total_reward)\\n\\n    # Preparing the reward info for tracking\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_health\": healthy_reward,\\n        \"reward_control\": -ctrl_cost,\\n        \"reward_control_penalty\": -control_penalty,\\n        \"total_normalized_reward\": normalized_reward,\\n    }\\n\\n    return normalized_reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746941180, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=235, prompt_tokens=1960, total_tokens=2195, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness -1.8950689465671535\n",
      "[1.55, 1.1, 0.6, 0.12, -0.1, 0.35, 0.035, 0.035, 0.025, 0.04]\n",
      "Successfully saved GPTHopper_refine_10_50_4.xml\n",
      "improved parameter [1.55, 1.1, 0.6, 0.12, -0.1, 0.35, 0.035, 0.035, 0.025, 0.04]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The hopper is a two-dimensional one-legged figure consisting of four main body parts - the torso at the top, the thigh in the middle, the leg at the bottom, and a single foot on which the entire body rests.\\nThe goal is to make hops that move in the forward (right) direction by applying torque to the three hinges that connect the four body parts, you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass HopperEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"hopper.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_state_range: Tuple[float, float] = (-100.0, 100.0),\\n        healthy_z_range: Tuple[float, float] = (0.7, float(\"inf\")),\\n        healthy_angle_range: Tuple[float, float] = (-0.2, 0.2),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_state_range,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_state_range = healthy_state_range\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n        state = self.state_vector()[2:]\\n\\n        min_state, max_state = self._healthy_state_range\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n\\n        is_healthy = all((healthy_state, healthy_z, healthy_angle))\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n        \\n        \\n        \\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 94.19255287549959\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 94.19255287549959\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVtg53oJTHcCQXHHA4JYptWghY5DF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    time_penalty = -0.01  # Small negative reward to encourage faster completion\\n    control_effort = self.control_cost(action)\\n    \\n    # Implement exponential decay to the control cost to mitigate high penalization for control\\n    ctrl_cost = np.exp(-self._ctrl_cost_weight * control_effort)\\n\\n    # Total reward calculation\\n    reward = forward_reward + healthy_reward + time_penalty + ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_time\": time_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746941893, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=175, prompt_tokens=1960, total_tokens=2135, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 2.9000108027389393\n",
      "[1.55, 1.15, 0.7, 0.2, -0.1, 0.35, 0.05, 0.035, 0.025, 0.045]\n",
      "Successfully saved GPTHopper_refine_10_50_6.xml\n",
      "improved parameter [1.55, 1.15, 0.7, 0.2, -0.1, 0.35, 0.05, 0.035, 0.025, 0.045]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The hopper is a two-dimensional one-legged figure consisting of four main body parts - the torso at the top, the thigh in the middle, the leg at the bottom, and a single foot on which the entire body rests.\\nThe goal is to make hops that move in the forward (right) direction by applying torque to the three hinges that connect the four body parts, you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass HopperEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"hopper.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_state_range: Tuple[float, float] = (-100.0, 100.0),\\n        healthy_z_range: Tuple[float, float] = (0.7, float(\"inf\")),\\n        healthy_angle_range: Tuple[float, float] = (-0.2, 0.2),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_state_range,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_state_range = healthy_state_range\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n        state = self.state_vector()[2:]\\n\\n        min_state, max_state = self._healthy_state_range\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n\\n        is_healthy = all((healthy_state, healthy_z, healthy_angle))\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n        \\n        \\n        \\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 94.19255287549959\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    time_penalty = -0.01  # Small negative reward to encourage faster completion\\n    control_effort = self.control_cost(action)\\n    \\n    # Implement exponential decay to the control cost to mitigate high penalization for control\\n    ctrl_cost = np.exp(-self._ctrl_cost_weight * control_effort)\\n\\n    # Total reward calculation\\n    reward = forward_reward + healthy_reward + time_penalty + ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_time\": time_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 94.19255287549959\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVtsHaYjb4XnoA4MFHTmN9wV8nEbK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward velocity as the primary goal\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Reward for being in a healthy state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Introduce a penalty for time taken to encourage faster hopping\\n    # This could be a small negative reward for each step taken\\n    time_penalty = -0.02  # Increased penalty for slow progress\\n    \\n    # Control cost penalty, but apply a square-root scale to reduce its harshness on control\\n    control_effort = self.control_cost(action)\\n    ctrl_cost = np.sqrt(control_effort)  # Reduced impact of control cost\\n\\n    # Total reward calculation incorporating forward movement, health, time penalty and control cost\\n    reward = forward_reward + healthy_reward + time_penalty - ctrl_cost\\n\\n    # Dictionary to provide detailed reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_time\": time_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746942649, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=245, prompt_tokens=2023, total_tokens=2268, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 3.154165219847654\n",
      "[1.57, 1.17, 0.72, 0.22, -0.12, 0.37, 0.03, 0.03, 0.02, 0.04]\n",
      "Successfully saved GPTHopper_refine_10_50_9.xml\n",
      "improved parameter [1.57, 1.17, 0.72, 0.22, -0.12, 0.37, 0.03, 0.03, 0.02, 0.04]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The hopper is a two-dimensional one-legged figure consisting of four main body parts - the torso at the top, the thigh in the middle, the leg at the bottom, and a single foot on which the entire body rests.\\nThe goal is to make hops that move in the forward (right) direction by applying torque to the three hinges that connect the four body parts, you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass HopperEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"hopper.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_state_range: Tuple[float, float] = (-100.0, 100.0),\\n        healthy_z_range: Tuple[float, float] = (0.7, float(\"inf\")),\\n        healthy_angle_range: Tuple[float, float] = (-0.2, 0.2),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_state_range,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_state_range = healthy_state_range\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n        state = self.state_vector()[2:]\\n\\n        min_state, max_state = self._healthy_state_range\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n\\n        is_healthy = all((healthy_state, healthy_z, healthy_angle))\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n        \\n        \\n        \\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 94.19255287549959\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    time_penalty = -0.01  # Small negative reward to encourage faster completion\\n    control_effort = self.control_cost(action)\\n    \\n    # Implement exponential decay to the control cost to mitigate high penalization for control\\n    ctrl_cost = np.exp(-self._ctrl_cost_weight * control_effort)\\n\\n    # Total reward calculation\\n    reward = forward_reward + healthy_reward + time_penalty + ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_time\": time_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 94.19255287549959\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVu4PuxZNm9PsYww5HPNtDezszjHv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward reward based on x_velocity, weighted by a factor\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Healthy reward based on the robot\\'s health status\\n    healthy_reward = self.healthy_reward \\n    \\n    # Encourage the robot to maintain balance by rewarding for z-position\\n    # This provides positive reinforcement for being upright\\n    z_reward = 1.0 if self.data.qpos[1] > 0.7 else 0.0\\n    \\n    # Introduce a velocity reward to encourage jumping with appropriate speed\\n    velocity_reward = 0.5 * np.clip(x_velocity, 0, 5.0)  # Normalizes velocity for more steady rewards\\n    \\n    # Control cost to penalize excessive torques\\n    control_effort = self.control_cost(action)\\n    \\n    # Small negative penalty over time to encourage faster trials without severely punishing the agent\\n    time_penalty = -0.01\\n    \\n    # Combining all parts of the rewards into a total reward\\n    reward = forward_reward + healthy_reward + z_reward + velocity_reward + time_penalty - control_effort\\n    \\n    # Creating a detailed reward information dictionary for analysis\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_health\": healthy_reward,\\n        \"reward_z\": z_reward,\\n        \"reward_velocity\": velocity_reward,\\n        \"reward_ctrl\": -control_effort,\\n        \"reward_time\": time_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746943401, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=326, prompt_tokens=2023, total_tokens=2349, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 5.310759650336114\n",
      "[1.6, 1.2, 0.75, 0.25, -0.1, 0.4, 0.04, 0.02, 0.02, 0.03]\n",
      "Successfully saved GPTHopper_refine_10_50_12.xml\n",
      "improved parameter [1.6, 1.2, 0.75, 0.25, -0.1, 0.4, 0.04, 0.02, 0.02, 0.03]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The hopper is a two-dimensional one-legged figure consisting of four main body parts - the torso at the top, the thigh in the middle, the leg at the bottom, and a single foot on which the entire body rests.\\nThe goal is to make hops that move in the forward (right) direction by applying torque to the three hinges that connect the four body parts, you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass HopperEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"hopper.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_state_range: Tuple[float, float] = (-100.0, 100.0),\\n        healthy_z_range: Tuple[float, float] = (0.7, float(\"inf\")),\\n        healthy_angle_range: Tuple[float, float] = (-0.2, 0.2),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_state_range,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_state_range = healthy_state_range\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n        state = self.state_vector()[2:]\\n\\n        min_state, max_state = self._healthy_state_range\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n\\n        is_healthy = all((healthy_state, healthy_z, healthy_angle))\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n        \\n        \\n        \\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 94.19255287549959\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    time_penalty = -0.01  # Small negative reward to encourage faster completion\\n    control_effort = self.control_cost(action)\\n    \\n    # Implement exponential decay to the control cost to mitigate high penalization for control\\n    ctrl_cost = np.exp(-self._ctrl_cost_weight * control_effort)\\n\\n    # Total reward calculation\\n    reward = forward_reward + healthy_reward + time_penalty + ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_time\": time_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 94.19255287549959\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVuGomFSsUlAWsCqaw59MwM7HaW47', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To design a new reward function for the Hopper environment aimed at improving the fitness of the agent, we will focus on unique components that haven\\'t been emphasized, while maintaining the core elements that reward forward motion and healthy states.\\n\\nWe will introduce the following concepts:\\n1. A reward for consistent velocity increase to encourage sustained forward movement.\\n2. A small penalty for too much lateral motion to discourage unnecessary side-to-side movements which could hinder forward progress.\\n3. Maintain control rewards but introduce a smoothness measure to prevent jerky movements by slightly penalizing large changes in action.\\n\\nThis should balance rewarding good movement with penalties for undesirable behaviors, promoting efficient forward hops. Below is the proposed reward function code:\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    # Reward for maintaining velocity increase\\n    velocity_increase = max(0, x_velocity - abs(self.prev_velocity))  # Reward sustained or increased speed\\n    self.prev_velocity = x_velocity  # Update the previous velocity for the next step \\n    velocity_reward = velocity_increase * 0.5  # Scale the velocity increase reward\\n    \\n    # Penalty for excessive lateral movement (assuming lateral velocity is some other part of the state)\\n    lateral_velocity = abs(self.data.qvel[1])  # Assuming the lateral velocity is at index 1\\n    lateral_penalty = -lateral_velocity  # Penalize lateral movement\\n    \\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Total reward calculation\\n    reward = forward_reward + healthy_reward + velocity_reward + lateral_penalty - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_velocity\": velocity_reward,\\n        \"reward_lateral\": lateral_penalty,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n```\\n\\nIn this implementation:\\n- We introduce `velocity_increase` to reward the agent for increasing speed, which encourages the agent to improve its motion efficiency.\\n- We add a `lateral_penalty` to discourage side movements which are not conducive to forward progress, thus enhancing the focus on straight line hops.\\n- Control costs are still included to ensure that excessive joint torques are disincentivized, promoting energy-efficient actions.\\n- The overall structure allows for clear breakdown in the reward info for debugging and understanding agent behavior. \\n\\nThis approach should yield a different but promising trajectory towards improving the agent\\'s ability to learn effective hopping while minimizing less desirable motion patterns.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746944170, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=535, prompt_tokens=2023, total_tokens=2558, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Error evaluating design: 'GPTHopperEnv' object has no attribute 'prev_velocity'\n",
      "[1.7, 1.3, 0.8, 0.3, -0.15, 0.45, 0.03, 0.015, 0.015, 0.025]\n",
      "Successfully saved GPTHopper_refine_10_50_15.xml\n",
      "improved parameter [1.7, 1.3, 0.8, 0.3, -0.15, 0.45, 0.03, 0.015, 0.015, 0.025]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The hopper is a two-dimensional one-legged figure consisting of four main body parts - the torso at the top, the thigh in the middle, the leg at the bottom, and a single foot on which the entire body rests.\\nThe goal is to make hops that move in the forward (right) direction by applying torque to the three hinges that connect the four body parts, you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass HopperEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"hopper.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-3,\\n        healthy_reward: float = 1.0,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_state_range: Tuple[float, float] = (-100.0, 100.0),\\n        healthy_z_range: Tuple[float, float] = (0.7, float(\"inf\")),\\n        healthy_angle_range: Tuple[float, float] = (-0.2, 0.2),\\n        reset_noise_scale: float = 5e-3,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            healthy_reward,\\n            terminate_when_unhealthy,\\n            healthy_state_range,\\n            healthy_z_range,\\n            healthy_angle_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n\\n        self._healthy_state_range = healthy_state_range\\n        self._healthy_z_range = healthy_z_range\\n        self._healthy_angle_range = healthy_angle_range\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def is_healthy(self):\\n        z, angle = self.data.qpos[1:3]\\n        state = self.state_vector()[2:]\\n\\n        min_state, max_state = self._healthy_state_range\\n        min_z, max_z = self._healthy_z_range\\n        min_angle, max_angle = self._healthy_angle_range\\n\\n        healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\\n        healthy_z = min_z < z < max_z\\n        healthy_angle = min_angle < angle < max_angle\\n\\n        is_healthy = all((healthy_state, healthy_z, healthy_angle))\\n\\n        return is_healthy\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = np.clip(self.data.qvel.flatten(), -10, 10)\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": x_position_after,\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n            \"x_velocity\": x_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"z_distance_from_origin\": self.data.qpos[1] - self.init_qpos[1],\\n        }\\n        \\n        \\n        \\nThere are also some reward functions and their fitness. \\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    costs = ctrl_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info\\n\\nfitness: 94.19255287549959\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    healthy_reward = self.healthy_reward\\n    time_penalty = -0.01  # Small negative reward to encourage faster completion\\n    control_effort = self.control_cost(action)\\n    \\n    # Implement exponential decay to the control cost to mitigate high penalization for control\\n    ctrl_cost = np.exp(-self._ctrl_cost_weight * control_effort)\\n\\n    # Total reward calculation\\n    reward = forward_reward + healthy_reward + time_penalty + ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_time\": time_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 94.19255287549959\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVuN57rG02587wKAynOxCfkPf0hie', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here\\'s a proposed reward function that aims to encourage the hopper to move effectively while incorporating elements to avoid excessive control efforts and promote faster movements. The adjustments include a time penalty adjusted to provide a stronger incentive for speed, the use of a normalized control cost to reduce the negative impact of high control efforts, and the inclusion of a cumulative motion reward based on distance traveled.\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward reward based on the x_velocity\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Healthy reward to promote staying within healthy state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Time penalty to discourage long episodes\\n    time_penalty = -0.05 * self.dt  # Encourages quicker completion of each step\\n    \\n    # Normalize and decay the control effort to avoid heavy penalization\\n    control_effort = self.control_cost(action)\\n    normalized_ctrl_cost = np.clip(np.exp(-self._ctrl_cost_weight * control_effort), 0, 1)\\n    \\n    # Cumulative motion reward based on distance traveled\\n    distance_reward = (self.data.qpos[0] - self.init_qpos[0]) * self._forward_reward_weight\\n    \\n    # Total reward calculation\\n    reward = forward_reward + healthy_reward + time_penalty + normalized_ctrl_cost + distance_reward\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": normalized_ctrl_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_time\": time_penalty,\\n        \"reward_distance\": distance_reward,\\n    }\\n\\n    return reward, reward_info\\n``` \\n\\nThis function builds upon the insights gained from previous reward functions, reinforcing positive behaviors while aiming to optimize the learning process for the hopper environment.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746944559, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=365, prompt_tokens=2023, total_tokens=2388, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 2.9658920320127335\n"
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "coarse_best = [(10,50)]\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_hopper_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "    print(f\"Initial parameter:{parameter}\")\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        iteration +=1   \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list[morphology_index],  # 这本身已经是list结构，可以保留\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],  # 👈 用 [] 包装成列表\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        print(\"improved parameter\", improved_parameter)\n",
    "        shutil.copy(improved_morphology, \"GPTHopper.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTHopperEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "        improved_material = compute_hopper_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            iteration +=1\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            # break\n",
    "            \n",
    "        iteration +=1        \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            [rewardfunc_list[rewardfunc_index]],\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        shutil.copy(best_morphology, \"GPTHopper.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTHopperEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_hopper_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "    logging.info(\"____________________________________________\")\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_morphology': 'results/Div_m25_r5/assets/GPTHopper_refine_10_50_12.xml',\n",
       "  'best_parameter': [1.6, 1.2, 0.75, 0.25, -0.1, 0.4, 0.04, 0.02, 0.02, 0.03],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_10_50_5.py',\n",
       "  'best_fitness': 9.248461495176302,\n",
       "  'best_material': 0.00506634175268914,\n",
       "  'best_efficiency': 1825.4713058524633,\n",
       "  'best_iteration': 16}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 07:18:59,973 - Final optimized result: rewardfunc_index1 morphology_index13\n",
    "2025-04-07 07:18:59,973 -   Morphology: results/Div_m25_r5/assets/GPTHopper_refine_1_13_0.xml\n",
    "2025-04-07 07:18:59,973 -   Parameter: [1.75, 1.15, 0.75, 0.25, 0.2, -0.2, 0.01, 0.025, 0.035, 0.025]\n",
    "2025-04-07 07:18:59,973 -   Rewardfunc: results/Div_m25_r5/env/GPTrewardfunc_1.py\n",
    "2025-04-07 07:18:59,973 -   Fitness: 9.31747932141698\n",
    "2025-04-07 07:18:59,973 -   Material: 0.00399820025046861\n",
    "2025-04-07 07:18:59,973 -   Efficiency: 2330.4183726977967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "3e6 steps train\n",
      "\n",
      "best_fitness:15.099855627034716\n",
      "best_efficiency:3776.6631687007007\n"
     ]
    }
   ],
   "source": [
    "# robodesign best\n",
    "\n",
    "best_morphology = \"results/Div_m25_r5/assets/GPTHopper_refine_1_13_0.xml\"\n",
    "best_rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_1.py\"\n",
    "morphology_index=999\n",
    "rewardfunc_index=999\n",
    "best_parameter = [1.75, 1.15, 0.75, 0.25, 0.2, -0.2, 0.01, 0.025, 0.035, 0.025]\n",
    "\n",
    "shutil.copy(best_morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "best_fitness, _ = Eva(model_path)\n",
    "best_material = compute_hopper_volume(best_parameter)\n",
    "best_efficiency = best_fitness / best_material\n",
    "\n",
    "logging.info(\"3e6 steps train\\n\")\n",
    "logging.info(f\"best_fitness:{best_fitness}\")\n",
    "logging.info(f\"best_efficiency:{best_efficiency}\")\n",
    "print(\"3e6 steps train\\n\")\n",
    "print(f\"best_fitness:{best_fitness}\")\n",
    "print(f\"best_efficiency:{best_efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 02:15:56,188 - Initial morphology:results/Div_m25_r5/assets/GPTHopper_6.xml\n",
    "2025-04-07 02:15:56,188 - Initial parameter:[1.0, 0.8, 0.5, 0.2, 0.18, -0.2, 0.03, 0.02, 0.02, 0.03]\n",
    "2025-04-07 02:15:56,188 - Initial rewardfunc:results/Div_m25_r5/env/GPTrewardfunc_2.py\n",
    "2025-04-07 02:15:56,188 - Initial fitness:5.2437351113867665\n",
    "2025-04-07 02:15:56,188 - Initial efficiency:1951.4412234802828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "3e6 steps train\n",
      "\n",
      "fitness:7.887110028279259\n",
      "efficiency:2935.1657389952516\n"
     ]
    }
   ],
   "source": [
    "# coarse only best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTHopper_6.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_2.py\"\n",
    "\n",
    "morphology_index=777\n",
    "rewardfunc_index=777\n",
    "parameter = [1.0, 0.8, 0.5, 0.2, 0.18, -0.2, 0.03, 0.02, 0.02, 0.03]\n",
    "\n",
    "shutil.copy(morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块rewardfunc\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_hopper_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"coarse only best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "human 3e6 steps train\n",
      "\n",
      "fitness:6.8610045928497785\n",
      "efficiency:433.6914525166767\n"
     ]
    }
   ],
   "source": [
    "# human\n",
    "\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTHopper_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "\n",
    "morphology_index=888\n",
    "rewardfunc_index=888\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_hopper_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"human best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"human 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 10:50:19,543 - morphology: 50, rewardfunc: 2, material cost: 0.005429560128926578 reward: 1016.4147318163663 fitness: 3.0493164867488556 efficiency: 561.6139087406524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Morphology Design) 3e6 steps train\n",
      "\n",
      "fitness:4.233495359427274\n",
      "efficiency:267.60377825224765\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Morphology Design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTHopper_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_2.py\"\n",
    "\n",
    "morphology_index=555\n",
    "rewardfunc_index=555\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_hopper_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign (w/o Morphology Design) best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Morphology Design) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 09:35:14,008 - morphology: 18, rewardfunc: 10, material cost: 0.0022881266493645657 reward: 1471.1455354782527 fitness: 3.783628418579109 efficiency: 1653.5922168599618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Reward Shaping) 3e6 steps train\n",
      "\n",
      "fitness:4.457903414345486\n",
      "efficiency:1948.2765150187327\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Reward Shaping)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTHopper_18.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "\n",
    "morphology_index=444\n",
    "rewardfunc_index=444\n",
    "\n",
    "parameter =  [0.9, 0.4, 0.15, 0.05, 0.05, -0.05, 0.02, 0.03, 0.04, 0.01]\n",
    "\n",
    "shutil.copy(morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_hopper_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign (w/o Reward Shaping) best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Reward Shaping) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-08 00:50:43,757 - Initial morphology:results/noDiv_m25_r5/assets/GPTHopper_16.xml\n",
    "2025-04-08 00:50:43,757 - Initial parameter:[1.5, 1.1, 0.7, 0.3, 0.4, -0.1, 0.06, 0.04, 0.03, 0.02]\n",
    "2025-04-08 00:50:43,757 - Initial rewardfunc:results/noDiv_m25_r5/env/GPTrewardfunc_2.py\n",
    "2025-04-08 00:50:43,757 - Initial fitness:1.0491079684899782\n",
    "2025-04-08 00:50:43,757 - Initial efficiency:1222.0645575053077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Diversity Reflection) 3e6 steps train\n",
      "\n",
      "fitness:4.726470799983718\n",
      "efficiency:491.66090927902854\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Diversity Reflection)\n",
    "\n",
    "morphology = \"results/noDiv_m25_r5/assets/GPTHopper_16.xml\"\n",
    "rewardfunc = \"results/noDiv_m25_r5/env/GPTrewardfunc_2.py\"\n",
    "\n",
    "morphology_index=333\n",
    "rewardfunc_index=333\n",
    "\n",
    "parameter =  [1.5, 1.1, 0.7, 0.3, 0.4, -0.1, 0.06, 0.04, 0.03, 0.02]\n",
    "\n",
    "shutil.copy(morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_hopper_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign (w/o Diversity Reflection) best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Diversity Reflection) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-09 04:00:22,222 - iteration:2, morphology: 0, rewardfunc: 2, material cost: 0.015820013405927 reward: 1428.1315493516208 fitness: 3.4351751639953476 efficiency: 217.14110322487792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      " eureka reward 3e6 steps train\n",
      "\n",
      "fitness:6.7727740674190535\n",
      "efficiency:428.1143064570109\n"
     ]
    }
   ],
   "source": [
    "# eureka reward\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTHopper_50.xml\"\n",
    "rewardfunc = \"results/eureka/env/GPTrewardfunc_2_2.py\"\n",
    "\n",
    "\n",
    "morphology_index=222\n",
    "rewardfunc_index=222\n",
    "\n",
    "parameter = [1.45, 1.06, 0.6, 0.1, -0.13, 0.26, 0.05, 0.05, 0.04, 0.06]\n",
    "\n",
    "shutil.copy(morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_hopper_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" eureka reward best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" eureka reward 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      " eureka reward 3e6 steps train\n",
      "\n",
      "fitness:1.608018014941748\n",
      "efficiency:609.0407908588114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "morphology = \"results/eureka_morphology/assets/GPTHopper_6.xml\"\n",
    "rewardfunc = \"results/eureka_morphology/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "parameter = [0.7, 0.42, 0.22, 0.09, -0.05, 0.09, 0.035, 0.035, 0.025, 0.015]\n",
    "\n",
    "shutil.copy(morphology, \"GPTHopper.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTHopperEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_hopper_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" eureka reward best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" eureka reward 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
