{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTAnt import GPTAntEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        api_key = \"<api_key>\"\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = \"gpt-4-turbo\"\n",
    "        \n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTAnt_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        # env_path = os.path.join(os.path.dirname(__file__), \"env\", \"ant_v5.py\")\n",
    "        # with open(env_path, \"r\") as f:\n",
    "        #     env_content = f.read().rstrip()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums\n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_ant_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = ant_design(parameter)  \n",
    "            filename = f\"GPTAnt_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_ant_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(initial_parameter)})\n",
    "\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "\n",
    "        xml_file = ant_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTAnt_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_ant_volume(diverse_parameter['parameters']))\n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistant\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = ant_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTAnt_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\":reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "    \n",
    "    \n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\"best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\" \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = ant_design(parameter)  \n",
    "        filename = f\"GPTAnt_refine_{step}_{rewardfunc_index}_{morphology_index}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"parameters.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 26\n",
    "rewardfunc_nums = 6\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n",
    "\n",
    "\n",
    "\n",
    "# return file list of morphology and reward function: [GPTAnt_{i}.xml] and [GPTAnt_{j}.py]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter coarse optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.3, 0.1, 0.05, 0.15, 0.1, 0.1, 0.1, 0.03, 0.03, 0.03]\n",
      "params: [0.25, 0.2, 0.1, 0.1, 0.2, 0.15, 0.15, 0.02, 0.02, 0.02]\n",
      "params: [0.15, 0.25, 0.15, 0.25, 0.1, 0.2, 0.1, 0.04, 0.04, 0.04]\n",
      "params: [0.2, 0.05, 0.1, 0.05, 0.05, 0.05, 0.05, 0.01, 0.01, 0.01]\n",
      "params: [0.4, 0.15, 0.2, 0.3, 0.15, 0.25, 0.2, 0.05, 0.05, 0.05]\n",
      "params: [0.1, 0.3, 0.2, 0.2, 0.3, 0.15, 0.1, 0.02, 0.02, 0.02]\n",
      "params: [0.5, 0.4, 0.3, 0.35, 0.25, 0.3, 0.25, 0.08, 0.06, 0.06]\n",
      "params: [0.2, 0.1, 0.2, 0.15, 0.1, 0.2, 0.15, 0.025, 0.025, 0.025]\n",
      "params: [0.35, 0.07, 0.12, 0.4, 0.2, 0.5, 0.3, 0.015, 0.015, 0.015]\n",
      "params: [0.45, 0.2, 0.05, 0.05, 0.2, 0.1, 0.05, 0.025, 0.025, 0.025]\n",
      "params: [0.6, 0.25, 0.2, 0.3, 0.45, 0.35, 0.4, 0.1, 0.08, 0.08]\n",
      "params: [0.15, 0.07, 0.08, 0.1, 0.05, 0.12, 0.06, 0.02, 0.02, 0.02]\n",
      "params: [0.5, 0.35, 0.25, 0.2, 0.3, 0.15, 0.2, 0.05, 0.05, 0.04]\n",
      "params: [0.1, 0.15, 0.05, 0.4, 0.15, 0.3, 0.2, 0.04, 0.04, 0.02]\n",
      "params: [0.25, 0.5, 0.15, 0.1, 0.25, 0.18, 0.12, 0.03, 0.02, 0.02]\n",
      "params: [0.2, 0.05, 0.1, 0.5, 0.05, 0.25, 0.05, 0.015, 0.015, 0.015]\n",
      "params: [0.35, 0.2, 0.2, 0.35, 0.2, 0.1, 0.1, 0.06, 0.06, 0.06]\n",
      "params: [0.05, 0.4, 0.2, 0.25, 0.2, 0.15, 0.15, 0.01, 0.01, 0.01]\n",
      "params: [0.1, 0.2, 0.3, 0.15, 0.25, 0.2, 0.2, 0.01, 0.015, 0.02]\n",
      "params: [0.55, 0.1, 0.05, 0.05, 0.1, 0.08, 0.03, 0.02, 0.02, 0.02]\n",
      "params: [0.3, 0.08, 0.04, 0.2, 0.1, 0.15, 0.07, 0.025, 0.03, 0.035]\n",
      "params: [0.4, 0.12, 0.18, 0.22, 0.08, 0.3, 0.15, 0.02, 0.015, 0.01]\n",
      "params: [0.25, 0.4, 0.3, 0.4, 0.3, 0.12, 0.12, 0.05, 0.04, 0.04]\n",
      "params: [0.2, 0.3, 0.15, 0.1, 0.05, 0.15, 0.2, 0.03, 0.02, 0.02]\n",
      "params: [0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015]\n",
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n"
     ]
    }
   ],
   "source": [
    "morphology_list = [f'results/Div_m25_r5/assets/GPTAnt_{i}.xml' for i in range(0,26) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,6)]\n",
    "\n",
    "parameter_list = [[0.3, 0.1, 0.05, 0.15, 0.1, 0.1, 0.1, 0.03, 0.03, 0.03],\n",
    " [0.25, 0.2, 0.1, 0.1, 0.2, 0.15, 0.15, 0.02, 0.02, 0.02],\n",
    " [0.15, 0.25, 0.15, 0.25, 0.1, 0.2, 0.1, 0.04, 0.04, 0.04],\n",
    " [0.2, 0.05, 0.1, 0.05, 0.05, 0.05, 0.05, 0.01, 0.01, 0.01],\n",
    " [0.4, 0.15, 0.2, 0.3, 0.15, 0.25, 0.2, 0.05, 0.05, 0.05],\n",
    " [0.1, 0.3, 0.2, 0.2, 0.3, 0.15, 0.1, 0.02, 0.02, 0.02],\n",
    " [0.5, 0.4, 0.3, 0.35, 0.25, 0.3, 0.25, 0.08, 0.06, 0.06],\n",
    " [0.2, 0.1, 0.2, 0.15, 0.1, 0.2, 0.15, 0.025, 0.025, 0.025],\n",
    " [0.35, 0.07, 0.12, 0.4, 0.2, 0.5, 0.3, 0.015, 0.015, 0.015],\n",
    " [0.45, 0.2, 0.05, 0.05, 0.2, 0.1, 0.05, 0.025, 0.025, 0.025],\n",
    " [0.6, 0.25, 0.2, 0.3, 0.45, 0.35, 0.4, 0.1, 0.08, 0.08],\n",
    " [0.15, 0.07, 0.08, 0.1, 0.05, 0.12, 0.06, 0.02, 0.02, 0.02],\n",
    " [0.5, 0.35, 0.25, 0.2, 0.3, 0.15, 0.2, 0.05, 0.05, 0.04],\n",
    " [0.1, 0.15, 0.05, 0.4, 0.15, 0.3, 0.2, 0.04, 0.04, 0.02],\n",
    " [0.25, 0.5, 0.15, 0.1, 0.25, 0.18, 0.12, 0.03, 0.02, 0.02],\n",
    " [0.2, 0.05, 0.1, 0.5, 0.05, 0.25, 0.05, 0.015, 0.015, 0.015],\n",
    " [0.35, 0.2, 0.2, 0.35, 0.2, 0.1, 0.1, 0.06, 0.06, 0.06],\n",
    " [0.05, 0.4, 0.2, 0.25, 0.2, 0.15, 0.15, 0.01, 0.01, 0.01],\n",
    " [0.1, 0.2, 0.3, 0.15, 0.25, 0.2, 0.2, 0.01, 0.015, 0.02],\n",
    " [0.55, 0.1, 0.05, 0.05, 0.1, 0.08, 0.03, 0.02, 0.02, 0.02],\n",
    " [0.3, 0.08, 0.04, 0.2, 0.1, 0.15, 0.07, 0.025, 0.03, 0.035],\n",
    " [0.4, 0.12, 0.18, 0.22, 0.08, 0.3, 0.15, 0.02, 0.015, 0.01],\n",
    " [0.25, 0.4, 0.3, 0.4, 0.3, 0.12, 0.12, 0.05, 0.04, 0.04],\n",
    " [0.2, 0.3, 0.15, 0.1, 0.05, 0.15, 0.2, 0.03, 0.02, 0.02],\n",
    "[0.08, 0.32, 0.27, 0.16, 0.04, 0.14, 0.045, 0.02, 0.015, 0.015],\n",
    "[0.25, 0.2, 0.2, 0.2, 0.2,0.4,0.4, 0.08, 0.08, 0.08 ]\n",
    "                 ]\n",
    "\n",
    "\n",
    "material_list = [compute_ant_volume(parameter) for parameter in parameter_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'morphology_nums:{morphology_nums}')\n",
    "logging.info(f'rewardfunc_nums:{rewardfunc_nums}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'morphology_list:{morphology_list}')\n",
    "logging.info(f'material_list:{material_list}')\n",
    "logging.info(f'_________________________________enter coarse optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [65.76430696628158 9.332153046637826 27.49097298587031 228.2097970494801\n",
      "  13.050052751227259 1552.2917121961602 0.9104570123156992\n",
      "  48.89612689745986 7.034496743763769 7.4182394798083555\n",
      "  4.6163442674044255 54.76806618536609 5.081310178591359\n",
      "  50.756260826484144 0.3998242407655389 78.19442201715779\n",
      "  3.4217758073483178 3026.527223369712 379.47465765581677\n",
      "  -4.014168091877938 21.420420550229224 60.15076306164326\n",
      "  1.1030938076895507 70.36574217207183 2912.473029236097\n",
      "  8.679565785234878]]\n"
     ]
    }
   ],
   "source": [
    "efficiency_matrix = np.array([[70.12901472277564, 3.8950108978919857, 36.38857403994179,\n",
    "        304.4116553571402, 8.020361623253192, 2017.8516590285078,\n",
    "        1.2415581186726266, 96.21844827615774, 6.819354811214885,\n",
    "        17.44389647393199, 2.961451393360138, 48.904232779343815,\n",
    "        2.677638986129011, 6.792677716341557, 0.1082644939638393,\n",
    "        178.92171372457375, 0.5305996358522651, 2777.3886334384847,\n",
    "        429.7795806064214, -3.9330932345003826, 9.731541015144876,\n",
    "        110.05889487574825, 1.3136332680205476, 79.87286091934128,\n",
    "        991.6757041414498, 15.258911788833924],\n",
    "       [63.24032185908237, 7.058282876583571, 29.60644096432709,\n",
    "        196.65112945445128, 10.592262912674121, 2328.240901485146,\n",
    "        14.249609494065261, 91.78182496369733, 10.319916115600542,\n",
    "        12.874172232006638, 4.474624645915348, 80.88527697953651,\n",
    "        14.273630365535217, 15.933260766873094, 0.4474799031617528,\n",
    "        195.83407065661854, 4.3861795690295775, 3859.6762180864503,\n",
    "        553.0126300970579, -3.9431994809443767, 23.35683214655262,\n",
    "        108.76369206964385, 1.5505922045510778, 164.08396086658882,\n",
    "        3142.5781115169507, 16.029479624540965],\n",
    "       [64.43158787667042, 8.141649213230023, 34.03087821710788,\n",
    "        239.54529125822526, 14.986610488684708, 2909.1893716011145,\n",
    "        2.1217889315831404, 96.52944382960696, 13.438467711297411,\n",
    "        16.942951352754896, 3.522105055618922, 40.78230525373979,\n",
    "        7.147495893693545, -0.7721329475976957, 0.15241059365665097,\n",
    "        137.75465803123544, 5.304310064373623, 2629.51604113934,\n",
    "        815.8128430519389, -3.9466165400134794, 24.732374620766524,\n",
    "        44.57757014440089, 1.0228895460372942, 217.98712766789797,\n",
    "        1864.959214669196, 18.03633557101774],\n",
    "       [72.84319575651284, 12.491211315362388, 25.668535935748118,\n",
    "        151.76169703159263, 11.29488863559477, 4846.415432733164,\n",
    "        8.879156521089088, 78.3430023117369, 20.23957107441512,\n",
    "        5.283127328548385, 6.675908657060871, 86.84222482929549,\n",
    "        4.323941019522104, -8.155150851430042, 0.2709330317808642,\n",
    "        242.33690702783886, 1.1519432201609145, 3927.5924351284716,\n",
    "        776.8823685360268, -3.9006578615629777, 13.468037088493993,\n",
    "        51.47757253378492, 1.2405526954218664, 90.24069454088942,\n",
    "        2662.2264439903734, 17.47779279594377],\n",
    "       [49.57189495703011, 3.798243661884369, 26.097717711919106,\n",
    "        328.0945829646423, 4.146449930076461, 2983.9363184163567,\n",
    "        14.596416580989677, 100.24855867883234, 15.567627652266964,\n",
    "        5.166609489553714, 5.364329426378386, 36.79022401302955,\n",
    "        4.897485971948941, 25.99373528155112, 0.43409586104502706,\n",
    "        141.97469571998894, 0.22459944692637557, 2364.93178435668,\n",
    "        197.7430958007017, -3.987572097722751, 12.964421558578492,\n",
    "        105.83639972826796, 1.2150169196799276, 100.11125346604726,\n",
    "        6537.446107236452, 15.376241750159544],\n",
    "       [65.76430696628158, 9.332153046637826, 27.49097298587031,\n",
    "        228.2097970494801, 13.050052751227259, 1552.2917121961602,\n",
    "        0.9104570123156992, 48.89612689745986, 7.034496743763769,\n",
    "        7.4182394798083555, 4.6163442674044255, 54.76806618536609,\n",
    "        5.081310178591359, 50.756260826484144, 0.3998242407655389,\n",
    "        78.19442201715779, 3.4217758073483178, 3026.527223369712,\n",
    "        379.47465765581677, -4.014168091877938, 21.420420550229224,\n",
    "        60.15076306164326, 1.1030938076895507, 70.36574217207183,\n",
    "        2912.473029236097, 8.679565785234878]], dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitness_matrix = np.array([[8.370409798250126, 0.2694031161199691, 1.2053973361422903,\n",
    "        10.31310120379217, 2.428685533666429, 18.40640581562191,\n",
    "        0.7657339535055587, 3.7940199974245616, 1.2484189790466274,\n",
    "        6.7439026027340105, 3.1544368738885797, 0.7976274302844721,\n",
    "        1.4960668379240454, 0.1361814876394193, 0.008067331072490797,\n",
    "        6.465820806363407, 0.12091377508465335, 5.012471511630434,\n",
    "        3.0494048294076372, -2.7487032997006002, 1.170796293860529,\n",
    "        29.766605852576514, 0.12986853889942698, 3.1823596808905643,\n",
    "        5.333683749926142, 2.779807766447567],\n",
    "       [7.548194022492659, 0.4881946292979448, 0.9807343654599262,\n",
    "        6.6623040353839755, 3.207495735630992, 21.237709262482284,\n",
    "        8.788480901294038, 3.619078103535323, 1.8892665798378692,\n",
    "        4.977223050665417, 4.7662173390834734, 1.319237864667737,\n",
    "        7.975050093490848, 0.31943443289816575, 0.03334397451021867,\n",
    "        7.076994637976672, 0.9995286352556212, 6.965721993081067,\n",
    "        3.923777352478973, -2.7557662070083957, 2.8100475013102706,\n",
    "        29.41630439378042, 0.15329479614758057, 6.537567020041629,\n",
    "        16.902216860080227, 2.920186745231664],\n",
    "       [7.690380317072141, 0.5631269657260631, 1.1272969890069966,\n",
    "        8.115506709950298, 4.5381699482271705, 26.53699538744324,\n",
    "        1.3086184228109425, 3.8062829612328115, 2.460179680414685,\n",
    "        6.550234570389192, 3.751626005432334, 0.6651588930428338,\n",
    "        3.9934926389057277, -0.015479935579204317, 0.01135687863089945,\n",
    "        4.978137731471474, 1.2087534758156344, 4.745599548763368,\n",
    "        5.788417448742299, -2.758154271816045, 2.975538252297298,\n",
    "        12.05648086737562, 0.10112500500199245, 8.68522096315044,\n",
    "        10.030600342445501, 3.2857877673333813],\n",
    "       [8.694367116185184, 0.8639696629072092, 0.8502884670792923,\n",
    "        5.141503989096024, 3.420261320151747, 44.20795196061497,\n",
    "        5.476241123498292, 3.089167646685705, 3.705257367677796,\n",
    "        2.042484957120455, 7.110949881453576, 1.416395610239175,\n",
    "        2.4158987832031924, -0.163496727100688, 0.02018858063087549,\n",
    "        8.757500601757037, 0.2625064060006432, 7.088293281449661,\n",
    "        5.512194979465266, -2.7260353355043763, 1.6203320608981857,\n",
    "        13.922660350083554, 0.12264364027941445, 3.595443365581854,\n",
    "        14.318666741188752, 3.1840346695023825],\n",
    "       [5.916767502101989, 0.2627100937890489, 0.8645054179592436,\n",
    "        11.115450341612473, 1.255607095327473, 27.21882084790471,\n",
    "        9.002375005629885, 3.9529325524856374, 2.849964895192011,\n",
    "        1.9974385445350915, 5.7139004827812006, 0.6000480975032809,\n",
    "        2.736353328356418, 0.5211296174748321, 0.03234666232696633,\n",
    "        5.130640225012216, 0.05118203099814166, 4.268093076107038,\n",
    "        1.4030418812960872, -2.786776699484189, 1.559742356242155,\n",
    "        28.624586855280924, 0.12011911995399198, 3.988714226168625,\n",
    "        35.16136366209836, 2.8011824714228344],\n",
    "       [7.849450068302702, 0.6454695960500014, 0.910657987552753,\n",
    "        7.731473783114316, 3.9517512824607057, 14.159668809681037,\n",
    "        0.5615265504306113, 1.9280386097386324, 1.2878050029767925,\n",
    "        2.8679306031393277, 4.917172239366828, 0.8932664804330582,\n",
    "        2.839060713851651, 1.0175756005229313, 0.02979291181225311,\n",
    "        2.8257672604141844, 0.779758979094023, 5.462102531734381,\n",
    "        2.692477506866849, -2.8053637231153403, 2.577078897793551,\n",
    "        16.268417539618518, 0.1090541664566055, 2.803569300346892,\n",
    "        15.664606890398295, 1.5812087200637983]], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix_select = efficiency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.25, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.08, 0.08, 0.08]\n",
      "ChatCompletion(id='chatcmpl-BVtOb10ctLwLHqRLXto2rzQRRt2Mj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.25, 0.18, 0.18, 0.22, 0.22, 0.42, 0.4, 0.07, 0.07, 0.07],\\n  \"desciption\": \"Optimized for reduced material cost while maintaining effective control over leg movement with slightly longer legs and ankles.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746940809, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=84, prompt_tokens=2512, total_tokens=2596, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.25, 0.18, 0.18, 0.22, 0.22, 0.42, 0.4, 0.07, 0.07, 0.07]\n",
      "Successfully saved GPTAnt_refine_5_25_0.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.18, 0.18, 0.22, 0.22, 0.42, 0.4, 0.07, 0.07, 0.07]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nfitness: 8.679565785234878\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nbest fitness: 8.679565785234878\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVtUst3nyKtJ8WRfWPNCfczmUehZs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward reward based on the forward velocity\\n    forward_reward = x_velocity * self._forward_reward_weight\\n\\n    # Healthy reward for keeping the robot in a healthy state\\n    healthy_reward = self.healthy_reward\\n\\n    # Rewards accumulation\\n    rewards = forward_reward + healthy_reward\\n\\n    # Control cost to encourage efficient movement\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Contact cost to minimize unwanted interactions with the ground\\n    contact_cost = self.contact_cost\\n\\n    # Combining costs\\n    costs = ctrl_cost + contact_cost\\n\\n    # Total reward calculation\\n    reward = rewards - costs\\n\\n    # Normalization of the total reward (optional experimentation)\\n    normalized_reward = np.clip(reward, -1.0, 1.0)  # Clipping the reward for stability\\n\\n    # Exploring potential exponential scaling (may improve agent motivation)\\n    # Using an exponential transformation to emphasize rewards\\n    exp_reward = np.exp(normalized_reward) - 1  # Shift to keep it in a range that avoids overflow\\n\\n    # New fitness optimization: encouraging speed while factoring in costs\\n    final_reward = exp_reward + costs * -0.1  # Penalize costs slightly to encourage forward velocity indirectly\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return final_reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746941198, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=319, prompt_tokens=2182, total_tokens=2501, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.18, 0.18, 0.22, 0.22, 0.42, 0.4, 0.07, 0.07, 0.07]\n",
      "ChatCompletion(id='chatcmpl-BVtbHqZtcO4ENtZsxkZeFyeRnwXep', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"parameters\": [0.25, 0.15, 0.15, 0.20, 0.20, 0.45, 0.45, 0.06, 0.06, 0.06],\\n  \"description\": \"Reduced size of torso and leg parts to minimize material costs while increasing leg and ankle lengths for better movement efficiency.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746941595, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=84, prompt_tokens=2512, total_tokens=2596, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "[0.25, 0.15, 0.15, 0.2, 0.2, 0.45, 0.45, 0.06, 0.06, 0.06]\n",
      "Successfully saved GPTAnt_refine_5_25_2.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "params: [0.25, 0.15, 0.15, 0.2, 0.2, 0.45, 0.45, 0.06, 0.06, 0.06]\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\n\\nYou are a reward engineer trying to write reward functions to solve refiforcement learning tasks as effective as possible.\\nYou goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The ant is a 3D quadruped robot consisting of a torso (free rotational body) with four legs attached to it, where each leg has two body parts. \\nThe goal is to coordinate the four legs to move in the forward (right) direction by applying torque to the eight hinges connecting the two body parts of each leg and the torso (nine body parts and eight hinges), You should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass AntEnv(MujocoEnv, utils.EzPickle):\\n    \\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"ant.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1,\\n        ctrl_cost_weight: float = 0.5,\\n        contact_cost_weight: float = 5e-4,\\n        healthy_reward: float = 1.0,\\n        main_body: Union[int, str] = 1,\\n        terminate_when_unhealthy: bool = True,\\n        healthy_z_range: Tuple[float, float] = (0.2, 1.8),\\n        contact_force_range: Tuple[float, float] = (-1.0, 1.0),\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        include_cfrc_ext_in_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            contact_cost_weight,\\n            healthy_reward,\\n            main_body,\\n            terminate_when_unhealthy,\\n            healthy_z_range,\\n            contact_force_range,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            include_cfrc_ext_in_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n        self._contact_cost_weight = contact_cost_weight\\n\\n        self._healthy_reward = healthy_reward\\n        self._terminate_when_unhealthy = terminate_when_unhealthy\\n        self._healthy_z_range = healthy_z_range\\n\\n        self._contact_force_range = contact_force_range\\n\\n        self._main_body = main_body\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n        self._include_cfrc_ext_in_observation = include_cfrc_ext_in_observation\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,  # needs to be defined after\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = self.data.qpos.size + self.data.qvel.size\\n        obs_size -= 2 * exclude_current_positions_from_observation\\n        obs_size += self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation\\n\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n            \"cfrc_ext\": self.data.cfrc_ext[1:].size * include_cfrc_ext_in_observation,\\n        }\\n\\n    @property\\n    def healthy_reward(self):\\n        return self.is_healthy * self._healthy_reward\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    @property\\n    def contact_forces(self):\\n        raw_contact_forces = self.data.cfrc_ext\\n        min_value, max_value = self._contact_force_range\\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\\n        return contact_forces\\n\\n    @property\\n    def contact_cost(self):\\n        contact_cost = self._contact_cost_weight * np.sum(\\n            np.square(self.contact_forces)\\n        )\\n        return contact_cost\\n\\n    @property\\n    def is_healthy(self):\\n        state = self.state_vector()\\n        min_z, max_z = self._healthy_z_range\\n        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\\n        return is_healthy\\n\\n    def step(self, action):\\n        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        terminated = (not self.is_healthy) and self._terminate_when_unhealthy\\n        info = {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, terminated, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        if self._include_cfrc_ext_in_observation:\\n            contact_force = self.contact_forces[1:].flatten()\\n            return np.concatenate((position, velocity, contact_force))\\n        else:\\n            return np.concatenate((position, velocity))\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\nPlease carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nfitness: 8.679565785234878\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    healthy_reward = self.healthy_reward\\n    rewards = forward_reward + healthy_reward\\n\\n    ctrl_cost = self.control_cost(action)\\n    contact_cost = self.contact_cost\\n    costs = ctrl_cost + contact_cost\\n\\n    reward = rewards - costs\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n    }\\n\\n    return reward, reward_info    \\n\\nbest fitness: 8.679565785234878\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVthP1Zr6mkab9aE2Un8HF6ZOYgP3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here\\'s a refined reward function that builds on the existing one while incorporating additional factors to potentially enhance the agent\\'s performance in moving forward quickly. This version adds a velocity penalty that encourages acceleration, rewards for maintaining a stable height, and tries to further reinforce healthy states.\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Forward reward based on x velocity\\n    forward_reward = x_velocity * self._forward_reward_weight\\n    \\n    # Healthy reward if in a proper state\\n    healthy_reward = self.healthy_reward\\n    \\n    # Introducing a velocity-based bonus for acceleration\\n    velocity_bonus = max(0, x_velocity - self.last_x_velocity) * 0.2  # Change in velocity\\n    self.last_x_velocity = x_velocity  # Update for next step\\n    \\n    # Control cost based on action taken\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Contact cost based on the sum of contact forces\\n    contact_cost = self.contact_cost\\n    \\n    # Height penalty to encourage maintaining a good z position\\n    height_penalty = -0.1 * (self.data.qpos[2] - 0.5)**2  # Encouraging z position around 0.5\\n\\n    # Total rewards and costs\\n    rewards = forward_reward + healthy_reward + velocity_bonus + height_penalty\\n    costs = ctrl_cost + contact_cost\\n    \\n    # Total reward\\n    reward = rewards - costs\\n\\n    # Reward info dictionary\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_contact\": -contact_cost,\\n        \"reward_survive\": healthy_reward,\\n        \"reward_velocity_bonus\": velocity_bonus,\\n        \"reward_height\": height_penalty,\\n    }\\n\\n    return reward, reward_info\\n``` \\n\\nIn this reward function:\\n1. A `velocity_bonus` is added to reinforce accelerative behavior.\\n2. A `height_penalty` encourages the quadruped to maintain a height around a certain z-coordinate, promoting stability.\\n3. The `healthy_reward` is also retained to ensure the agent remains in a healthy state. \\n4. The history of the last x-velocity is maintained for the acceleration calculation, which should be initialized properly in your environment\\'s constructor.\\n\\nThis design encourages both forward momentum and the stability needed for continued success in the task.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746941975, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=482, prompt_tokens=2182, total_tokens=2664, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPTAntEnv' object has no attribute 'last_x_velocity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m GPTAntEnv\u001b[38;5;241m.\u001b[39m_get_rew \u001b[38;5;241m=\u001b[39m _get_rew\n\u001b[1;32m     91\u001b[0m model_path \u001b[38;5;241m=\u001b[39m Train(morphology_index, rewardfunc_index, folder_name, stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine\u001b[39m\u001b[38;5;124m'\u001b[39m, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e5\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m improved_fitness, _ \u001b[38;5;241m=\u001b[39m \u001b[43mEva\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m improved_material \u001b[38;5;241m=\u001b[39m compute_ant_volume(best_parameter)\n\u001b[1;32m     94\u001b[0m improved_efficiency \u001b[38;5;241m=\u001b[39m improved_fitness \u001b[38;5;241m/\u001b[39m improved_material\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/utils.py:169\u001b[0m, in \u001b[0;36mEva\u001b[0;34m(model_path, run_steps, folder_name, video, rewardfunc_index, morphology_index)\u001b[0m\n\u001b[1;32m    167\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[1;32m    168\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)  \u001b[38;5;66;03m# 记录动作\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# print(f\"Step {step}: done={done}, truncated={truncated}, reward={reward}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/utils.py:503\u001b[0m, in \u001b[0;36mFitnessWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 503\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# 计算位移\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     x_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 访问原始环境的 qpos\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/gymnasium/wrappers/common.py:283\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/gymnasium/utils/passive_env_checker.py:207\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    209\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    210\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/gymnasium/envs/robodesign/GPTAnt.py:359\u001b[0m, in \u001b[0;36mGPTAntEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    356\u001b[0m x_velocity, y_velocity \u001b[38;5;241m=\u001b[39m xy_velocity\n\u001b[1;32m    358\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[0;32m--> 359\u001b[0m reward, reward_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_rew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_velocity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m terminated \u001b[38;5;241m=\u001b[39m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_healthy) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminate_when_unhealthy\n\u001b[1;32m    361\u001b[0m info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreward_info,\n\u001b[1;32m    368\u001b[0m }\n",
      "File \u001b[0;32m~/autodl-tmp/Ant-powered/GPTrewardfunc.py:10\u001b[0m, in \u001b[0;36m_get_rew\u001b[0;34m(self, x_velocity, action)\u001b[0m\n\u001b[1;32m      7\u001b[0m healthy_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhealthy_reward\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Introducing a velocity-based bonus for acceleration\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m velocity_bonus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, x_velocity \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_x_velocity\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# Change in velocity\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_x_velocity \u001b[38;5;241m=\u001b[39m x_velocity  \u001b[38;5;66;03m# Update for next step\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Control cost based on action taken\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPTAntEnv' object has no attribute 'last_x_velocity'"
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "coarse_best = [(5,25)]\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_ant_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list[morphology_index],  # 这本身已经是list结构，可以保留\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],  # 👈 用 [] 包装成列表\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "\n",
    "\n",
    "        shutil.copy(improved_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        \n",
    "        model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "        improved_fitness, _ = Eva(model_path)\n",
    "        improved_material = compute_ant_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "        iteration +=1\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            \n",
    "            \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            [rewardfunc_list[rewardfunc_index]],\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "\n",
    "        shutil.copy(best_morphology, \"GPTAnt.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTAntEnv._get_rew = _get_rew\n",
    "        \n",
    "        model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "        improved_fitness, _ = Eva(model_path)\n",
    "        improved_material = compute_ant_volume(best_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        iteration +=1\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_24.xml',\n",
       "  'best_parameter': [0.08,\n",
       "   0.32,\n",
       "   0.27,\n",
       "   0.16,\n",
       "   0.04,\n",
       "   0.14,\n",
       "   0.045,\n",
       "   0.02,\n",
       "   0.015,\n",
       "   0.015],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_4_24_0.py',\n",
       "  'best_fitness': 56.989413719003714,\n",
       "  'best_material': 0.0053784556056496475,\n",
       "  'best_efficiency': 10595.869501858635,\n",
       "  'best_iteration': 1},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_5.xml',\n",
       "  'best_parameter': [0.1, 0.3, 0.2, 0.2, 0.3, 0.15, 0.1, 0.02, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_3.py',\n",
       "  'best_fitness': 44.20795196061497,\n",
       "  'best_material': 0.009121783424101478,\n",
       "  'best_efficiency': 4846.415432733164,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_17.xml',\n",
       "  'best_parameter': [0.05, 0.4, 0.2, 0.25, 0.2, 0.15, 0.15, 0.01, 0.01, 0.01],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_3_17_0.py',\n",
       "  'best_fitness': 7.255647833180635,\n",
       "  'best_material': 0.0018047425741153315,\n",
       "  'best_efficiency': 4020.322863352015,\n",
       "  'best_iteration': 1},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_refine_1_17_1.xml',\n",
       "  'best_parameter': [0.05, 0.5, 0.3, 0.25, 0.3, 0.2, 0.2, 0.01, 0.01, 0.01],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_1_17_2.py',\n",
       "  'best_fitness': 24.1200473778651,\n",
       "  'best_material': 0.002152766378650272,\n",
       "  'best_efficiency': 11204.21036721492,\n",
       "  'best_iteration': 3},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_24.xml',\n",
       "  'best_parameter': [0.08,\n",
       "   0.32,\n",
       "   0.27,\n",
       "   0.16,\n",
       "   0.04,\n",
       "   0.14,\n",
       "   0.045,\n",
       "   0.02,\n",
       "   0.015,\n",
       "   0.015],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_1.py',\n",
       "  'best_fitness': 16.902216860080227,\n",
       "  'best_material': 0.0053784556056496475,\n",
       "  'best_efficiency': 3142.5781115169507,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_5.xml',\n",
       "  'best_parameter': [0.1, 0.3, 0.2, 0.2, 0.3, 0.15, 0.1, 0.02, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_4.py',\n",
       "  'best_fitness': 27.21882084790471,\n",
       "  'best_material': 0.009121783424101478,\n",
       "  'best_efficiency': 2983.9363184163567,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_5.xml',\n",
       "  'best_parameter': [0.1, 0.3, 0.2, 0.2, 0.3, 0.15, 0.1, 0.02, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_2.py',\n",
       "  'best_fitness': 26.53699538744324,\n",
       "  'best_material': 0.009121783424101478,\n",
       "  'best_efficiency': 2909.1893716011145,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_17.xml',\n",
       "  'best_parameter': [0.05, 0.4, 0.2, 0.25, 0.2, 0.15, 0.15, 0.01, 0.01, 0.01],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_0.py',\n",
       "  'best_fitness': 5.012471511630434,\n",
       "  'best_material': 0.0018047425741153315,\n",
       "  'best_efficiency': 2777.3886334384847,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_24.xml',\n",
       "  'best_parameter': [0.08,\n",
       "   0.32,\n",
       "   0.27,\n",
       "   0.16,\n",
       "   0.04,\n",
       "   0.14,\n",
       "   0.045,\n",
       "   0.02,\n",
       "   0.015,\n",
       "   0.015],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_3_24_0.py',\n",
       "  'best_fitness': 30.19486025456924,\n",
       "  'best_material': 0.0053784556056496475,\n",
       "  'best_efficiency': 5614.039134738213,\n",
       "  'best_iteration': 1},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_17.xml',\n",
       "  'best_parameter': [0.05, 0.4, 0.2, 0.25, 0.2, 0.15, 0.15, 0.01, 0.01, 0.01],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_2_17_1.py',\n",
       "  'best_fitness': 10.48515551225945,\n",
       "  'best_material': 0.0018047425741153315,\n",
       "  'best_efficiency': 5809.779002636527,\n",
       "  'best_iteration': 2},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_17.xml',\n",
       "  'best_parameter': [0.05, 0.4, 0.2, 0.25, 0.2, 0.15, 0.15, 0.01, 0.01, 0.01],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_4_17_1.py',\n",
       "  'best_fitness': 14.14845182465572,\n",
       "  'best_material': 0.0018047425741153315,\n",
       "  'best_efficiency': 7839.59553433329,\n",
       "  'best_iteration': 2},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_5.xml',\n",
       "  'best_parameter': [0.1, 0.3, 0.2, 0.2, 0.3, 0.15, 0.1, 0.02, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_1.py',\n",
       "  'best_fitness': 21.237709262482284,\n",
       "  'best_material': 0.009121783424101478,\n",
       "  'best_efficiency': 2328.240901485146,\n",
       "  'best_iteration': 0},\n",
       " {'best_morphology': 'results/Div_m25_r5/assets/GPTAnt_5.xml',\n",
       "  'best_parameter': [0.1, 0.3, 0.2, 0.2, 0.3, 0.15, 0.1, 0.02, 0.02, 0.02],\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_0_5_1.py',\n",
       "  'best_fitness': 36.31838114316985,\n",
       "  'best_material': 0.009121783424101478,\n",
       "  'best_efficiency': 3981.5000482482205,\n",
       "  'best_iteration': 2}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26274.418612610392"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " {'best_morphology': 'results/noDiv_m25_r5/assets/GPTAnt_refine_4_2_2.xml',\n",
    "  'best_parameter': [0.1,\n",
    "   0.25,\n",
    "   0.17,\n",
    "   0.25,\n",
    "   0.15,\n",
    "   0.12,\n",
    "   0.18,\n",
    "   0.03,\n",
    "   0.03,\n",
    "   0.03],\n",
    "  'best_rewardfunc': 'results/noDiv_m25_r5/env/GPTrewardfunc_4.py',\n",
    "  'best_fitness': 67.6709174730936,\n",
    "  'best_material': 0.014709160909209498,\n",
    "  'best_efficiency': 4600.5967227351775,\n",
    "  'best_iteration': 3},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
