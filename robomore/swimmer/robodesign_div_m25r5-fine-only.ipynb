{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTSwimmer import GPTSwimmerEnv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        api_key = \"<api_key>\"\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTSwimmer_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "        messages.append({\"role\": \"assistant\", \"content\": initial_code})\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "            # print(diverse_messages)\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            messages.append({\"role\": \"assistant\", \"content\": diverse_code})\n",
    "\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums                                                                                                                                                                                                                                                                                   \n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_swimmer_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = swimmer_design(parameter)  \n",
    "            filename = f\"GPTSwimmer_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_swimmer_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(initial_parameter)})\n",
    "\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "\n",
    "        xml_file = swimmer_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTSwimmer_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_swimmer_volume(diverse_parameter['parameters'])) \n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistant\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = swimmer_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTSwimmer_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\":reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine4_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, rewardfunc_index, morphology_index, iteration):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\" This is best parameter, please carefully review it, you can reduce the geom size, best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\" \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        # print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = swimmer_design(parameter)  \n",
    "        filename = f\"GPTSwimmer_refine4_{rewardfunc_index}_{morphology_index}_{iteration}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"fineonly_parameter.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 51\n",
    "rewardfunc_nums = 11\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3, 1.2, 0.6, 0.025, 0.015, 0.025]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphology_list = [f'results/Div_m25_r5/assets/GPTSwimmer_{i}.xml' for i in range(0,51) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,11)]\n",
    "\n",
    "parameter_list = [[0.6, 0.8, 0.5, 0.05, 0.05, 0.05],\n",
    " [1.0, 0.5, 0.3, 0.04, 0.04, 0.04],\n",
    " [0.35, 0.35, 0.35, 0.03, 0.03, 0.03],\n",
    " [0.8, 0.4, 0.2, 0.02, 0.03, 0.04],\n",
    " [0.2, 1.0, 0.7, 0.03, 0.02, 0.02],\n",
    " [0.5, 0.5, 0.5, 0.06, 0.06, 0.06],\n",
    " [1.2, 0.3, 0.3, 0.1, 0.02, 0.02],\n",
    " [0.9, 0.6, 0.9, 0.07, 0.05, 0.07],\n",
    " [0.4, 0.75, 1.2, 0.025, 0.025, 0.025],\n",
    " [0.45, 0.45, 1.5, 0.05, 0.05, 0.03],\n",
    " [0.2, 1.0, 0.6, 0.03, 0.06, 0.04],\n",
    " [0.1, 0.8, 1.5, 0.025, 0.025, 0.025],\n",
    " [0.15, 1.3, 0.4, 0.07, 0.03, 0.05],\n",
    " [0.7, 0.1, 1.3, 0.02, 0.07, 0.02],\n",
    " [0.2, 1.2, 0.3, 0.06, 0.02, 0.04],\n",
    " [0.5, 1.0, 0.2, 0.08, 0.03, 0.05],\n",
    " [0.3, 0.3, 0.3, 0.1, 0.1, 0.1],\n",
    " [0.4, 0.8, 1.2, 0.025, 0.05, 0.075],\n",
    " [0.7, 0.2, 1.1, 0.02, 0.07, 0.02],\n",
    " [0.8, 0.3, 0.5, 0.02, 0.05, 0.03],\n",
    " [1.1, 0.4, 0.6, 0.03, 0.03, 0.03],\n",
    " [0.9, 0.45, 0.9, 0.04, 0.02, 0.04],\n",
    " [0.3, 0.7, 1.0, 0.06, 0.04, 0.02],\n",
    " [0.2, 0.9, 0.5, 0.1, 0.03, 0.05],\n",
    " [1.5, 0.3, 0.3, 0.03, 0.05, 0.05],\n",
    " [0.1, 1.2, 0.6, 0.02, 0.04, 0.04],\n",
    " [0.25, 1.5, 0.25, 0.03, 0.01, 0.03],\n",
    " [0.3, 1.2, 0.6, 0.025, 0.015, 0.025],\n",
    " [0.5, 0.2, 0.8, 0.04, 0.06, 0.02],\n",
    " [0.2, 1.5, 0.2, 0.015, 0.04, 0.015],\n",
    " [0.2, 1.0, 0.2, 0.01, 0.08, 0.01],\n",
    " [0.8, 0.1, 0.8, 0.05, 0.02, 0.05],\n",
    "[0.2, 1.5, 0.5, 0.03, 0.01, 0.03],\n",
    "[0.25, 1.5, 0.25, 0.03, 0.01, 0.03],\n",
    " [0.9, 0.9, 0.1, 0.02, 0.02, 0.08],\n",
    " [1.5, 0.3, 0.3, 0.03, 0.05, 0.05],\n",
    " [0.1, 1.0, 0.1, 0.01, 0.06, 0.01],\n",
    " [0.2, 1.2, 0.2, 0.04, 0.01, 0.04],\n",
    " [0.4, 0.2, 0.8, 0.08, 0.01, 0.05],\n",
    " [0.4, 1.0, 0.4, 0.02, 0.04, 0.02],\n",
    " [0.1, 0.8, 0.8, 0.05, 0.02, 0.02],\n",
    " [0.8, 0.3, 1.4, 0.03, 0.06, 0.02],\n",
    " [0.4, 0.5, 0.5, 0.025, 0.025, 0.025],\n",
    " [0.6, 1.0, 0.3, 0.015, 0.03, 0.015],\n",
    " [0.2, 0.5, 1.0, 0.04, 0.03, 0.02],\n",
    " [0.15, 1.2, 0.8, 0.02, 0.04, 0.03],\n",
    " [1.2, 0.3, 0.5, 0.02, 0.02, 0.02],\n",
    " [0.5, 0.3, 0.2, 0.055, 0.08, 0.055],\n",
    " [0.1, 0.4, 0.9, 0.02, 0.03, 0.04],\n",
    " [0.05, 1.0, 0.05, 0.05, 0.01, 0.05],\n",
    " [1.0, 1.0, 1.0, 0.1, 0.1, 0.1]]\n",
    "\n",
    "material_list = [compute_swimmer_volume(parameter) for parameter in parameter_list]\n",
    "parameter_list[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'morphology_nums:{morphology_nums}')\n",
    "logging.info(f'rewardfunc_nums:{rewardfunc_nums}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'morphology_list:{morphology_list}')\n",
    "logging.info(f'material_list:{material_list}')\n",
    "logging.info(f'_________________________________enter coarse optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 results/Div_m25_r5/env/GPTrewardfunc_10.py\n",
      "50 results/Div_m25_r5/assets/GPTSwimmer_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i, rewardfunc in enumerate(rewardfunc_list):\n",
    "    for j, morphology in enumerate(morphology_list):\n",
    "        # if i not in [10]:\n",
    "        #     continue\n",
    "        if j not in [50] or i not in [10]:\n",
    "            continue\n",
    "        \n",
    "        print(i, rewardfunc)\n",
    "        print(j, morphology)\n",
    "        shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "        shutil.copy(rewardfunc, \"GPTrewardfunc.py\")         \n",
    "\n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "        model_path = Train(j,  i, folder_name, total_timesteps=5e5)\n",
    "        # model_path = f\"results/Div_m50_r10/coarse/SAC_morphology{j}_rewardfunc{i}_500000.0steps\"\n",
    "        fitness, reward = Eva(model_path)\n",
    "        material = material_list[j]\n",
    "        efficiency = fitness/material\n",
    "        fitness_matrix[i][j] = fitness\n",
    "        efficiency_matrix[i][j] = efficiency\n",
    "        \n",
    "        logging.info(\"___________________finish coarse optimization_____________________\")\n",
    "        logging.info(f\"morphology: {j}, rewardfunc: {i}, material cost: {material} reward: {reward} fitness: {fitness} efficiency: {efficiency}\")\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness = fitness\n",
    "            best_morphology = morphology\n",
    "            best_efficiency = efficiency\n",
    "            best_rewardfunc = rewardfunc\n",
    "            best_material = material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix_select = efficiency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均值： 4607.744248970781\n",
      "标准差： 5127.2952687360375\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'_________________________________end coarse optimization stage_________________________________')\n",
    "logging.info(f\"Stage1: Final best morphology: {best_morphology}, Fitness: {best_fitness}, best_efficiency: {best_efficiency}, best reward function: {best_rewardfunc}, Material cost: {best_material}, Reward: {best_reward}\")\n",
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'fitness_matrix:{fitness_matrix}')\n",
    "logging.info(f'efficiency_matrix:{efficiency_matrix}')\n",
    "logging.info(f'_________________________________enter fine optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameter:[1.0, 1.0, 1.0, 0.1, 0.1, 0.1]\n",
      "[1.2, 1.2, 1.2, 0.08, 0.08, 0.08]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_1.xml\n",
      "improved parameter [1.2, 1.2, 1.2, 0.08, 0.08, 0.08]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW6bM6Y5nboJpOzE2v8x3SNQp9EAk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Based on the information you\\'ve provided, I will design a new reward function to encourage the swimmer to achieve higher velocities while balancing control costs. The goal is to reward fast forward motion while also carefully managing the torque applied, avoiding overly aggressive actions that may hinder performance.\\n\\nHere\\'s the proposed reward function:\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an increasing reward for higher velocities\\n    forward_reward = self._forward_reward_weight * (np.sqrt(x_velocity + 1e-5))  # To avoid sqrt(0)\\n    \\n    # Penalize control actions with a more aggressive cost for large torques\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Normalize the forward reward with respect to the control cost\\n    reward = forward_reward - (ctrl_cost * 2)  # increased penalty for control cost\\n    \\n    # Encourage straight and steady motion by factoring in stabilization\\n    stability_penalty = np.clip(np.abs(x_velocity), 0.0, 1.0)  # Assuming we want velocity stability\\n    \\n    # Final reward calculation\\n    reward -= stability_penalty  # Applying a penalty based on instability\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_stability\": -stability_penalty,\\n    }\\n\\n    return reward, reward_info\\n```\\n\\n### Explanation:\\n1. **Forward Reward**: The forward reward now utilizes a square root function to emphasize getting close to a target speed without overwhelming the reward. This ensures that the agent values gradual increases in speed rather than sudden bursts.\\n\\n2. **Control Cost**: The control cost is calculated normally, but I\\'m considering it more heavily in the final reward. An increased penalty for control costs discourages reckless movement and promotes smoother acceleration.\\n\\n3. **Stability Penalty**: Introducing stability as a component means that inconsistency in velocity is penalized. This will promote more stable and fluid movements which are essential for navigating through the pool effectively.\\n\\n4. **Reward Info**: The reward information returned includes the breakdown of each component, allowing for better tracking and adjustment of how individual components are influencing the overall reward.\\n\\nThis formulation encourages the swimmer to not only maximize speed but also to do so in a controlled manner, balancing between acceleration and stability.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746991572, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=485, prompt_tokens=1609, total_tokens=2094, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Swimmer/GPTrewardfunc.py:4: RuntimeWarning: invalid value encountered in sqrt\n",
      "  forward_reward = self._forward_reward_weight * (x_velocity ** 1.5)  # Higher exponent for stronger emphasis on speed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 6.116616262771498\n",
      "[1.5, 1.4, 1.3, 0.06, 0.06, 0.06]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_3.xml\n",
      "improved parameter [1.5, 1.4, 1.3, 0.06, 0.06, 0.06]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an increasing reward for higher velocities\\n    forward_reward = self._forward_reward_weight * (np.sqrt(x_velocity + 1e-5))  # To avoid sqrt(0)\\n    \\n    # Penalize control actions with a more aggressive cost for large torques\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Normalize the forward reward with respect to the control cost\\n    reward = forward_reward - (ctrl_cost * 2)  # increased penalty for control cost\\n    \\n    # Encourage straight and steady motion by factoring in stabilization\\n    stability_penalty = np.clip(np.abs(x_velocity), 0.0, 1.0)  # Assuming we want velocity stability\\n    \\n    # Final reward calculation\\n    reward -= stability_penalty  # Applying a penalty based on instability\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_stability\": -stability_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW6ljydL8YwzRLtFe1yA52rphh7Ih', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746992215, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=226, prompt_tokens=1765, total_tokens=1991, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 25.767077566369117\n",
      "[1.6, 1.5, 1.4, 0.05, 0.05, 0.05]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_5.xml\n",
      "improved parameter [1.6, 1.5, 1.4, 0.05, 0.05, 0.05]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW6vvnqf5EKGjZsWPjyAGHTvXRGUr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion using a combination of linear and exponential scaling\\n    forward_reward = self._forward_reward_weight * (x_velocity + np.log(1 + x_velocity))\\n    \\n    # Penalize for control effort to motivate smoother actions\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Introduce a weighted reward component to encourage maintaining a positive speed\\n    speed_penalty = -np.abs(x_velocity) * 0.1  # Smaller penalty for reducing speed motivation\\n\\n    # Apply a quadratic penalty for control actions to discourage excessive torque\\n    reward = forward_reward - (ctrl_cost ** 2) + speed_penalty\\n\\n    # Adding a consistency reward based on maintaining velocity close to previous frame\\n    velocity_consistency = -np.abs(x_velocity - self.data.qvel[0]) * 0.5\\n    reward += velocity_consistency  # Encourage stable movements\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_speed_penalty\": speed_penalty,\\n        \"reward_velocity_consistency\": velocity_consistency,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746992847, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=261, prompt_tokens=1754, total_tokens=2015, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 30.2168426284121\n",
      "[1.8, 1.7, 1.6, 0.04, 0.04, 0.04]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_7.xml\n",
      "improved parameter [1.8, 1.7, 1.6, 0.04, 0.04, 0.04]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW76ADYEbhhN4EG6jRHk2wjzwDh9w', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward for forward movement: encourage higher speeds but reduce the risk of infinite reward\\n    forward_reward = self._forward_reward_weight * (x_velocity ** 2)  # Quadratic reward for speed\\n    \\n    # Control cost: penalize large actions to discourage excessive torque applications\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Introduce an additional penalty for erratic movements\\n    jerk_penalty = np.sum(np.square(np.diff(self.data.qvel)))  # Penalty for acceleration changes\\n    jerk_penalty_weight = 0.1  # weight factor for the jerk penalty\\n\\n    # Total reward calculation\\n    reward = forward_reward - ctrl_cost - (jerk_penalty_weight * jerk_penalty)\\n\\n    # Build reward info for tracking\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_jerk\": -jerk_penalty_weight * jerk_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746993482, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_ded0d14823', usage=CompletionUsage(completion_tokens=224, prompt_tokens=1754, total_tokens=1978, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 35.19570492362039\n",
      "[1.8, 1.5, 1.4, 0.03, 0.03, 0.03]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_9.xml\n",
      "improved parameter [1.8, 1.5, 1.4, 0.03, 0.03, 0.03]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW7GOqHdFTsow4AnGDQyslCwyUXzN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with a linear reward for velocity\\n    forward_reward = self._forward_reward_weight * (x_velocity ** 2)  # Using squared velocity for stronger incentive at higher speeds\\n    \\n    # Apply control cost to discourage excessive torque usage\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a dampened penalty on control cost to balance between torque use and speed\\n    dampened_ctrl_cost = np.sqrt(ctrl_cost)  # Apply square root for a less aggressive penalty\\n     \\n    # Penalize for excessive turns (angling movements) to promote smoother motion\\n    angle_penalty = np.sum(np.abs(action)) * 0.01  # Weight to control impact\\n\\n    # Combine rewards and penalties\\n    reward = forward_reward - dampened_ctrl_cost - angle_penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -dampened_ctrl_cost,\\n        \"reward_angle_penalty\": -angle_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746994116, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=239, prompt_tokens=1754, total_tokens=1993, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 34.13681097497555\n",
      "[1.8, 1.6, 1.5, 0.025, 0.025, 0.025]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_11.xml\n",
      "improved parameter [1.8, 1.6, 1.5, 0.025, 0.025, 0.025]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW7QcuO61ucH9WerFw4bmsc1MK5F1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with a logarithmic scale for smoother scaling\\n    forward_reward = self._forward_reward_weight * np.log1p(x_velocity)  # Using log1p to avoid log(0)\\n\\n    # Control cost as per existing method\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a potential reward for maintaining a target speed with a soft constraint\\n    target_speed = 1.0  # desired target speed\\n    speed_reward = -np.abs(x_velocity - target_speed) * 2.0  # Penalty for deviation from target speed\\n    \\n    # Penalize high control efforts to avoid excessive torque\\n    control_penalty = ctrl_cost * 2.0\\n\\n    # Combine all components to get the total reward\\n    reward = forward_reward + speed_reward - control_penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_speed\": speed_reward,\\n        \"reward_ctrl\": -control_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746994750, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=237, prompt_tokens=1754, total_tokens=1991, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1536)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 58.11676292399898\n",
      "[1.9, 1.7, 1.6, 0.02, 0.02, 0.02]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_13.xml\n",
      "improved parameter [1.9, 1.7, 1.6, 0.02, 0.02, 0.02]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW7arlf62n71jK3ece0CNMCkTXeMf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with a linear scale to promote speed\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    \\n    # Introduce a penalty for excessive control to minimize unnecessary torque\\n    ctrl_cost = self.control_cost(action)\\n\\n    # Implement a penalty for inefficient movements: measure energy wasted based on control cost\\n    energy_penalty = np.sum(np.square(action)) * self._ctrl_cost_weight\\n\\n    # Combine rewards and penalties to create the total reward\\n    reward = forward_reward - ctrl_cost - energy_penalty\\n\\n    # Encourage reaching a target velocity while punishing immediate drastic changes in velocity\\n    target_velocity = 2.0  # Define a target velocity\\n    target_penalty = np.maximum(0, target_velocity - x_velocity)\\n\\n    # Final reward incorporates penalties for deviation from target velocity\\n    reward -= target_penalty * 0.5  # Adjust the impact of this penalty\\n\\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_energy\": -energy_penalty,\\n        \"reward_target_velocity\": -target_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746995385, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=267, prompt_tokens=1754, total_tokens=2021, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1536)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 67.29341401188037\n",
      "[2.1, 1.9, 1.8, 0.015, 0.015, 0.015]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_15.xml\n",
      "improved parameter [2.1, 1.9, 1.8, 0.015, 0.015, 0.015]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW7l6tCS7MdWxuQqjVh54fwe9K2cP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion by combining linear and exponential rewards\\n    forward_reward = self._forward_reward_weight * (x_velocity + np.log1p(x_velocity))  # Adding log(1+x) to enhance reward scaling for positive velocities\\n\\n    # Introduce a control cost that includes both linear and quadratic penalties\\n    ctrl_cost = self.control_cost(action)\\n    ctrl_weighted_penalty = (ctrl_cost ** 2 + ctrl_cost) * 0.5  # More aggressive control penalty\\n\\n    # Penalize for excessive acceleration to promote smoother motion\\n    acceleration_penalty = np.abs(x_velocity - self.data.qvel[0]) * 0.1  # Force to keep changes in speed gradual\\n    \\n    # Combine all components for the total reward\\n    reward = forward_reward - ctrl_weighted_penalty - acceleration_penalty\\n\\n    # Return the reward and components for analysis\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_weighted_penalty,\\n        \"reward_acceleration\": -acceleration_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746996020, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=247, prompt_tokens=1754, total_tokens=2001, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1536)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 80.43730003211553\n",
      "[2.5, 2.2, 2.0, 0.012, 0.012, 0.012]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_17.xml\n",
      "improved parameter [2.5, 2.2, 2.0, 0.012, 0.012, 0.012]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with an exponential scale for higher velocities\\n    forward_reward = self._forward_reward_weight * np.exp(x_velocity) - 1  # Avoid zero rewards from exp(0)\\n\\n    # Aggressively penalize large control actions to reduce torque\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a quadratic penalty for control actions to discourage jerky movements\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Encourage consistency in speed by applying a penalty for sudden changes in velocity\\n    velocity_change_penalty = np.abs(x_velocity - self.data.qvel[0])  # Change in velocity\\n    reward -= velocity_change_penalty * 0.5  # Weight to adjust impact of penalty\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_velocity_change\": -velocity_change_penalty,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW7vMwfFNRUXes9CHUnKlQZEFoE3O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Based on the analysis of the provided reward functions and their fitness scores, I suggest a reward function that focuses on encouraging smooth and consistent forward motion while minimizing control costs. This new reward function will introduce a mechanism to reward acceleration towards the target and penalize both large control changes and erratic movement patterns. Here\\'s the proposed reward function code:\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with a linear scaling of x_velocity\\n    forward_reward = self._forward_reward_weight * x_velocity ** 2  # Squared to emphasize faster speeds\\n    \\n    # Penalize control costs to encourage efficient movements\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a squared term for control cost to discourage high torque actions\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Penalize sudden changes in velocity to promote smoother motion\\n    smoothness_penalty = np.abs(x_velocity - self.data.qvel[0])  # Difference from previous velocity\\n    reward -= smoothness_penalty * 0.3  # Adjustment weight for smoothness penalty\\n    \\n    # Reward for maintaining a velocity above a threshold to encourage constant motion\\n    active_motion_bonus = 1.0 if x_velocity > 0.5 else 0.0  # Bonus for staying above a speed threshold\\n    reward += active_motion_bonus\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_smoothness\": -smoothness_penalty,\\n        \"reward_active_motion\": active_motion_bonus,\\n    }\\n\\n    return reward, reward_info\\n``` \\n\\nThis reward function aims to differentiate itself from previous attempts by:\\n1. Squaring the forward motion component to heavily reward higher velocities.\\n2. Using a quadratic penalty for control costs to heavily penalize inefficient movements.\\n3. Introducing a penalty for sudden changes in velocity, which can lead to jerky motions.\\n4. Adding an incentive to maintain movement above a certain speed threshold, encouraging the swimmer to keep making progress.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746996656, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=433, prompt_tokens=1754, total_tokens=2187, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 86.95739788282096\n",
      "[2.6, 2.4, 2.2, 0.01, 0.01, 0.01]\n",
      "Successfully saved GPTSwimmer_refine4_10_50_19.xml\n",
      "improved parameter [2.6, 2.4, 2.2, 0.01, 0.01, 0.01]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\n\\nTask Description: The swimmers consist of three or more segments (\\'***links***\\') and one less articulation joints (\\'***rotors***\\') - one rotor joint connects exactly two links to form a linear chain.\\nThe swimmer is suspended in a two-dimensional pool and always starts in the same position (subject to some deviation drawn from a uniform distribution),\\nand the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\n\\nHere is the environment codes:\\nclass SwimmerEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"swimmer.xml\",\\n        frame_skip: int = 4,\\n        default_camera_config: Dict[str, Union[float, int]] = {},\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 1e-4,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - 2 * exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 2 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 2 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        xy_position_before = self.data.qpos[0:2].copy()\\n        self.do_simulation(action, self.frame_skip)\\n        xy_position_after = self.data.qpos[0:2].copy()\\n\\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\\n        x_velocity, y_velocity = xy_velocity\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\\n            \"x_position\": xy_position_after[0],\\n            \"y_position\": xy_position_after[1],\\n            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\\n            \"x_velocity\": x_velocity,\\n            \"y_velocity\": y_velocity,\\n            **reward_info,\\n        }\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[2:]\\n\\n        observation = np.concatenate([position, velocity]).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = self.init_qvel + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nv\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n            \"y_position\": self.data.qpos[1],\\n            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\\n        }\\n\\nThere are also some reward functions and their fitness. \\n\\n1.You should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.\\n2.Please carefully observe these reward funcions and their fitness, try to write a reward function which is different with these reward functions to further improve the fitness.\\n\\nreward function:\\nimport numpy as np \\n\\ndef _get_rew(self, x_velocity: float, action):\\n    forward_reward = self._forward_reward_weight * x_velocity\\n    ctrl_cost = self.control_cost(action)\\n\\n    reward = forward_reward - ctrl_cost\\n\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n    }\\n\\n    return reward, reward_info\\nfitness: 6.824475402416735\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Encourage forward motion with a linear scaling of x_velocity\\n    forward_reward = self._forward_reward_weight * x_velocity ** 2  # Squared to emphasize faster speeds\\n    \\n    # Penalize control costs to encourage efficient movements\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Introduce a squared term for control cost to discourage high torque actions\\n    reward = forward_reward - (ctrl_cost ** 2)\\n    \\n    # Penalize sudden changes in velocity to promote smoother motion\\n    smoothness_penalty = np.abs(x_velocity - self.data.qvel[0])  # Difference from previous velocity\\n    reward -= smoothness_penalty * 0.3  # Adjustment weight for smoothness penalty\\n    \\n    # Reward for maintaining a velocity above a threshold to encourage constant motion\\n    active_motion_bonus = 1.0 if x_velocity > 0.5 else 0.0  # Bonus for staying above a speed threshold\\n    reward += active_motion_bonus\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_smoothness\": -smoothness_penalty,\\n        \"reward_active_motion\": active_motion_bonus,\\n    }\\n\\n    return reward, reward_info\\n\\nbest fitness: 6.824475402416735\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BW85ZGLbyxzSS9AWy3uQefRxG6LWx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Encouraging faster forward motion by using the cube of x_velocity\\n    forward_reward = self._forward_reward_weight * (x_velocity ** 3)  # Cubed to emphasize very fast speeds\\n    \\n    # Penalizing control costs to encourage efficient movements\\n    ctrl_cost = self.control_cost(action)\\n    \\n    # Adjust control cost to accentuate the penalty for high torque actions\\n    reward = forward_reward - (ctrl_cost * 1.5)  # Increased penalty weight for control cost\\n    \\n    # Reward for maintaining a velocity above a higher threshold to prevent stagnation\\n    active_motion_bonus = 1.5 if x_velocity > 0.7 else 0.0  # Higher threshold encourages better speed\\n    reward += active_motion_bonus\\n    \\n    # Introduce a penalty for excessive actions to encourage more measured movements\\n    action_penalty = np.sum(np.abs(action))  # Total torque applied\\n    reward -= action_penalty * 0.1  # Penalty weight for action intensity\\n    \\n    # Collect information for reward components\\n    reward_info = {\\n        \"reward_forward\": forward_reward,\\n        \"reward_ctrl\": -ctrl_cost,\\n        \"reward_active_motion\": active_motion_bonus,\\n        \"reward_action_penalty\": -action_penalty,\\n    }\\n\\n    return reward, reward_info\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746997289, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=287, prompt_tokens=1810, total_tokens=2097, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-37859:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37858:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37861:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37872:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37868:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37869:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37857:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37871:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37866:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37863:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37865:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37870:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37860:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37864:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37867:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-37862:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 32, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda3/envs/robodesign/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m GPTSwimmerEnv\u001b[38;5;241m.\u001b[39m_get_rew \u001b[38;5;241m=\u001b[39m _get_rew\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmorphology_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewardfunc_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     improved_fitness, _ \u001b[38;5;241m=\u001b[39m Eva(model_path)\n\u001b[1;32m     97\u001b[0m     improved_material \u001b[38;5;241m=\u001b[39m compute_swimmer_volume(best_parameter)\n",
      "File \u001b[0;32m~/autodl-tmp/Swimmer/utils.py:109\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(morphology, rewardfunc, folder_name, total_timesteps, stage, callback, iter)\u001b[0m\n\u001b[1;32m     91\u001b[0m model \u001b[38;5;241m=\u001b[39m SAC(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m     envs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39mtensorboard_name\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 训练 100 万步\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     callback \u001b[38;5;241m=\u001b[39m DynamicRewardLoggingCallback()\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:557\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mreset_noise(env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# Select action randomly or according to policy\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m    560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:390\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._sample_action\u001b[0;34m(self, learning_starts, action_noise, n_envs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Note: when using continuous actions,\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;66;03m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself._last_obs was not set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 390\u001b[0m     unscaled_action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:353\u001b[0m, in \u001b[0;36mSACPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:170\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    168\u001b[0m mean_actions, log_std, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action_dist_params(obs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Note: the action is squashed\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_from_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:190\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.actions_from_params\u001b[0;34m(self, mean_actions, log_std, deterministic)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mactions_from_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean_actions: th\u001b[38;5;241m.\u001b[39mTensor, log_std: th\u001b[38;5;241m.\u001b[39mTensor, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Update the proba distribution\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:224\u001b[0m, in \u001b[0;36mSquashedDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mproba_distribution\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSquashedDiagGaussianDistribution, mean_actions: th\u001b[38;5;241m.\u001b[39mTensor, log_std: th\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    223\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSquashedDiagGaussianDistribution:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:164\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m action_std \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mones_like(mean_actions) \u001b[38;5;241m*\u001b[39m log_std\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/distributions/normal.py:57\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/robodesign/lib/python3.8/site-packages/torch/distributions/distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     67\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m     68\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "coarse_best = [(10,50)]\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_swimmer_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "    print(f\"Initial parameter:{parameter}\")\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        iteration +=1   \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list[morphology_index],  # 这本身已经是list结构，可以保留\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],  # 👈 用 [] 包装成列表\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        print(\"improved parameter\", improved_parameter)\n",
    "        shutil.copy(improved_morphology, \"GPTSwimmer.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTSwimmerEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            break\n",
    "            \n",
    "        improved_material = compute_swimmer_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            # break\n",
    "            \n",
    "        iteration +=1        \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            [rewardfunc_list[rewardfunc_index]],\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        shutil.copy(best_morphology, \"GPTSwimmer.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTSwimmerEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_swimmer_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            break\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "    logging.info(\"____________________________________________\")\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41062.27460151978"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/Div_m25_r5/env/GPTrewardfunc_refine4_10_50_18.py'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rewardfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6, 2.4, 2.2, 0.01, 0.01, 0.01]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/Div_m25_r5/assets/GPTSwimmer_refine4_10_50_19.xml'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-10 14:21:58,984 - Final optimized result: rewardfunc_index2 morphology_index4\n",
    "2025-04-10 14:21:58,984 -   Morphology: results/Div_m25_r5/assets/GPTSwimmer_refine_2_4_2.xml\n",
    "2025-04-10 14:21:58,984 -   Parameter: [0.25, 1.3, 0.65, 0.023, 0.018, 0.018]\n",
    "2025-04-10 14:21:58,984 -   Rewardfunc: results/Div_m25_r5/env/GPTSwimmer_refine_2_4_3.py\n",
    "2025-04-10 14:21:58,984 -   Fitness: 70.88658010548744\n",
    "2025-04-10 14:21:58,984 -   Material: 0.0025001569263455457\n",
    "2025-04-10 14:21:58,984 -   Efficiency: 28352.852318394926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "3e6 steps train\n",
      "\n",
      "best_fitness:93.59566031694601\n",
      "best_efficiency:41149.7568822981\n"
     ]
    }
   ],
   "source": [
    "best_morphology = 'results/Div_m25_r5/assets/GPTSwimmer_refine4_10_50_19.xml'\n",
    "best_rewardfunc = 'results/Div_m25_r5/env/GPTrewardfunc_refine4_10_50_18.py'\n",
    "morphology_index=9998\n",
    "rewardfunc_index=9998\n",
    "best_parameter = [2.6, 2.4, 2.2, 0.01, 0.01, 0.01]\n",
    "\n",
    "shutil.copy(best_morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "# autodl-tmp/Swimmer/results/Div_m25_r5/fine/SAC_morphology999_rewardfunc999_3000000.0steps\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "best_fitness, _ = Eva(model_path)\n",
    "best_material = compute_swimmer_volume(best_parameter)\n",
    "best_efficiency = best_fitness / best_material\n",
    "logging.info(\"3e6 steps train\\n\")\n",
    "logging.info(f\"best_fitness:{best_fitness}\")\n",
    "logging.info(f\"best_efficiency:{best_efficiency}\")\n",
    "print(\"3e6 steps train\\n\")\n",
    "print(f\"best_fitness:{best_fitness}\")\n",
    "print(f\"best_efficiency:{best_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2025-05-11 12:53:55,783 - Final optimized result: rewardfunc_index1 morphology_index8\n",
    "2025-05-11 12:53:55,783 -   Morphology: results/Div_m25_r5/assets/GPTSwimmer_refine2_1_8_0.xml\n",
    "2025-05-11 12:53:55,783 -   Parameter: [0.4, 1.1, 0.5, 0.02, 0.02, 0.02]\n",
    "2025-05-11 12:53:55,783 -   Rewardfunc: results/Div_m25_r5/env/GPTrewardfunc_refine_1_8_1.py\n",
    "2025-05-11 12:53:55,783 -   Fitness: 89.93837841576415\n",
    "2025-05-11 12:53:55,783 -   Material: 0.002613805087786708\n",
    "2025-05-11 12:53:55,783 -   Efficiency: 34408.98437148628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Run 11\n",
      "Run 12\n",
      "Run 13\n",
      "Run 14\n",
      "Run 15\n",
      "Run 16\n",
      "Run 17\n",
      "Run 18\n",
      "Run 19\n",
      "Run 20\n",
      "Run 21\n",
      "Run 22\n",
      "Run 23\n",
      "Run 24\n",
      "Run 25\n",
      "Run 26\n",
      "Run 27\n",
      "Run 28\n",
      "Run 29\n",
      "Run 30\n",
      "Run 31\n",
      "Run 32\n",
      "Run 33\n",
      "Run 34\n",
      "Run 35\n",
      "Run 36\n",
      "Run 37\n",
      "Run 38\n",
      "Run 39\n",
      "Run 40\n",
      "Run 41\n",
      "Run 42\n",
      "Run 43\n",
      "Run 44\n",
      "Run 45\n",
      "Run 46\n",
      "Run 47\n",
      "Run 48\n",
      "Run 49\n",
      "Run 50\n",
      "Run 51\n",
      "Run 52\n",
      "Run 53\n",
      "Run 54\n",
      "Run 55\n",
      "Run 56\n",
      "Run 57\n",
      "Run 58\n",
      "Run 59\n",
      "Run 60\n",
      "Run 61\n",
      "Run 62\n",
      "Run 63\n",
      "Run 64\n",
      "Run 65\n",
      "Run 66\n",
      "Run 67\n",
      "Run 68\n",
      "Run 69\n",
      "Run 70\n",
      "Run 71\n",
      "Run 72\n",
      "Run 73\n",
      "Run 74\n",
      "Run 75\n",
      "Run 76\n",
      "Run 77\n",
      "Run 78\n",
      "Run 79\n",
      "Run 80\n",
      "Run 81\n",
      "Run 82\n",
      "Run 83\n",
      "Run 84\n",
      "Run 85\n",
      "Run 86\n",
      "Run 87\n",
      "Run 88\n",
      "Run 89\n",
      "Run 90\n",
      "Run 91\n",
      "Run 92\n",
      "Run 93\n",
      "Run 94\n",
      "Run 95\n",
      "Run 96\n",
      "Run 97\n",
      "Run 98\n",
      "Run 99\n",
      "Saved qpos log to /root/autodl-tmp/Swimmer/qpos.txt\n",
      "Average Fitness: 109.3875, Average Reward: 5509.0910\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_fitness' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m fitness, _ \u001b[38;5;241m=\u001b[39m Eva_with_qpos_logging(model_path, run_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, rewardfunc_index\u001b[38;5;241m=\u001b[39mrewardfunc_index, morphology_index\u001b[38;5;241m=\u001b[39mmorphology_index)\n\u001b[1;32m     20\u001b[0m best_material \u001b[38;5;241m=\u001b[39m compute_swimmer_volume(best_parameter)\n\u001b[0;32m---> 21\u001b[0m best_efficiency \u001b[38;5;241m=\u001b[39m \u001b[43mbest_fitness\u001b[49m \u001b[38;5;241m/\u001b[39m best_material\n\u001b[1;32m     22\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3e6 steps train\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_fitness:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_fitness\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_fitness' is not defined"
     ]
    }
   ],
   "source": [
    "best_morphology = \"results/Div_m25_r5/assets/GPTSwimmer_refine2_1_8_0.xml\"\n",
    "best_rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_refine_1_8_1.py\"\n",
    "morphology_index=9999\n",
    "rewardfunc_index=9999\n",
    "best_parameter = [2, 2, 2, 0.01, 0.01, 0.01]\n",
    "\n",
    "shutil.copy(best_morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "# autodl-tmp/Swimmer/results/Div_m25_r5/fine/SAC_morphology999_rewardfunc999_3000000.0steps\n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "# best_fitness, _ = Eva(model_path)\n",
    "fitness, _ = Eva_with_qpos_logging(model_path, run_steps=100, video = True, rewardfunc_index=rewardfunc_index, morphology_index=morphology_index)\n",
    "\n",
    "best_material = compute_swimmer_volume(best_parameter)\n",
    "best_efficiency = best_fitness / best_material\n",
    "logging.info(\"3e6 steps train\\n\")\n",
    "logging.info(f\"best_fitness:{best_fitness}\")\n",
    "logging.info(f\"best_efficiency:{best_efficiency}\")\n",
    "print(\"3e6 steps train\\n\")\n",
    "print(f\"best_fitness:{best_fitness}\")\n",
    "print(f\"best_efficiency:{best_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "3e6 steps train\n",
      "\n",
      "best_fitness:73.16345150502791\n",
      "best_efficiency:29263.543713622083\n"
     ]
    }
   ],
   "source": [
    "best_morphology = \"results/Div_m25_r5/assets/GPTSwimmer_refine_2_4_2.xml\"\n",
    "best_rewardfunc = \"results/Div_m25_r5/env/GPTSwimmer_refine_2_4_3.py\"\n",
    "morphology_index=999\n",
    "rewardfunc_index=999\n",
    "best_parameter = [0.25, 1.3, 0.65, 0.023, 0.018, 0.018]\n",
    "\n",
    "shutil.copy(best_morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "# autodl-tmp/Swimmer/results/Div_m25_r5/fine/SAC_morphology999_rewardfunc999_3000000.0steps\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "best_fitness, _ = Eva(model_path)\n",
    "best_material = compute_swimmer_volume(best_parameter)\n",
    "best_efficiency = best_fitness / best_material\n",
    "logging.info(\"3e6 steps train\\n\")\n",
    "logging.info(f\"best_fitness:{best_fitness}\")\n",
    "logging.info(f\"best_efficiency:{best_efficiency}\")\n",
    "print(\"3e6 steps train\\n\")\n",
    "print(f\"best_fitness:{best_fitness}\")\n",
    "print(f\"best_efficiency:{best_efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-10 02:00:20,461 - Initial morphology:results/Div_m25_r5/assets/GPTSwimmer_4.xml\n",
    "2025-04-10 02:00:20,461 - Initial parameter:[0.2, 1.0, 0.7, 0.03, 0.02, 0.02]\n",
    "2025-04-10 02:00:20,461 - Initial rewardfunc:results/Div_m25_r5/env/GPTrewardfunc_0.py\n",
    "2025-04-10 02:00:20,461 - Initial fitness:63.203097217867956\n",
    "2025-04-10 02:00:20,461 - Initial efficiency:21931.145365424352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "3e6 steps train\n",
      "\n",
      "fitness:63.99026937044875\n",
      "efficiency:22204.290000193658\n"
     ]
    }
   ],
   "source": [
    "morphology = \"results/Div_m25_r5/assets/GPTSwimmer_4.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=777\n",
    "rewardfunc_index=777\n",
    "parameter = [0.2, 1.0, 0.7, 0.03, 0.02, 0.02]\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"coarse only best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "3e6 steps train\n",
      "\n",
      "human_fitness:2.0148512417686497\n",
      "human_efficiency:18.863149101313184\n"
     ]
    }
   ],
   "source": [
    "#  human\n",
    "morphology = \"results/Div_m25_r5/assets/GPTSwimmer_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "parameter =  [1.0, 1.0, 1.0, 0.1, 0.1, 0.1]\n",
    "\n",
    "\n",
    "morphology_index=888\n",
    "rewardfunc_index=888\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"human 3e6 steps train\\n\")\n",
    "logging.info(f\"human_fitness:{fitness}\")\n",
    "logging.info(f\"human_efficiency:{efficiency}\")\n",
    "print(\"3e6 steps train\\n\")\n",
    "print(f\"human_fitness:{fitness}\")\n",
    "print(f\"human_efficiency:{efficiency}\")\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 11:05:41,435 - morphology: 50, rewardfunc: 0, material cost: 0.10681415022205296 reward: 41.660022777401224 fitness: 1.6689821209181794 efficiency: 15.625103204477863\n",
    "[1.0, 1.0, 1.0, 0.1, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Morphology Design) 3e6 steps train\n",
      "\n",
      "fitness:2.3706576073842083\n",
      "efficiency:22.194228034917792\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Morphology Design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTSwimmer_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_0.py\"\n",
    "parameter =  [1.0, 1.0, 1.0, 0.1, 0.1, 0.1]\n",
    "\n",
    "\n",
    "morphology_index=555\n",
    "rewardfunc_index=555\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"human 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Morphology Design) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 09:14:35,499 - morphology: 4, rewardfunc: 10, material cost: 0.002881887660893037 reward: 1588.2197695801924 fitness: 63.53375844670141 efficiency: 22045.883088660583\n",
    "[0.2, 1.0, 0.7, 0.03, 0.02, 0.02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Robodesign (w/o Reward Shaping)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTSwimmer_4.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "parameter =  [0.2, 1.0, 0.7, 0.03, 0.02, 0.02]\n",
    "\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTSwimmer_refine_2_4_2.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "parameter = [0.25, 1.3, 0.65, 0.023, 0.018, 0.018]\n",
    "\n",
    "morphology_index=555\n",
    "rewardfunc_index=555\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign (w/o Reward Shaping) 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Reward Shaping) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-08 05:01:03,460 - Final optimized result: rewardfunc_index2 morphology_index18\n",
    "2025-04-08 05:01:03,460 -   Morphology: results/noDiv_m25_r5/assets/GPTSwimmer_refine_2_18_0.xml\n",
    "2025-04-08 05:01:03,460 -   Parameter: [0.45, 0.45, 0.45, 0.035, 0.035, 0.035]\n",
    "2025-04-08 05:01:03,460 -   Rewardfunc: results/noDiv_m25_r5/env/GPTSwimmer_refine_2_18_1.py\n",
    "2025-04-08 05:01:03,460 -   Fitness: 41.06518362642221\n",
    "2025-04-08 05:01:03,460 -   Material: 0.005734191990964771\n",
    "2025-04-08 05:01:03,460 -   Efficiency: 7161.459485683012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Diversity Reflection) 3e6 steps train\n",
      "\n",
      "fitness:61.67922159794303\n",
      "efficiency:10756.39282659693\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Diversity Reflection)\n",
    "\n",
    "morphology = \"results/noDiv_m25_r5/assets/GPTSwimmer_refine_2_18_0.xml\"\n",
    "rewardfunc = \"results/noDiv_m25_r5/env/GPTSwimmer_refine_2_18_1.py\"\n",
    "parameter =  [0.45, 0.45, 0.45, 0.035, 0.035, 0.035]\n",
    "\n",
    "morphology_index=333\n",
    "rewardfunc_index=333\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"Robodesign (w/o Diversity Reflection) 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Diversity Reflection) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-09 04:36:01,592 - iteration:2, morphology: 0, rewardfunc: 9, material cost: 0.10681415022205296 reward: 49.70920622052905 fitness: 1.992258670845264 efficiency: 18.651636198983123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "eureka reward 3e6 steps train\n",
      "\n",
      "fitness:2.5589952822877184\n",
      "efficiency:23.95745579558415\n"
     ]
    }
   ],
   "source": [
    "# eureka reward\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTSwimmer_50.xml\"\n",
    "rewardfunc = \"results/eureka1/env/GPTrewardfunc_9_2.py\"\n",
    "parameter =  [1.0, 1.0, 1.0, 0.1, 0.1, 0.1]\n",
    "\n",
    "morphology_index=222\n",
    "rewardfunc_index=222\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka reward 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka reward 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eureka morphology\n",
    "\n",
    "morphology = \"results/Eureka_morphology/assets/GPTSwimmer_14.xml\"\n",
    "rewardfunc = \"results/Eureka_morphology/env/GPTrewardfunc_0.py\"\n",
    "parameter =  [0.85, 0.65, 0.45, 0.035, 0.025, 0.015]\n",
    "\n",
    "morphology_index=222\n",
    "rewardfunc_index=222\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka reward 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka reward 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-06 09:59:14,743 - morphology: 2, rewardfunc: 0, material cost: 0.014476458947741768 reward: 1409.4540161000132 fitness: 56.384153229858015 efficiency: 3894.885719870989\n",
    "[0.9, 0.7, 0.5, 0.035, 0.025, 0.015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "eureka morphology 3e6 steps train\n",
      "\n",
      "fitness:89.09046298232943\n",
      "efficiency:16344.884802286162\n"
     ]
    }
   ],
   "source": [
    "# eureka morphology\n",
    "\n",
    "morphology = \"results/Eureka_morphology/assets/GPTSwimmer_2.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_10.py\"\n",
    "\n",
    "parameter =  [0.9, 0.7, 0.5, 0.035, 0.025, 0.015]\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "shutil.copy(morphology, \"GPTSwimmer.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTSwimmerEnv._get_rew = _get_rew\n",
    "\n",
    "\n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_swimmer_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka morphology 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka morphology 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
