{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTCheetah import GPTCheetahEnv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        api_key = \"<api_key>\"\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTCheetah_{i}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "        return files\n",
    "    \n",
    "    def generate_rewardfunc_div(self, rewardfunc_nums, folder_name):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        # 生成初始 Reward Function\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=1, timeout=10\n",
    "        )\n",
    "\n",
    "        rewardfunc_files = []\n",
    "\n",
    "        initial_code = self.extract_code(response.choices[0].message.content)\n",
    "        if initial_code:\n",
    "            reward_code = \"import numpy as np\\n\" + self.indent_code(initial_code) + \"\\n\"\n",
    "\n",
    "            file_path = os.path.join(folder_name, \"env\", \"GPTrewardfunc_0.py\")\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(reward_code)\n",
    "            rewardfunc_files.append(file_path)\n",
    "            print(f\"initial Saved: {file_path}\")\n",
    "        messages.append({\"role\": \"assistant\", \"content\": initial_code})\n",
    "\n",
    "        # 生成不同的多样化 Reward Functions\n",
    "        for i in range(1, rewardfunc_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": rewardfunc_div_prompts + zeroshot_rewardfunc_format}\n",
    "            ]\n",
    "            # print(diverse_messages)\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model, messages=diverse_messages, n=1\n",
    "            )\n",
    "            diverse_code = self.extract_code(response.choices[0].message.content)\n",
    "            messages.append({\"role\": \"assistant\", \"content\": diverse_code})\n",
    "\n",
    "            if diverse_code:\n",
    "                reward_code =  \"import numpy as np\\n\" + self.indent_code(diverse_code) + \"\\n\"\n",
    "                file_path = os.path.join(folder_name, \"env\", f\"GPTrewardfunc_{i}.py\")\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(reward_code)\n",
    "                rewardfunc_files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return rewardfunc_files\n",
    "\n",
    "\n",
    "    def generate_morphology(self, morphology_nums, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=morphology_nums                                                                                                                                                                                                                                                                                   \n",
    "        )\n",
    "\n",
    "        # 解析所有 response 里的参数\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            print(f\"Response {i}:\")\n",
    "            print(json.dumps(choice.message.content, indent=4))\n",
    "\n",
    "        parameter_list = [json.loads(choice.message.content).get('parameters', []) for choice in responses.choices]\n",
    "        material_list = [compute_cheetah_volume(parameter) for parameter in parameter_list]\n",
    "\n",
    "        xml_files = []\n",
    "        for i, parameter in enumerate(parameter_list):\n",
    "            if not isinstance(parameter, list):\n",
    "                print(f\"Skipping invalid parameter {i}: {parameter}\")\n",
    "                continue\n",
    "\n",
    "            xml_file = cheetah_design(parameter)  \n",
    "            filename = f\"GPTCheetah_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            xml_files.append(file_path)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            print(f\"Successfully saved {filename}\")\n",
    "            \n",
    "        return xml_files, material_list, parameter_list\n",
    "    \n",
    "    def generate_morphology_div(self, morphology_nums, folder_name):\n",
    "\n",
    "        material_list = []\n",
    "        xml_files = []\n",
    "        parameter_list = []\n",
    "        \n",
    "        # 生成初始 morphology\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "            n=1\n",
    "        )\n",
    "\n",
    "        initial_parameter = json.loads(response.choices[0].message.content)\n",
    "        parameter_list.append(initial_parameter['parameters'])\n",
    "        material_list.append(compute_cheetah_volume(initial_parameter['parameters']))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(initial_parameter)})\n",
    "        logging.info(f\"generate initial_parameter{initial_parameter['parameters']}\" )\n",
    "        xml_file = cheetah_design(initial_parameter['parameters'])  \n",
    "\n",
    "        filename = f\"GPTCheetah_0.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        xml_files.append(file_path)\n",
    "\n",
    "        # 生成不同的多样化设计\n",
    "        for i in range(1, morphology_nums):\n",
    "            diverse_messages = messages + [\n",
    "                {\"role\": \"user\", \"content\": morphology_div_prompts + morphology_format}\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=diverse_messages,\n",
    "                response_format={'type': 'json_object'},\n",
    "                n=1\n",
    "            )\n",
    "\n",
    "            diverse_parameter = json.loads(response.choices[0].message.content)\n",
    "            material_list.append(compute_cheetah_volume(diverse_parameter['parameters'])) \n",
    "            parameter_list.append(diverse_parameter['parameters'])\n",
    "            messages.append({\"role\": \"assistant\", \"content\": json.dumps(diverse_parameter)})\n",
    "            logging.info(f\"generate diverse_parameter{ diverse_parameter['parameters']}\")\n",
    "            xml_file = cheetah_design(diverse_parameter['parameters'])  \n",
    "            filename = f\"GPTCheetah_{i}.xml\"\n",
    "            file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(xml_file)\n",
    "            xml_files.append(file_path)\n",
    "\n",
    "        return xml_files, material_list, parameter_list\n",
    "\n",
    "\n",
    "    def improve_rewardfunc(self, best_rewardfunc, rewardfunc_list, fitness_list, folder_name, step, rewardfunc_index, morphology_index):\n",
    "        reward_improve_prompts = prompts.reward_improve_prompts\n",
    "\n",
    "        for rewardfunc_file, fitness in zip(rewardfunc_list, fitness_list):\n",
    "            with open(rewardfunc_file, \"r\") as fp:\n",
    "                reward_content = fp.read()\n",
    "            reward_improve_prompts += f\"\\nreward function:\\n{reward_content}\\nfitness: {fitness}\\n\"\n",
    "\n",
    "        with open(best_rewardfunc, \"r\") as fp:\n",
    "            best_reward_content = fp.read()\n",
    "        reward_improve_prompts += f\"\\nbest reward function:\\n{best_reward_content}\\nbest fitness: {max(fitness_list)}\\n\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\":reward_improve_prompts+ zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "        print(messages)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages\n",
    "        )\n",
    "\n",
    "        print(response)\n",
    "        reward_code = self.extract_code(response.choices[0].message.content)\n",
    "\n",
    "        if reward_code:\n",
    "            full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "            file_name =  f\"GPTrewardfunc_refine_{step}_{rewardfunc_index}_{morphology_index}.py\"\n",
    "            file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "            with open(file_path, \"w\") as fp:\n",
    "                fp.write(full_code)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    def improve_morphology(self, best_parameter, parameter_list, fitness_list, folder_name, rewardfunc_index, morphology_index, iteration):\n",
    "        morphology_improve_prompts = prompts.morphology_improve_prompts\n",
    "        for parameter_content, fitness in zip(parameter_list, fitness_list):\n",
    "            morphology_improve_prompts = morphology_improve_prompts + f\"parameter:{parameter_content} \\n\" + f\"fintess:{fitness}\"\n",
    "        morphology_improve_prompts = morphology_improve_prompts + f\"best parameter:{best_parameter} \\n\" + f\"best fintess:{max(fitness_list)}\" \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful mujoco robot designer\"},\n",
    "            {\"role\": \"user\", \"content\": morphology_improve_prompts + morphology_format}\n",
    "        ]\n",
    "        \n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={'type': 'json_object'},\n",
    "        )\n",
    "        # print(responses)\n",
    "        parameter = json.loads(responses.choices[0].message.content).get('parameters', []) \n",
    "        print(parameter)\n",
    "        xml_file = cheetah_design(parameter)  \n",
    "        filename = f\"GPTCheetah_refine2_{rewardfunc_index}_{morphology_index}_{iteration}.xml\"\n",
    "        file_path = os.path.join(folder_name, \"assets\", filename)\n",
    "\n",
    "        with open(file_path, \"w\") as fp:\n",
    "            fp.write(xml_file)\n",
    "\n",
    "        print(f\"Successfully saved {filename}\")\n",
    "        return file_path, parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/Div_m25_r5\"\n",
    "log_file = os.path.join(folder_name, \"parameters_fineonly.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "morphology_nums = 51\n",
    "rewardfunc_nums = 11\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n",
    "designer = DGA()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print configuration info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f\"start!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter coarse optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 24)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphology_list = [f'results/Div_m25_r5/assets/GPTCheetah_{i}.xml' for i in range(0,51) ]\n",
    "rewardfunc_list = [f'results/Div_m25_r5/env/GPTrewardfunc_{i}.py' for i in range(0,11)]\n",
    "\n",
    "parameter_list = np.array([[-0.4,\n",
    "  0.5,\n",
    "  0.8,\n",
    "  0.2,\n",
    "  0.3,\n",
    "  -0.3,\n",
    "  0.2,\n",
    "  -0.4,\n",
    "  0.15,\n",
    "  -0.5,\n",
    "  0.2,\n",
    "  -0.25,\n",
    "  0.15,\n",
    "  -0.35,\n",
    "  0.1,\n",
    "  -0.45,\n",
    "  0.08,\n",
    "  0.04,\n",
    "  0.05,\n",
    "  0.04,\n",
    "  0.03,\n",
    "  0.05,\n",
    "  0.04,\n",
    "  0.03],\n",
    " [-0.6,\n",
    "  0.6,\n",
    "  1.1,\n",
    "  0.25,\n",
    "  0.4,\n",
    "  -0.35,\n",
    "  0.25,\n",
    "  -0.45,\n",
    "  0.2,\n",
    "  -0.55,\n",
    "  0.3,\n",
    "  -0.3,\n",
    "  0.25,\n",
    "  -0.4,\n",
    "  0.15,\n",
    "  -0.5,\n",
    "  0.07,\n",
    "  0.03,\n",
    "  0.06,\n",
    "  0.05,\n",
    "  0.04,\n",
    "  0.06,\n",
    "  0.05,\n",
    "  0.04],\n",
    " [-0.9,\n",
    "  0.8,\n",
    "  1.4,\n",
    "  0.3,\n",
    "  0.6,\n",
    "  -0.6,\n",
    "  0.5,\n",
    "  -0.75,\n",
    "  0.35,\n",
    "  -0.95,\n",
    "  0.4,\n",
    "  -0.5,\n",
    "  0.4,\n",
    "  -0.7,\n",
    "  0.25,\n",
    "  -0.85,\n",
    "  0.09,\n",
    "  0.045,\n",
    "  0.065,\n",
    "  0.055,\n",
    "  0.045,\n",
    "  0.065,\n",
    "  0.055,\n",
    "  0.045],\n",
    " [-0.5,\n",
    "  0.7,\n",
    "  1.0,\n",
    "  0.35,\n",
    "  0.55,\n",
    "  -0.6,\n",
    "  0.4,\n",
    "  -0.8,\n",
    "  0.2,\n",
    "  -1.0,\n",
    "  -0.45,\n",
    "  -0.6,\n",
    "  0.3,\n",
    "  -0.9,\n",
    "  0.15,\n",
    "  -1.1,\n",
    "  0.1,\n",
    "  0.05,\n",
    "  0.07,\n",
    "  0.06,\n",
    "  0.04,\n",
    "  0.07,\n",
    "  0.06,\n",
    "  0.04],\n",
    " [-0.8,\n",
    "  0.9,\n",
    "  1.3,\n",
    "  0.4,\n",
    "  0.7,\n",
    "  -0.7,\n",
    "  0.6,\n",
    "  -0.9,\n",
    "  0.3,\n",
    "  -1.2,\n",
    "  0.5,\n",
    "  -0.65,\n",
    "  0.5,\n",
    "  -1.0,\n",
    "  0.2,\n",
    "  -1.3,\n",
    "  0.12,\n",
    "  0.06,\n",
    "  0.08,\n",
    "  0.07,\n",
    "  0.05,\n",
    "  0.08,\n",
    "  0.07,\n",
    "  0.05],\n",
    " [-1.0,\n",
    "  1.0,\n",
    "  1.5,\n",
    "  0.5,\n",
    "  0.8,\n",
    "  -0.8,\n",
    "  0.5,\n",
    "  -1.0,\n",
    "  0.25,\n",
    "  -1.25,\n",
    "  -0.3,\n",
    "  -0.6,\n",
    "  0.4,\n",
    "  -0.8,\n",
    "  0.2,\n",
    "  -1.0,\n",
    "  0.08,\n",
    "  0.045,\n",
    "  0.09,\n",
    "  0.075,\n",
    "  0.055,\n",
    "  0.09,\n",
    "  0.075,\n",
    "  0.055],\n",
    " [-0.3,\n",
    "  0.4,\n",
    "  0.9,\n",
    "  0.15,\n",
    "  0.25,\n",
    "  -0.25,\n",
    "  0.15,\n",
    "  -0.35,\n",
    "  0.1,\n",
    "  -0.45,\n",
    "  0.35,\n",
    "  -0.2,\n",
    "  0.2,\n",
    "  -0.3,\n",
    "  0.05,\n",
    "  -0.4,\n",
    "  0.05,\n",
    "  0.025,\n",
    "  0.035,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.035,\n",
    "  0.03,\n",
    "  0.02],\n",
    " [-0.7,\n",
    "  0.6,\n",
    "  1.2,\n",
    "  0.25,\n",
    "  0.5,\n",
    "  -0.45,\n",
    "  0.3,\n",
    "  -0.6,\n",
    "  0,\n",
    "  -0.7,\n",
    "  -0.2,\n",
    "  -0.4,\n",
    "  0.1,\n",
    "  -0.55,\n",
    "  -0.1,\n",
    "  -0.65,\n",
    "  0.06,\n",
    "  0.03,\n",
    "  0.045,\n",
    "  0.04,\n",
    "  0.035,\n",
    "  0.045,\n",
    "  0.04,\n",
    "  0.035],\n",
    " [-0.6,\n",
    "  0.75,\n",
    "  1.25,\n",
    "  0.2,\n",
    "  0.2,\n",
    "  -0.2,\n",
    "  0.05,\n",
    "  -0.25,\n",
    "  0.1,\n",
    "  -0.3,\n",
    "  0.15,\n",
    "  -0.15,\n",
    "  0.05,\n",
    "  -0.2,\n",
    "  0.02,\n",
    "  -0.25,\n",
    "  0.04,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02],\n",
    " [-0.2,\n",
    "  0.3,\n",
    "  0.5,\n",
    "  0.1,\n",
    "  0.1,\n",
    "  -0.1,\n",
    "  0,\n",
    "  -0.15,\n",
    "  -0.05,\n",
    "  -0.2,\n",
    "  -0.1,\n",
    "  -0.15,\n",
    "  0.05,\n",
    "  -0.25,\n",
    "  0.1,\n",
    "  -0.3,\n",
    "  0.02,\n",
    "  0.01,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.015],\n",
    " [-1.2,\n",
    "  1.2,\n",
    "  2.0,\n",
    "  0.6,\n",
    "  0.9,\n",
    "  -0.5,\n",
    "  0.7,\n",
    "  -1.0,\n",
    "  0.4,\n",
    "  -1.4,\n",
    "  -0.6,\n",
    "  -0.3,\n",
    "  0.2,\n",
    "  -0.5,\n",
    "  0.0,\n",
    "  -0.8,\n",
    "  0.15,\n",
    "  0.07,\n",
    "  0.1,\n",
    "  0.08,\n",
    "  0.06,\n",
    "  0.1,\n",
    "  0.08,\n",
    "  0.06],\n",
    " [-0.8,\n",
    "  1.1,\n",
    "  1.8,\n",
    "  0.3,\n",
    "  0.3,\n",
    "  -0.2,\n",
    "  -0.2,\n",
    "  -0.6,\n",
    "  0.1,\n",
    "  -1.0,\n",
    "  -0.5,\n",
    "  -0.7,\n",
    "  0.3,\n",
    "  -0.9,\n",
    "  0.1,\n",
    "  -1.1,\n",
    "  0.06,\n",
    "  0.03,\n",
    "  0.045,\n",
    "  0.04,\n",
    "  0.03,\n",
    "  0.045,\n",
    "  0.04,\n",
    "  0.03],\n",
    " [-0.5,\n",
    "  0.8,\n",
    "  1.6,\n",
    "  0.25,\n",
    "  0.2,\n",
    "  -0.2,\n",
    "  0.1,\n",
    "  -0.35,\n",
    "  0,\n",
    "  -0.5,\n",
    "  -0.3,\n",
    "  -0.3,\n",
    "  0.2,\n",
    "  -0.4,\n",
    "  0.15,\n",
    "  -0.55,\n",
    "  0.05,\n",
    "  0.025,\n",
    "  0.035,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.035,\n",
    "  0.03,\n",
    "  0.025],\n",
    " [-0.9,\n",
    "  0.9,\n",
    "  1.5,\n",
    "  0.2,\n",
    "  0.3,\n",
    "  0.0,\n",
    "  -0.1,\n",
    "  -0.2,\n",
    "  0.1,\n",
    "  -0.3,\n",
    "  0.2,\n",
    "  0.1,\n",
    "  0.1,\n",
    "  -0.1,\n",
    "  0.15,\n",
    "  -0.25,\n",
    "  0.07,\n",
    "  0.03,\n",
    "  0.045,\n",
    "  0.035,\n",
    "  0.025,\n",
    "  0.045,\n",
    "  0.035,\n",
    "  0.025],\n",
    " [-0.6,\n",
    "  1.3,\n",
    "  2.0,\n",
    "  0.4,\n",
    "  0.8,\n",
    "  -0.6,\n",
    "  -0.2,\n",
    "  -0.7,\n",
    "  0,\n",
    "  -0.9,\n",
    "  -0.7,\n",
    "  -0.2,\n",
    "  0.5,\n",
    "  -0.3,\n",
    "  0.8,\n",
    "  -0.4,\n",
    "  0.05,\n",
    "  0.02,\n",
    "  0.04,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.04,\n",
    "  0.03,\n",
    "  0.025],\n",
    " [-0.25,\n",
    "  0.55,\n",
    "  0.85,\n",
    "  0.2,\n",
    "  0.15,\n",
    "  -0.25,\n",
    "  0.05,\n",
    "  -0.45,\n",
    "  -0.02,\n",
    "  -0.65,\n",
    "  0.1,\n",
    "  -0.15,\n",
    "  0.07,\n",
    "  -0.3,\n",
    "  0.01,\n",
    "  -0.5,\n",
    "  0.03,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.017,\n",
    "  0.013,\n",
    "  0.02,\n",
    "  0.017,\n",
    "  0.013],\n",
    " [-0.1,\n",
    "  0.2,\n",
    "  0.6,\n",
    "  0.1,\n",
    "  0.8,\n",
    "  -0.5,\n",
    "  0.8,\n",
    "  -0.8,\n",
    "  0.9,\n",
    "  -0.9,\n",
    "  -0.3,\n",
    "  -0.2,\n",
    "  -0.3,\n",
    "  -0.5,\n",
    "  -0.2,\n",
    "  -0.7,\n",
    "  0.035,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02],\n",
    " [-0.15,\n",
    "  0.15,\n",
    "  0.45,\n",
    "  0.05,\n",
    "  0.05,\n",
    "  -0.05,\n",
    "  -0.05,\n",
    "  -0.1,\n",
    "  -0.1,\n",
    "  -0.15,\n",
    "  -0.1,\n",
    "  -0.05,\n",
    "  0.1,\n",
    "  -0.1,\n",
    "  0.15,\n",
    "  -0.15,\n",
    "  0.01,\n",
    "  0.005,\n",
    "  0.01,\n",
    "  0.007,\n",
    "  0.005,\n",
    "  0.01,\n",
    "  0.007,\n",
    "  0.005],\n",
    " [-0.7,\n",
    "  0.7,\n",
    "  0.9,\n",
    "  0.1,\n",
    "  -0.3,\n",
    "  -0.6,\n",
    "  -0.5,\n",
    "  -0.9,\n",
    "  -0.85,\n",
    "  -1.2,\n",
    "  0.4,\n",
    "  -0.25,\n",
    "  0.5,\n",
    "  -0.55,\n",
    "  0.6,\n",
    "  -0.8,\n",
    "  0.05,\n",
    "  0.02,\n",
    "  0.06,\n",
    "  0.05,\n",
    "  0.04,\n",
    "  0.06,\n",
    "  0.05,\n",
    "  0.04],\n",
    " [-1.0,\n",
    "  1.5,\n",
    "  2.2,\n",
    "  0.3,\n",
    "  1.2,\n",
    "  -0.3,\n",
    "  0.6,\n",
    "  -0.5,\n",
    "  -0.2,\n",
    "  -0.8,\n",
    "  0.5,\n",
    "  -0.1,\n",
    "  0.3,\n",
    "  -0.2,\n",
    "  0.05,\n",
    "  -0.4,\n",
    "  0.08,\n",
    "  0.04,\n",
    "  0.07,\n",
    "  0.06,\n",
    "  0.05,\n",
    "  0.07,\n",
    "  0.06,\n",
    "  0.05],\n",
    " [-0.5,\n",
    "  0.4,\n",
    "  0.7,\n",
    "  0.2,\n",
    "  0.1,\n",
    "  -0.15,\n",
    "  -0.1,\n",
    "  -0.35,\n",
    "  0.05,\n",
    "  -0.45,\n",
    "  -0.2,\n",
    "  -0.25,\n",
    "  0.3,\n",
    "  -0.35,\n",
    "  0.25,\n",
    "  -0.4,\n",
    "  0.03,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.017,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.017,\n",
    "  0.015],\n",
    " [-0.8,\n",
    "  0.9,\n",
    "  1.8,\n",
    "  0.4,\n",
    "  -0.2,\n",
    "  -0.3,\n",
    "  -0.15,\n",
    "  -0.45,\n",
    "  -0.1,\n",
    "  -0.65,\n",
    "  0.25,\n",
    "  -0.15,\n",
    "  0.2,\n",
    "  -0.35,\n",
    "  0.1,\n",
    "  -0.55,\n",
    "  0.04,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02],\n",
    " [-0.6,\n",
    "  0.8,\n",
    "  1.3,\n",
    "  0.4,\n",
    "  0.5,\n",
    "  -0.4,\n",
    "  0.4,\n",
    "  -0.6,\n",
    "  0.3,\n",
    "  -0.7,\n",
    "  0.4,\n",
    "  -0.3,\n",
    "  0.4,\n",
    "  -0.4,\n",
    "  0.3,\n",
    "  -0.5,\n",
    "  0.06,\n",
    "  0.03,\n",
    "  0.04,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.04,\n",
    "  0.03,\n",
    "  0.02],\n",
    " [-0.4,\n",
    "  0.6,\n",
    "  1.0,\n",
    "  0.2,\n",
    "  -0.1,\n",
    "  -0.2,\n",
    "  -0.15,\n",
    "  -0.3,\n",
    "  -0.1,\n",
    "  -0.4,\n",
    "  -0.2,\n",
    "  -0.1,\n",
    "  -0.15,\n",
    "  -0.25,\n",
    "  -0.1,\n",
    "  -0.35,\n",
    "  0.035,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.01,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.01],\n",
    " [-0.2,\n",
    "  0.3,\n",
    "  0.6,\n",
    "  0.1,\n",
    "  0.05,\n",
    "  -0.03,\n",
    "  -0.02,\n",
    "  -0.05,\n",
    "  -0.03,\n",
    "  -0.07,\n",
    "  -0.04,\n",
    "  -0.02,\n",
    "  0.02,\n",
    "  -0.03,\n",
    "  0.03,\n",
    "  -0.05,\n",
    "  0.02,\n",
    "  0.01,\n",
    "  0.018,\n",
    "  0.015,\n",
    "  0.012,\n",
    "  0.018,\n",
    "  0.015,\n",
    "  0.012],\n",
    " [-0.3,\n",
    "  0.2,\n",
    "  0.6,\n",
    "  -0.1,\n",
    "  0,\n",
    "  -0.2,\n",
    "  0,\n",
    "  -0.3,\n",
    "  0.2,\n",
    "  -0.4,\n",
    "  -0.3,\n",
    "  -0.1,\n",
    "  0.1,\n",
    "  -0.2,\n",
    "  0.2,\n",
    "  -0.3,\n",
    "  0.025,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.01,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.01],\n",
    " [-1.2,\n",
    "  2.3,\n",
    "  3.5,\n",
    "  0.8,\n",
    "  1.5,\n",
    "  -0.8,\n",
    "  1.3,\n",
    "  -1.0,\n",
    "  1.7,\n",
    "  -1.3,\n",
    "  -1.4,\n",
    "  -1.0,\n",
    "  -1.3,\n",
    "  -1.4,\n",
    "  -1.7,\n",
    "  -1.8,\n",
    "  0.15,\n",
    "  0.07,\n",
    "  0.12,\n",
    "  0.1,\n",
    "  0.08,\n",
    "  0.12,\n",
    "  0.1,\n",
    "  0.08],\n",
    " [-0.6,\n",
    "  1.2,\n",
    "  1.8,\n",
    "  0.15,\n",
    "  -0.5,\n",
    "  -0.45,\n",
    "  -0.6,\n",
    "  -0.85,\n",
    "  -0.8,\n",
    "  -1.2,\n",
    "  0.2,\n",
    "  -0.15,\n",
    "  0.1,\n",
    "  -0.25,\n",
    "  0.05,\n",
    "  -0.35,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.04,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.04,\n",
    "  0.025,\n",
    "  0.02],\n",
    " [-0.8,\n",
    "  0.5,\n",
    "  1.0,\n",
    "  0.3,\n",
    "  0.2,\n",
    "  -0.1,\n",
    "  0.3,\n",
    "  -0.6,\n",
    "  0.1,\n",
    "  -0.9,\n",
    "  0.4,\n",
    "  -0.2,\n",
    "  0.2,\n",
    "  -0.4,\n",
    "  0.05,\n",
    "  -0.6,\n",
    "  0.06,\n",
    "  0.03,\n",
    "  0.045,\n",
    "  0.04,\n",
    "  0.03,\n",
    "  0.045,\n",
    "  0.04,\n",
    "  0.03],\n",
    " [-0.2,\n",
    "  0.3,\n",
    "  0.9,\n",
    "  0.1,\n",
    "  0.6,\n",
    "  -0.3,\n",
    "  0.8,\n",
    "  -0.6,\n",
    "  1.0,\n",
    "  -0.8,\n",
    "  1.0,\n",
    "  -0.2,\n",
    "  1.2,\n",
    "  -0.4,\n",
    "  1.5,\n",
    "  -0.6,\n",
    "  0.02,\n",
    "  0.01,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015],\n",
    " [-0.1,\n",
    "  0.2,\n",
    "  0.4,\n",
    "  0.3,\n",
    "  0.5,\n",
    "  -0.2,\n",
    "  0.5,\n",
    "  -0.4,\n",
    "  0.5,\n",
    "  -0.6,\n",
    "  0.4,\n",
    "  -0.3,\n",
    "  0.4,\n",
    "  -0.5,\n",
    "  0.3,\n",
    "  -0.6,\n",
    "  0.02,\n",
    "  0.01,\n",
    "  0.015,\n",
    "  0.013,\n",
    "  0.011,\n",
    "  0.015,\n",
    "  0.013,\n",
    "  0.011],\n",
    " [-0.5,\n",
    "  0.9,\n",
    "  1.7,\n",
    "  0.3,\n",
    "  0.6,\n",
    "  -0.1,\n",
    "  0.6,\n",
    "  -0.5,\n",
    "  1.0,\n",
    "  -0.8,\n",
    "  0.3,\n",
    "  -0.1,\n",
    "  0.3,\n",
    "  -0.5,\n",
    "  0.6,\n",
    "  -0.8,\n",
    "  0.04,\n",
    "  0.02,\n",
    "  0.035,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.035,\n",
    "  0.03,\n",
    "  0.025],\n",
    " [-0.35,\n",
    "  0.6,\n",
    "  1.25,\n",
    "  0.4,\n",
    "  0.45,\n",
    "  -0.2,\n",
    "  0.2,\n",
    "  -0.4,\n",
    "  -0.05,\n",
    "  -0.6,\n",
    "  0.6,\n",
    "  -0.35,\n",
    "  0.15,\n",
    "  -0.65,\n",
    "  -0.1,\n",
    "  -0.85,\n",
    "  0.035,\n",
    "  0.015,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.025,\n",
    "  0.02],\n",
    " [-0.9,\n",
    "  1.0,\n",
    "  2.2,\n",
    "  0.5,\n",
    "  -1.1,\n",
    "  -0.9,\n",
    "  -1.4,\n",
    "  -1.2,\n",
    "  -1.8,\n",
    "  -1.5,\n",
    "  0.8,\n",
    "  -1.0,\n",
    "  1.3,\n",
    "  -1.3,\n",
    "  1.8,\n",
    "  -1.6,\n",
    "  0.09,\n",
    "  0.04,\n",
    "  0.07,\n",
    "  0.06,\n",
    "  0.05,\n",
    "  0.07,\n",
    "  0.06,\n",
    "  0.05],\n",
    " [-0.2,\n",
    "  0.3,\n",
    "  0.7,\n",
    "  0.15,\n",
    "  0.05,\n",
    "  -0.05,\n",
    "  0.02,\n",
    "  -0.08,\n",
    "  0.08,\n",
    "  -0.13,\n",
    "  0.05,\n",
    "  -0.03,\n",
    "  0.08,\n",
    "  -0.07,\n",
    "  0.12,\n",
    "  -0.11,\n",
    "  0.025,\n",
    "  0.012,\n",
    "  0.018,\n",
    "  0.016,\n",
    "  0.014,\n",
    "  0.018,\n",
    "  0.016,\n",
    "  0.014],\n",
    " [-0.6,\n",
    "  0.9,\n",
    "  1.4,\n",
    "  0.3,\n",
    "  0.8,\n",
    "  -0.7,\n",
    "  0.3,\n",
    "  -1.2,\n",
    "  0.5,\n",
    "  -1.6,\n",
    "  0.7,\n",
    "  -0.2,\n",
    "  0.3,\n",
    "  -0.6,\n",
    "  0.1,\n",
    "  -1.0,\n",
    "  0.06,\n",
    "  0.025,\n",
    "  0.045,\n",
    "  0.035,\n",
    "  0.03,\n",
    "  0.045,\n",
    "  0.035,\n",
    "  0.03],\n",
    " [-0.55,\n",
    "  0.55,\n",
    "  0.75,\n",
    "  0.21,\n",
    "  0.6,\n",
    "  -0.6,\n",
    "  0.2,\n",
    "  -0.8,\n",
    "  -0.1,\n",
    "  -1.0,\n",
    "  0.5,\n",
    "  -0.5,\n",
    "  0.2,\n",
    "  -0.7,\n",
    "  0.15,\n",
    "  -0.9,\n",
    "  0.045,\n",
    "  0.022,\n",
    "  0.038,\n",
    "  0.032,\n",
    "  0.025,\n",
    "  0.038,\n",
    "  0.032,\n",
    "  0.025],\n",
    " [-0.3,\n",
    "  1.2,\n",
    "  2.4,\n",
    "  0.5,\n",
    "  1.0,\n",
    "  -0.6,\n",
    "  0.9,\n",
    "  -0.8,\n",
    "  1.2,\n",
    "  -1.1,\n",
    "  0.6,\n",
    "  -0.4,\n",
    "  1.1,\n",
    "  -0.6,\n",
    "  1.4,\n",
    "  -0.9,\n",
    "  0.07,\n",
    "  0.05,\n",
    "  0.09,\n",
    "  0.07,\n",
    "  0.06,\n",
    "  0.09,\n",
    "  0.07,\n",
    "  0.06],\n",
    " [-0.8,\n",
    "  0.4,\n",
    "  0.5,\n",
    "  0.05,\n",
    "  -0.3,\n",
    "  -0.6,\n",
    "  -0.6,\n",
    "  -0.8,\n",
    "  -0.85,\n",
    "  -1.1,\n",
    "  0.3,\n",
    "  -0.45,\n",
    "  0.5,\n",
    "  -0.6,\n",
    "  0.65,\n",
    "  -0.75,\n",
    "  0.02,\n",
    "  0.01,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.012,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.012],\n",
    " [-0.2,\n",
    "  0.35,\n",
    "  0.6,\n",
    "  0.04,\n",
    "  0.7,\n",
    "  -0.3,\n",
    "  0.5,\n",
    "  -0.4,\n",
    "  -0.45,\n",
    "  -0.55,\n",
    "  -0.4,\n",
    "  -0.45,\n",
    "  0.1,\n",
    "  -0.5,\n",
    "  0.05,\n",
    "  -0.55,\n",
    "  0.02,\n",
    "  0.008,\n",
    "  0.015,\n",
    "  0.012,\n",
    "  0.01,\n",
    "  0.015,\n",
    "  0.012,\n",
    "  0.01],\n",
    " [-0.45,\n",
    "  0.45,\n",
    "  0.75,\n",
    "  0.12,\n",
    "  -0.225,\n",
    "  -0.225,\n",
    "  -0.075,\n",
    "  -0.225,\n",
    "  0.075,\n",
    "  -0.3,\n",
    "  0.225,\n",
    "  -0.075,\n",
    "  0.15,\n",
    "  -0.225,\n",
    "  0.3,\n",
    "  -0.3,\n",
    "  0.03,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.018,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.018,\n",
    "  0.015],\n",
    " [-0.5,\n",
    "  0.3,\n",
    "  0.75,\n",
    "  0.1,\n",
    "  0.6,\n",
    "  -0.4,\n",
    "  0.7,\n",
    "  -0.6,\n",
    "  0.9,\n",
    "  -0.8,\n",
    "  -0.6,\n",
    "  -0.5,\n",
    "  -0.7,\n",
    "  -0.7,\n",
    "  -0.9,\n",
    "  -0.9,\n",
    "  0.025,\n",
    "  0.015,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015],\n",
    " [-0.7,\n",
    "  0.8,\n",
    "  1.8,\n",
    "  0.6,\n",
    "  1,\n",
    "  -0.5,\n",
    "  0.9,\n",
    "  -0.8,\n",
    "  0.6,\n",
    "  -1,\n",
    "  -0.8,\n",
    "  -0.4,\n",
    "  -0.7,\n",
    "  -0.7,\n",
    "  -0.9,\n",
    "  -0.9,\n",
    "  0.02,\n",
    "  0.01,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015],\n",
    " [-0.7,\n",
    "  1.0,\n",
    "  1.2,\n",
    "  0.2,\n",
    "  0.3,\n",
    "  -0.1,\n",
    "  0.1,\n",
    "  -0.15,\n",
    "  -0.05,\n",
    "  -0.3,\n",
    "  0.2,\n",
    "  0,\n",
    "  0.2,\n",
    "  -0.1,\n",
    "  0.15,\n",
    "  -0.2,\n",
    "  0.03,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.018,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.018,\n",
    "  0.015],\n",
    " [-0.5,\n",
    "  0.9,\n",
    "  1.4,\n",
    "  0.2,\n",
    "  0.1,\n",
    "  -0.25,\n",
    "  0.3,\n",
    "  -0.6,\n",
    "  0.4,\n",
    "  -0.9,\n",
    "  -0.3,\n",
    "  -0.2,\n",
    "  0.2,\n",
    "  -0.5,\n",
    "  0.15,\n",
    "  -0.8,\n",
    "  0.04,\n",
    "  0.02,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.03,\n",
    "  0.02,\n",
    "  0.015],\n",
    " [-0.8,\n",
    "  1.5,\n",
    "  2.5,\n",
    "  0.2,\n",
    "  0.9,\n",
    "  -0.2,\n",
    "  1.2,\n",
    "  -0.4,\n",
    "  1.5,\n",
    "  -0.5,\n",
    "  0.7,\n",
    "  -0.1,\n",
    "  1.0,\n",
    "  -0.3,\n",
    "  1.3,\n",
    "  -0.4,\n",
    "  0.06,\n",
    "  0.035,\n",
    "  0.07,\n",
    "  0.05,\n",
    "  0.04,\n",
    "  0.07,\n",
    "  0.05,\n",
    "  0.04],\n",
    " [-0.6,\n",
    "  0.2,\n",
    "  0.8,\n",
    "  0.25,\n",
    "  -0.6,\n",
    "  -0.8,\n",
    "  -0.8,\n",
    "  -1.2,\n",
    "  -1.0,\n",
    "  -1.5,\n",
    "  -0.4,\n",
    "  -0.5,\n",
    "  0.2,\n",
    "  -0.8,\n",
    "  0.45,\n",
    "  -1.2,\n",
    "  0.04,\n",
    "  0.02,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.015],\n",
    " [-0.3,\n",
    "  0.6,\n",
    "  1.2,\n",
    "  0.5,\n",
    "  0.4,\n",
    "  -0.3,\n",
    "  0.3,\n",
    "  -0.4,\n",
    "  0.1,\n",
    "  -0.5,\n",
    "  -0.4,\n",
    "  -0.2,\n",
    "  -0.2,\n",
    "  -0.3,\n",
    "  -0.1,\n",
    "  -0.4,\n",
    "  0.03,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.018,\n",
    "  0.015,\n",
    "  0.02,\n",
    "  0.018,\n",
    "  0.015],\n",
    " [-0.8,\n",
    "  1.4,\n",
    "  2.1,\n",
    "  0.5,\n",
    "  0.9,\n",
    "  -0.4,\n",
    "  0.7,\n",
    "  -0.6,\n",
    "  0.2,\n",
    "  -0.7,\n",
    "  0.6,\n",
    "  -0.3,\n",
    "  0.5,\n",
    "  -0.4,\n",
    "  0.1,\n",
    "  -0.5,\n",
    "  0.045,\n",
    "  0.02,\n",
    "  0.032,\n",
    "  0.025,\n",
    "  0.015,\n",
    "  0.032,\n",
    "  0.025,\n",
    "  0.015],\n",
    " [-0.1,\n",
    "  0.24,\n",
    "  0.25,\n",
    "  0.05,\n",
    "  -0.08,\n",
    "  -0.07,\n",
    "  0.12,\n",
    "  -0.23,\n",
    "  0.2,\n",
    "  -0.5,\n",
    "  0,\n",
    "  -0.1,\n",
    "  0.15,\n",
    "  -0.2,\n",
    "  0.2,\n",
    "  -0.45,\n",
    "  0.02,\n",
    "  0.01,\n",
    "  0.015,\n",
    "  0.012,\n",
    "  0.01,\n",
    "  0.015,\n",
    "  0.012,\n",
    "  0.01],\n",
    " [-0.6,\n",
    "  0.4,\n",
    "  0.5,\n",
    "  0.1,\n",
    "  0,\n",
    "  -0.1,\n",
    "  -0.1,\n",
    "  -0.25,\n",
    "  0.03,\n",
    "  -0.4,\n",
    "  -0.2,\n",
    "  -0.2,\n",
    "  -0.1,\n",
    "  -0.3,\n",
    "  0,\n",
    "  -0.5,\n",
    "  0.04,\n",
    "  0.02,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.015,\n",
    "  0.025,\n",
    "  0.02,\n",
    "  0.015]])\n",
    "\n",
    "material_list = [compute_cheetah_volume(parameter) for parameter in parameter_list]\n",
    "parameter_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15 ,  0.15 ,  0.45 ,  0.05 ,  0.05 , -0.05 , -0.05 , -0.1  ,\n",
       "       -0.1  , -0.15 , -0.1  , -0.05 ,  0.1  , -0.1  ,  0.15 , -0.15 ,\n",
       "        0.01 ,  0.005,  0.01 ,  0.007,  0.005,  0.01 ,  0.007,  0.005])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_list[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'morphology_nums:{morphology_nums}')\n",
    "logging.info(f'rewardfunc_nums:{rewardfunc_nums}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'morphology_list:{morphology_list}')\n",
    "logging.info(f'material_list:{material_list}')\n",
    "logging.info(f'_________________________________enter coarse optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 results/Div_m25_r5/env/GPTrewardfunc_5.py\n",
      "50 results/Div_m25_r5/assets/GPTCheetah_50.xml\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i, rewardfunc in enumerate(rewardfunc_list):\n",
    "    for j, morphology in enumerate(morphology_list):\n",
    "        if i not in [5]:\n",
    "            continue\n",
    "        if j not in [50]:\n",
    "            continue\n",
    "        # if j < 24:\n",
    "        #     continue\n",
    "        print(i, rewardfunc)\n",
    "        print(j, morphology)\n",
    "        shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "        shutil.copy(rewardfunc, \"GPTrewardfunc.py\")         \n",
    "\n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "        # model_path = Train(j,  i, folder_name, total_timesteps=5e5)\n",
    "        model_path = f\"results/Div_m25_r5/coarse/SAC_morphology{j}_rewardfunc{i}_500000.0steps\"\n",
    "        fitness, reward = Eva(model_path)\n",
    "        material = material_list[j]\n",
    "        efficiency = fitness/material\n",
    "        fitness_matrix[i][j] = fitness\n",
    "        efficiency_matrix[i][j] = efficiency\n",
    "        \n",
    "        logging.info(\"___________________finish coarse optimization_____________________\")\n",
    "        logging.info(f\"morphology: {j}, rewardfunc: {i}, material cost: {material} reward: {reward} fitness: {fitness} efficiency: {efficiency}\")\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness = fitness\n",
    "            best_morphology = morphology\n",
    "            best_efficiency = efficiency\n",
    "            best_rewardfunc = rewardfunc\n",
    "            best_material = material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitness_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix_select = efficiency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print coarse optimization info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(f'_________________________________end coarse optimization stage_________________________________')\n",
    "logging.info(f\"Stage1: Final best morphology: {best_morphology}, Fitness: {best_fitness}, best_efficiency: {best_efficiency}, best reward function: {best_rewardfunc}, Material cost: {best_material}, Reward: {best_reward}\")\n",
    "logging.info(f'folder_name:{folder_name}')\n",
    "logging.info(f'parameter_list:{parameter_list}')\n",
    "logging.info(f'fitness_matrix:{fitness_matrix}')\n",
    "logging.info(f'efficiency_matrix:{efficiency_matrix}')\n",
    "logging.info(f'_________________________________enter fine optimization stage_________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration of fine optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enter fine optimization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameter:[-0.6    0.4    0.5    0.1    0.    -0.1   -0.1   -0.25   0.03  -0.4\n",
      " -0.2   -0.2   -0.1   -0.3    0.    -0.5    0.04   0.02   0.025  0.02\n",
      "  0.015  0.025  0.02   0.015]\n",
      "[-0.65, 0.45, 0.55, 0.12, 0.05, -0.15, -0.15, -0.2, 0.04, -0.35, -0.25, -0.25, -0.15, -0.35, 0.05, -0.55, 0.05, 0.03, 0.022, 0.019, 0.013, 0.022, 0.018, 0.014]\n",
      "Successfully saved GPTCheetah_refine2_5_50_1.xml\n",
      "improved parameter [-0.65, 0.45, 0.55, 0.12, 0.05, -0.15, -0.15, -0.2, 0.04, -0.35, -0.25, -0.25, -0.15, -0.35, 0.05, -0.55, 0.05, 0.03, 0.022, 0.019, 0.013, 0.022, 0.018, 0.014]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\nTask Description: The HalfCheetah is a 2-dimensional robot consisting of 9 body parts and 8 joints connecting them (including two paws). The goal is to apply torque to the joints to make the cheetah run forward (right) as fast as possible, with a positive reward based on the distance moved forward and a negative reward for moving backward.\\nThe cheetah\\'s torso and head are fixed, and torque can only be applied to the other 6 joints over the front and back thighs (which connect to the torso), the shins (which connect to the thighs), and the feet (which connect to the shins), and the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\nHere is the environment codes:\\nclass CheetahEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"half_cheetah.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 0.1,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\"x_position\": x_position_after, \"x_velocity\": x_velocity, **reward_info}\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n        }  \\n\\nThere are also some reward functions and their fitness. \\nYou should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.Please carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward function to encourage high-speed leap-and-recover style of motion.\\n    # The idea is to reward significant positive changes in velocity, simulating a leaping motion,\\n    # followed by minimal action to maintain a pose, simulating recovery. This mimics explosive\\n    # power actions followed by stability phases common in various dynamic sports or animal movements.\\n\\n    # Calculate basic forward motion reward\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Calculate control cost\\n    control_cost = self.control_cost(action)\\n\\n    # Encourage big positive spikes in velocity (leaping) followed by periods of lower velocity (recovery)\\n    # By rewarding the derivative of the velocity (acceleration) when it\\'s positive and above a threshold\\n    if not hasattr(self, \\'last_velocity\\'):\\n        self.last_velocity = x_velocity\\n    velocity_change = x_velocity - self.last_velocity\\n    self.last_velocity = x_velocity\\n\\n    # Only reward positive spikes in velocity change which are greater than 0.1 (threshold for \\'leap\\')\\n    leap_reward = 0\\n    if velocity_change > 0.1:\\n        leap_reward = np.exp(velocity_change * 10) - 1  # Scale and exponentiate for significant impact\\n\\n    # Combine all components\\n    total_reward = forward_reward - control_cost + leap_reward\\n\\n    # Reward info for debugging purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'leap_reward\\': leap_reward,\\n        \\'total_reward\\': total_reward\\n    }\\n\\n    return total_reward, reward_info\\n\\nfitness: 12808.292180728402\\n\\nbest reward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward function to encourage high-speed leap-and-recover style of motion.\\n    # The idea is to reward significant positive changes in velocity, simulating a leaping motion,\\n    # followed by minimal action to maintain a pose, simulating recovery. This mimics explosive\\n    # power actions followed by stability phases common in various dynamic sports or animal movements.\\n\\n    # Calculate basic forward motion reward\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Calculate control cost\\n    control_cost = self.control_cost(action)\\n\\n    # Encourage big positive spikes in velocity (leaping) followed by periods of lower velocity (recovery)\\n    # By rewarding the derivative of the velocity (acceleration) when it\\'s positive and above a threshold\\n    if not hasattr(self, \\'last_velocity\\'):\\n        self.last_velocity = x_velocity\\n    velocity_change = x_velocity - self.last_velocity\\n    self.last_velocity = x_velocity\\n\\n    # Only reward positive spikes in velocity change which are greater than 0.1 (threshold for \\'leap\\')\\n    leap_reward = 0\\n    if velocity_change > 0.1:\\n        leap_reward = np.exp(velocity_change * 10) - 1  # Scale and exponentiate for significant impact\\n\\n    # Combine all components\\n    total_reward = forward_reward - control_cost + leap_reward\\n\\n    # Reward info for debugging purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'leap_reward\\': leap_reward,\\n        \\'total_reward\\': total_reward\\n    }\\n\\n    return total_reward, reward_info\\n\\nbest fitness: 12808.292180728402\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVurWcb09CjMrcJP17PWVCL34KkVD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward function designed to optimize for high velocities with controlled motions.\\n    \\n    # Basic forward motion reward based on current velocity\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Calculate control cost based on the action taken\\n    control_cost = self.control_cost(action)\\n\\n    # Encourage significant positive changes in velocity (leaps)\\n    if not hasattr(self, 'last_velocity'):\\n        self.last_velocity = x_velocity\\n    velocity_change = x_velocity - self.last_velocity\\n    self.last_velocity = x_velocity\\n\\n    # Reward for larger positive spikes in velocity change, using a higher threshold for leap\\n    leap_reward = 0\\n    if velocity_change > 0.15:  # Increased threshold for more pronounced leaps\\n        leap_reward = np.exp(velocity_change * 12) - 1  # Higher scaling factor for leap impact\\n\\n    # Penalty for velocity falling below a minimum speed, encouraging sustained motion\\n    velocity_penalty = 0\\n    if x_velocity < 0.5:  # Threshold for minimum desired speed\\n        velocity_penalty = 1.0 - x_velocity  # Penalize reduction in speed\\n\\n    # Combine all components into the total reward\\n    total_reward = forward_reward - control_cost + leap_reward - velocity_penalty\\n\\n    # Reward information for diagnostics\\n    reward_info = {\\n        'forward_reward': forward_reward,\\n        'control_cost': control_cost,\\n        'leap_reward': leap_reward,\\n        'velocity_penalty': velocity_penalty,\\n        'total_reward': total_reward\\n    }\\n\\n    return total_reward, reward_info\\n```\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746946446, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=350, prompt_tokens=2104, total_tokens=2454, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 138.77250750838263\n",
      "[-0.6, 0.5, 0.6, 0.1, 0.1, -0.15, -0.15, -0.2, 0.04, -0.3, -0.2, -0.2, -0.05, -0.25, 0.05, -0.45, 0.05, 0.03, 0.03, 0.025, 0.02, 0.02, 0.025, 0.02]\n",
      "Successfully saved GPTCheetah_refine2_5_50_3.xml\n",
      "improved parameter [-0.6, 0.5, 0.6, 0.1, 0.1, -0.15, -0.15, -0.2, 0.04, -0.3, -0.2, -0.2, -0.05, -0.25, 0.05, -0.45, 0.05, 0.03, 0.03, 0.025, 0.02, 0.02, 0.025, 0.02]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "[{'role': 'system', 'content': 'You are a reinforcement learning reward function designer'}, {'role': 'user', 'content': '\\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible.\\nYour goal is to write a reward function for the enviroment that will help the agent learn the task described in text.\\nTask Description: The HalfCheetah is a 2-dimensional robot consisting of 9 body parts and 8 joints connecting them (including two paws). The goal is to apply torque to the joints to make the cheetah run forward (right) as fast as possible, with a positive reward based on the distance moved forward and a negative reward for moving backward.\\nThe cheetah\\'s torso and head are fixed, and torque can only be applied to the other 6 joints over the front and back thighs (which connect to the torso), the shins (which connect to the thighs), and the feet (which connect to the shins), and the goal is to move as fast as possible towards the right by applying torque to the rotors and using fluid friction and you should write a reward function to make the robot move as faster as possible.\\nHere is the environment codes:\\nclass CheetahEnv(MujocoEnv, utils.EzPickle):\\n    metadata = {\\n        \"render_modes\": [\\n            \"human\",\\n            \"rgb_array\",\\n            \"depth_array\",\\n        ],\\n    }\\n\\n    def __init__(\\n        self,\\n        xml_file: str = \"half_cheetah.xml\",\\n        frame_skip: int = 5,\\n        default_camera_config: Dict[str, Union[float, int]] = DEFAULT_CAMERA_CONFIG,\\n        forward_reward_weight: float = 1.0,\\n        ctrl_cost_weight: float = 0.1,\\n        reset_noise_scale: float = 0.1,\\n        exclude_current_positions_from_observation: bool = True,\\n        **kwargs,\\n    ):\\n        utils.EzPickle.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            default_camera_config,\\n            forward_reward_weight,\\n            ctrl_cost_weight,\\n            reset_noise_scale,\\n            exclude_current_positions_from_observation,\\n            **kwargs,\\n        )\\n\\n        self._forward_reward_weight = forward_reward_weight\\n        self._ctrl_cost_weight = ctrl_cost_weight\\n\\n        self._reset_noise_scale = reset_noise_scale\\n\\n        self._exclude_current_positions_from_observation = (\\n            exclude_current_positions_from_observation\\n        )\\n\\n        MujocoEnv.__init__(\\n            self,\\n            xml_file,\\n            frame_skip,\\n            observation_space=None,\\n            default_camera_config=default_camera_config,\\n            **kwargs,\\n        )\\n\\n        self.metadata = {\\n            \"render_modes\": [\\n                \"human\",\\n                \"rgb_array\",\\n                \"depth_array\",\\n            ],\\n            \"render_fps\": int(np.round(1.0 / self.dt)),\\n        }\\n\\n        obs_size = (\\n            self.data.qpos.size\\n            + self.data.qvel.size\\n            - exclude_current_positions_from_observation\\n        )\\n        self.observation_space = Box(\\n            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\\n        )\\n\\n        self.observation_structure = {\\n            \"skipped_qpos\": 1 * exclude_current_positions_from_observation,\\n            \"qpos\": self.data.qpos.size\\n            - 1 * exclude_current_positions_from_observation,\\n            \"qvel\": self.data.qvel.size,\\n        }\\n\\n    def control_cost(self, action):\\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\\n        return control_cost\\n\\n    def step(self, action):\\n        x_position_before = self.data.qpos[0]\\n        self.do_simulation(action, self.frame_skip)\\n        x_position_after = self.data.qpos[0]\\n        x_velocity = (x_position_after - x_position_before) / self.dt\\n\\n        observation = self._get_obs()\\n        reward, reward_info = self._get_rew(x_velocity, action)\\n        info = {\"x_position\": x_position_after, \"x_velocity\": x_velocity, **reward_info}\\n\\n        if self.render_mode == \"human\":\\n            self.render()\\n        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\\n        return observation, reward, False, False, info\\n\\n    def _get_obs(self):\\n        position = self.data.qpos.flatten()\\n        velocity = self.data.qvel.flatten()\\n\\n        if self._exclude_current_positions_from_observation:\\n            position = position[1:]\\n\\n        observation = np.concatenate((position, velocity)).ravel()\\n        return observation\\n\\n    def reset_model(self):\\n        noise_low = -self._reset_noise_scale\\n        noise_high = self._reset_noise_scale\\n\\n        qpos = self.init_qpos + self.np_random.uniform(\\n            low=noise_low, high=noise_high, size=self.model.nq\\n        )\\n        qvel = (\\n            self.init_qvel\\n            + self._reset_noise_scale * self.np_random.standard_normal(self.model.nv)\\n        )\\n\\n        self.set_state(qpos, qvel)\\n\\n        observation = self._get_obs()\\n        return observation\\n\\n    def _get_reset_info(self):\\n        return {\\n            \"x_position\": self.data.qpos[0],\\n        }  \\n\\nThere are also some reward functions and their fitness. \\nYou should observe the highest rewards from the fitness function and further encourage those motion patterns. Also, analyze the low fitness rewards, identify their shortcomings, and avoid those motion patterns. This is very important.Please carefully observe these reward funcions and their fitness, try to write a reward function to further improve the fitness.\\n\\n\\nreward function:\\nimport numpy as np\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward function to encourage high-speed leap-and-recover style of motion.\\n    # The idea is to reward significant positive changes in velocity, simulating a leaping motion,\\n    # followed by minimal action to maintain a pose, simulating recovery. This mimics explosive\\n    # power actions followed by stability phases common in various dynamic sports or animal movements.\\n\\n    # Calculate basic forward motion reward\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Calculate control cost\\n    control_cost = self.control_cost(action)\\n\\n    # Encourage big positive spikes in velocity (leaping) followed by periods of lower velocity (recovery)\\n    # By rewarding the derivative of the velocity (acceleration) when it\\'s positive and above a threshold\\n    if not hasattr(self, \\'last_velocity\\'):\\n        self.last_velocity = x_velocity\\n    velocity_change = x_velocity - self.last_velocity\\n    self.last_velocity = x_velocity\\n\\n    # Only reward positive spikes in velocity change which are greater than 0.1 (threshold for \\'leap\\')\\n    leap_reward = 0\\n    if velocity_change > 0.1:\\n        leap_reward = np.exp(velocity_change * 10) - 1  # Scale and exponentiate for significant impact\\n\\n    # Combine all components\\n    total_reward = forward_reward - control_cost + leap_reward\\n\\n    # Reward info for debugging purposes\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'leap_reward\\': leap_reward,\\n        \\'total_reward\\': total_reward\\n    }\\n\\n    return total_reward, reward_info\\n\\nfitness: 12808.292180728402\\n\\nbest reward function:\\nimport numpy as np \\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward function designed to optimize for high velocities with controlled motions.\\n    \\n    # Basic forward motion reward based on current velocity\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Calculate control cost based on the action taken\\n    control_cost = self.control_cost(action)\\n\\n    # Encourage significant positive changes in velocity (leaps)\\n    if not hasattr(self, \\'last_velocity\\'):\\n        self.last_velocity = x_velocity\\n    velocity_change = x_velocity - self.last_velocity\\n    self.last_velocity = x_velocity\\n\\n    # Reward for larger positive spikes in velocity change, using a higher threshold for leap\\n    leap_reward = 0\\n    if velocity_change > 0.15:  # Increased threshold for more pronounced leaps\\n        leap_reward = np.exp(velocity_change * 12) - 1  # Higher scaling factor for leap impact\\n\\n    # Penalty for velocity falling below a minimum speed, encouraging sustained motion\\n    velocity_penalty = 0\\n    if x_velocity < 0.5:  # Threshold for minimum desired speed\\n        velocity_penalty = 1.0 - x_velocity  # Penalize reduction in speed\\n\\n    # Combine all components into the total reward\\n    total_reward = forward_reward - control_cost + leap_reward - velocity_penalty\\n\\n    # Reward information for diagnostics\\n    reward_info = {\\n        \\'forward_reward\\': forward_reward,\\n        \\'control_cost\\': control_cost,\\n        \\'leap_reward\\': leap_reward,\\n        \\'velocity_penalty\\': velocity_penalty,\\n        \\'total_reward\\': total_reward\\n    }\\n\\n    return total_reward, reward_info\\n\\nbest fitness: 12808.292180728402\\n\\na template reward can be:\\n    def _get_rew(self, x_velocity: float, action):\\n        <reward function code you should write>\\n        return reward, reward_info    \\n\\nThe output of the reward function should consist of two items:  \\n(1) `reward`, which is the total reward.  \\n(2) `reward_info`, a dictionary of each individual reward component.  \\n\\nThe code output should be formatted as a Python code string: `\\'```python ...```\\'`.  \\n\\nSome helpful tips for writing the reward function code:  \\n(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like `numpy.exp` to the overall reward or its components.    \\n(3) Make sure the type of each input variable is correctly specified and the function name is \"def _get_rew():\"\\n(4) Most importantly, the reward code\\'s input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix `self.`). Under no circumstances can you introduce new input variables.\\n'}]\n",
      "ChatCompletion(id='chatcmpl-BVv1i4vFtr5cj4QLhm1Z5XeTQSP5i', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Based on the analysis of the previous reward functions and their performance, we can enhance the reward function to encourage rapid forward motion even further while controlling for excessive torque usage and sustaining a minimum speed. Here's an improved version of the reward function that accounts for these factors:\\n\\n```python\\ndef _get_rew(self, x_velocity: float, action):\\n    # Reward function designed to promote high-speed and controlled maneuvers\\n    \\n    # Basic forward motion reward based on current velocity\\n    forward_reward = self._forward_reward_weight * x_velocity\\n\\n    # Compute control cost\\n    control_cost = self.control_cost(action)\\n\\n    # Encourage distinct increases in velocity (leaps) with a higher threshold\\n    if not hasattr(self, 'last_velocity'):\\n        self.last_velocity = x_velocity\\n    velocity_change = x_velocity - self.last_velocity\\n    self.last_velocity = x_velocity\\n\\n    # Reward for larger positive spikes in velocity change\\n    leap_reward = 0\\n    if velocity_change > 0.2:  # Higher threshold for more significant leaps\\n        leap_reward = np.exp(velocity_change * 15) - 1  # Increased impact based on leap\\n\\n    # Penalty for falling below a minimum speed, fostering consistent high-speed movement\\n    velocity_penalty = 0\\n    if x_velocity < 0.6:  # Stricter threshold for minimum acceptable speed\\n        velocity_penalty = 1.0 - x_velocity  # More severe penalty for reduced speed\\n\\n    # Control for excessive torque usage by including a penalty related to action magnitude\\n    torque_penalty = self._ctrl_cost_weight * np.sum(np.square(action)) * 0.5  # Reduce torque's influence\\n\\n    # Combine all components into the total reward\\n    total_reward = forward_reward - control_cost + leap_reward - velocity_penalty - torque_penalty\\n\\n    # Reward information for diagnostics\\n    reward_info = {\\n        'forward_reward': forward_reward,\\n        'control_cost': control_cost,\\n        'leap_reward': leap_reward,\\n        'velocity_penalty': velocity_penalty,\\n        'torque_penalty': torque_penalty,\\n        'total_reward': total_reward\\n    }\\n\\n    return total_reward, reward_info\\n```\\n\\n### Key Changes and Enhancements:\\n1. **Increased Leap Threshold**: The leap threshold is raised to 0.2 to promote more aggressive leaps while reducing the frequency of minor adjustments.\\n2. **Higher Leap Reward Scaling**: The scaling factor for leap reward is increased to 15 to amplify the rewards for larger leaps.\\n3. **Stricter Minimum Speed Requirement**: The penalty for not maintaining a sufficient speed is tightened to 0.6, emphasizing the importance of sustained motion.\\n4. **Torque Penalty Adjustment**: The torque penalty is weighted slightly (0.5) to ensure control cost does not overly penalize the use of torque while still encouraging efficiency.\\n\\nThis new reward function retains the benefits of the previous designs, encouraging high velocities, significant motion bursts (leaps), and controlled movement while maintaining a focus on efficiency by managing torque usage.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746947078, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=635, prompt_tokens=2104, total_tokens=2739, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "improved_fitness 99.95450390553697\n"
     ]
    }
   ],
   "source": [
    "final_optimized_results = []  # 用来记录每个 coarse_best 的最优结果\n",
    "coarse_best = [(5,50)]\n",
    "for rewardfunc_index, morphology_index in coarse_best:\n",
    "    \n",
    "    morphology = morphology_list[morphology_index]\n",
    "    parameter = parameter_list[morphology_index]\n",
    "    rewardfunc = rewardfunc_list[rewardfunc_index]\n",
    "    \n",
    "    best_efficiency = efficiency_matrix_select[rewardfunc_index][morphology_index]\n",
    "    best_fitness = fitness_matrix[rewardfunc_index][morphology_index]\n",
    "    best_morphology = morphology\n",
    "    best_parameter = parameter\n",
    "    best_rewardfunc = rewardfunc\n",
    "    best_material = compute_cheetah_volume(parameter)\n",
    "    \n",
    "    \n",
    "    logging.info(f\"Initial morphology:{morphology}\")\n",
    "    logging.info(f\"Initial parameter:{parameter}\" )\n",
    "    logging.info(f\"Initial rewardfunc:{rewardfunc}\" )\n",
    "    logging.info(f\"Initial fitness:{best_fitness}\" )\n",
    "    logging.info(f\"Initial efficiency:{best_efficiency}\" )\n",
    "    iteration = 0\n",
    "    print(f\"Initial parameter:{parameter}\")\n",
    "    while True:\n",
    "        improved = False  # 标记是否有改进，方便控制循环\n",
    "\n",
    "        designer = DGA()\n",
    "        iteration +=1   \n",
    "         # -------- 优化 morphology --------\n",
    "        improved_morphology, improved_parameter = designer.improve_morphology(\n",
    "            best_parameter,\n",
    "            parameter_list[morphology_index],  # 这本身已经是list结构，可以保留\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],  # 👈 用 [] 包装成列表\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        print(\"improved parameter\", improved_parameter)\n",
    "        shutil.copy(improved_morphology, \"GPTCheetah.xml\")\n",
    "        shutil.copy(best_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTCheetahEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "        improved_material = compute_cheetah_volume(improved_parameter)\n",
    "        improved_efficiency = improved_fitness / improved_material\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "\n",
    "            best_fitness = improved_fitness\n",
    "            best_morphology = improved_morphology\n",
    "            best_parameter = improved_parameter\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "            iteration +=1\n",
    "            logging.info(f\"Morphology optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "\n",
    "        # -------- 没有进一步改进，跳出循环 --------\n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Morphology!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            improved = False\n",
    "            # break\n",
    "            \n",
    "        iteration +=1        \n",
    "        # -------- 优化 reward function --------\n",
    "        improved_rewardfunc = designer.improve_rewardfunc(\n",
    "            best_rewardfunc,\n",
    "            [rewardfunc_list[rewardfunc_index]],\n",
    "            [efficiency_matrix_select[rewardfunc_index, morphology_index]],\n",
    "            folder_name,\n",
    "            rewardfunc_index, \n",
    "            morphology_index,\n",
    "            iteration\n",
    "        )\n",
    "        \n",
    "        shutil.copy(best_morphology, \"GPTCheetah.xml\")\n",
    "        shutil.copy(improved_rewardfunc, \"GPTrewardfunc.py\")\n",
    "        \n",
    "        import GPTrewardfunc\n",
    "        importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "        from GPTrewardfunc import _get_rew\n",
    "        GPTCheetahEnv._get_rew = _get_rew\n",
    "        try:\n",
    "            model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "            improved_fitness, _ = Eva(model_path)\n",
    "            improved_material = compute_cheetah_volume(best_parameter)\n",
    "            improved_efficiency = improved_fitness / improved_material\n",
    "            print(\"improved_fitness\", improved_fitness)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating design: {e}\")\n",
    "            continue\n",
    "\n",
    "        if improved_efficiency > best_efficiency:\n",
    "            best_fitness = improved_fitness\n",
    "            best_rewardfunc = improved_rewardfunc\n",
    "            best_material = improved_material\n",
    "            best_efficiency = improved_efficiency\n",
    "            improved = True\n",
    "\n",
    "            logging.info(f\"Reward optimization improved iteration {iteration}: material={improved_material}, fitness={improved_fitness}, efficiency={improved_efficiency}\")\n",
    "        \n",
    "        if not improved:\n",
    "            logging.info(\"Not improved Reward!\")\n",
    "            logging.info(\"____________________________________________\")\n",
    "            break\n",
    "            \n",
    "    # 保存当前 coarse_best 的最终最优结果\n",
    "    final_optimized_results.append({\n",
    "        \"best_morphology\": best_morphology,\n",
    "        \"best_parameter\": best_parameter,\n",
    "        \"best_rewardfunc\": best_rewardfunc,\n",
    "        \"best_fitness\": best_fitness,\n",
    "        \"best_material\": best_material,\n",
    "        \"best_efficiency\": best_efficiency,\n",
    "        \"best_iteration\":iteration\n",
    "    })\n",
    "    logging.info(\"____________________________________________\")\n",
    "    logging.info(f\"Final optimized result: rewardfunc_index{rewardfunc_index} morphology_index{morphology_index}\")\n",
    "    logging.info(f\"  Morphology: {best_morphology}\")\n",
    "    logging.info(f\"  Parameter: {best_parameter}\")\n",
    "    logging.info(f\"  Rewardfunc: {best_rewardfunc}\")\n",
    "    logging.info(f\"  Fitness: {best_fitness}\")\n",
    "    logging.info(f\"  Material: {best_material}\")\n",
    "    logging.info(f\"  Efficiency: {best_efficiency}\")\n",
    "    logging.info(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.info(f\"{final_optimized_results}\")\n",
    "\n",
    "# logging.info(f\"fine optimization end: best material cost: {best_material}  fitness: {improved_fitness} merterial_efficiency: {improved_material_efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_morphology': 'results/Div_m25_r5/assets/GPTCheetah_50.xml',\n",
       "  'best_parameter': array([-0.6  ,  0.4  ,  0.5  ,  0.1  ,  0.   , -0.1  , -0.1  , -0.25 ,\n",
       "          0.03 , -0.4  , -0.2  , -0.2  , -0.1  , -0.3  ,  0.   , -0.5  ,\n",
       "          0.04 ,  0.02 ,  0.025,  0.02 ,  0.015,  0.025,  0.02 ,  0.015]),\n",
       "  'best_rewardfunc': 'results/Div_m25_r5/env/GPTrewardfunc_refine_5_50_2.py',\n",
       "  'best_fitness': 138.77250750838263,\n",
       "  'best_material': 0.008319517436858105,\n",
       "  'best_efficiency': 16680.355388592172,\n",
       "  'best_iteration': 4}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_optimized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "robodesign best 3e6 steps train\n",
      "\n",
      "fitness:135.71162111252883\n",
      "efficiency:495373.7128749287\n"
     ]
    }
   ],
   "source": [
    "# robodesign best\n",
    "parameter =  [-0.15, 0.15, 0.45, 0.05, 0.05, -0.05, -0.05, -0.1, -0.1, -0.15, -0.1, -0.05, 0.1, -0.1, 0.15, -0.15, 0.01, 0.005, 0.01, 0.007, 0.005, 0.01, 0.007, 0.005]\n",
    "xml_file = cheetah_design(parameter)  \n",
    "filename = r\"results/Div_m25_r5/assets/GPTCheetah_refine_2_17_0.xml\"\n",
    "with open(filename, \"w\") as fp:\n",
    "    fp.write(xml_file)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTCheetah_refine_2_17_0.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_refine_4_17_1.py\"\n",
    "\n",
    "morphology_index=9998\n",
    "rewardfunc_index=9998\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_cheetah_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" robodesign best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"robodesign best 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2025-04-07 02:24:55,179 - Final optimized result: rewardfunc_index2 morphology_index17\n",
    "2025-04-07 02:24:55,179 -   Morphology: results/Div_m25_r5/assets/GPTCheetah_refine_2_17_0.xml\n",
    "2025-04-07 02:24:55,179 -   Parameter: [-0.15, 0.15, 0.45, 0.05, 0.05, -0.05, -0.05, -0.1, -0.1, -0.15, -0.1, -0.05, 0.1, -0.1, 0.15, -0.15, 0.01, 0.005, 0.01, 0.007, 0.005, 0.01, 0.007, 0.005]\n",
    "2025-04-07 02:24:55,179 -   Rewardfunc: results/Div_m25_r5/env/GPTCheetah_refine_2_17_1.py\n",
    "2025-04-07 02:24:55,179 -   Fitness: 77.78216138110469\n",
    "2025-04-07 02:24:55,179 -   Material: 0.0002739580595121185\n",
    "2025-04-07 02:24:55,179 -   Efficiency: 283919.96030204033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "robodesign best 3e6 steps train\n",
      "\n",
      "fitness:123.73312278819954\n",
      "efficiency:451649.8730081208\n"
     ]
    }
   ],
   "source": [
    "# robodesign best\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTCheetah_refine_2_17_0.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTCheetah_refine_2_17_1.py\"\n",
    "\n",
    "morphology_index=999\n",
    "rewardfunc_index=999\n",
    "\n",
    "parameter = [-0.15, 0.15, 0.45, 0.05, 0.05, -0.05, -0.05, -0.1, -0.1, -0.15, -0.1, -0.05, 0.1, -0.1, 0.15, -0.15, 0.01, 0.005, 0.01, 0.007, 0.005, 0.01, 0.007, 0.005]\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_cheetah_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" robodesign best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"robodesign best 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 00:50:01,521 - Initial morphology:results/Div_m25_r5/assets/GPTCheetah_17.xml\n",
    "2025-04-07 00:50:01,522 - Initial parameter:[-0.15   0.15   0.45   0.05   0.05  -0.05  -0.05  -0.1   -0.1   -0.15\n",
    " -0.1   -0.05   0.1   -0.1    0.15  -0.15   0.01   0.005  0.01   0.007\n",
    "  0.005  0.01   0.007  0.005]\n",
    "2025-04-07 00:50:01,522 - Initial rewardfunc:results/Div_m25_r5/env/GPTrewardfunc_1.py\n",
    "2025-04-07 00:50:01,522 - Initial fitness:35.46140062498762\n",
    "2025-04-07 00:50:01,522 - Initial efficiency:129440.98336854727\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "robodesign coarse best 3e6 steps train\n",
      "\n",
      "fitness:95.46820595833779\n",
      "efficiency:348477.44990000833\n"
     ]
    }
   ],
   "source": [
    "# robodesign coarse best \n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTCheetah_17.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_1.py\"\n",
    "\n",
    "morphology_index=888\n",
    "rewardfunc_index=888\n",
    "\n",
    "parameter = [-0.15 ,  0.15  , 0.45  , 0.05 ,  0.05  ,-0.05 , -0.05 , -0.1 ,  -0.1 ,  -0.15,\n",
    " -0.1 ,  -0.05  , 0.1 ,  -0.1  ,  0.15 , -0.15  , 0.01  , 0.005 , 0.01 ,  0.007,\n",
    "  0.005,  0.01 ,  0.007 , 0.005]\n",
    "\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_cheetah_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" robodesign coarse best 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"robodesign coarse best 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# human\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTCheetah_50.xml\"\n",
    "rewardfunc = \"results/CheetahEureka_morphology/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=777\n",
    "rewardfunc_index=777\n",
    "\n",
    "material_list = [0.021184]\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=5e5)\n",
    "model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_1000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = material_list[0]\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" human 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"human 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-09 15:03:53,835 - morphology: 50, rewardfunc: 4, material cost: 0.008319517436858105 reward: 2049.3560760933597 fitness: 168.32404032342103 efficiency: 20232.428335048862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Morphology Design) 3e6 steps train\n",
      "\n",
      "fitness:250.14266670867656\n",
      "efficiency:11808.094161096891\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Morphology Design)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTCheetah_50.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_4.py\"\n",
    "\n",
    "morphology_index=666\n",
    "rewardfunc_index=666\n",
    "\n",
    "material_list = [0.021184]\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = material_list[0]\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" Robodesign (w/o Morphology Design) 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Morphology Design) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-06 15:46:10,818 - morphology: 17, rewardfunc: 0, material cost: 0.0002739580595121185 reward: 222.37654595342295 fitness: 24.967713123138623 efficiency: 91136.99070435333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Reward Shaping) 3e6 steps train\n",
      "\n",
      "fitness:26.528840834301896\n",
      "efficiency:96835.40933800633\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Reward Shaping)\n",
    "\n",
    "morphology = \"results/Div_m25_r5/assets/GPTCheetah_17.xml\"\n",
    "rewardfunc = \"results/Div_m25_r5/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=555\n",
    "rewardfunc_index=555\n",
    "morphology_index=17\n",
    "rewardfunc_index=0\n",
    "parameter =  [-0.15,\n",
    "  0.15,\n",
    "  0.45,\n",
    "  0.05,\n",
    "  0.05,\n",
    "  -0.05,\n",
    "  -0.05,\n",
    "  -0.1,\n",
    "  -0.1,\n",
    "  -0.15,\n",
    "  -0.1,\n",
    "  -0.05,\n",
    "  0.1,\n",
    "  -0.1,\n",
    "  0.15,\n",
    "  -0.15,\n",
    "  0.01,\n",
    "  0.005,\n",
    "  0.01,\n",
    "  0.007,\n",
    "  0.005,\n",
    "  0.01,\n",
    "  0.007,\n",
    "  0.005]\n",
    "\n",
    "\n",
    "# [-0.15 ,  0.15  , 0.45  , 0.05 ,  0.05  ,-0.05 , -0.05 , -0.1 ,  -0.1 ,  -0.15,\n",
    "#  -0.1 ,  -0.05  , 0.1 ,  -0.1  ,  0.15 , -0.15  , 0.01  , 0.005 , 0.01 ,  0.007,\n",
    "#   0.005,  0.01 ,  0.007 , 0.005]\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "# model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "model_path = f\"results/Div_m25_r5/coarse/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_500000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_cheetah_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" Robodesign (w/o Reward Shaping) 1e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Reward Shaping) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-07 15:03:56,961 - Final optimized result: rewardfunc_index4 morphology_index3\n",
    "2025-04-07 15:03:56,961 -   Morphology: results/noDiv_m25_r5/assets/GPTCheetah_refine_4_3_0.xml\n",
    "2025-04-07 15:03:56,961 -   Parameter: [-0.5, 0.5, 0.8, 0.2, 0.3, -0.4, 0.4, -0.5, 0.5, -0.6, -0.3, -0.5, 0.4, -0.6, 0.6, -0.7, 0.05, 0.04, 0.04, 0.03, 0.03, 0.04, 0.03, 0.03]\n",
    "2025-04-07 15:03:56,961 -   Rewardfunc: results/noDiv_m25_r5/env/GPTCheetah_refine_4_3_1.py\n",
    "2025-04-07 15:03:56,961 -   Fitness: 180.8154850762752\n",
    "2025-04-07 15:03:56,961 -   Material: 0.02788783835590477\n",
    "2025-04-07 15:03:56,961 -   Efficiency: 6483.667997809899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Robodesign (w/o Diversity Reflection) 3e6 steps train\n",
      "\n",
      "fitness:207.54673485196096\n",
      "efficiency:7442.195131915505\n"
     ]
    }
   ],
   "source": [
    "# Robodesign (w/o Diversity Reflection)\n",
    "\n",
    "morphology = \"results/noDiv_m25_r5/assets/GPTCheetah_refine_4_3_0.xml\"\n",
    "rewardfunc = \"results/noDiv_m25_r5/env/GPTCheetah_refine_4_3_1.py\"\n",
    "\n",
    "morphology_index=333\n",
    "rewardfunc_index=333\n",
    "\n",
    "parameter =  [-0.5, 0.5, 0.8, 0.2, 0.3, -0.4, 0.4, -0.5, 0.5, -0.6, -0.3, -0.5, 0.4, -0.6, 0.6, -0.7, 0.05, 0.04, 0.04, 0.03, 0.03, 0.04, 0.03, 0.03]\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_cheetah_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" Robodesign (w/o Diversity Reflection) 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"Robodesign (w/o Diversity Reflection) 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-08 19:41:04,459 - iteration:2, morphology: 0, rewardfunc: 5, material cost: 0.021184 reward: 2999.6270393357754 fitness: 173.16370628185507 efficiency: 8174.26861224769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      " eureka reward 3e6 steps train\n",
      "\n",
      "fitness:257.5534101158287\n",
      "efficiency:12157.921550029676\n"
     ]
    }
   ],
   "source": [
    "# eureka reward\n",
    "\n",
    "morphology = \"results/eureka/assets/GPTCheetah_0.xml\"\n",
    "rewardfunc = \"results/eureka/env/GPTrewardfunc_5_2.py\"\n",
    "\n",
    "morphology_index=222\n",
    "rewardfunc_index=222\n",
    "\n",
    "material_list = [0.021184]\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = material_list[0]\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\" eureka reward 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\" eureka reward 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025-04-08 07:55:18,132 - morphology: 15, rewardfunc: 0, material cost: 0.009341494464891529 reward: 1228.9937270418675 fitness: 78.32647964015682 efficiency: 8384.79109895467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "eureka morphology 3e6 steps train\n",
      "\n",
      "fitness:145.0851809530564\n",
      "efficiency:15531.260174518671\n"
     ]
    }
   ],
   "source": [
    "# eureka morphology\n",
    "\n",
    "morphology = \"results/CheetahEureka_morphology/assets/GPTCheetah_15.xml\"\n",
    "rewardfunc = \"results/CheetahEureka_morphology/env/GPTrewardfunc_0.py\"\n",
    "\n",
    "morphology_index=111\n",
    "rewardfunc_index=111\n",
    "\n",
    "parameter =  [-0.63,\n",
    " 0.61,\n",
    " 0.94,\n",
    " 0.51,\n",
    " 0.41,\n",
    " -0.71,\n",
    " 0.65,\n",
    " -1.1,\n",
    " 0.81,\n",
    " -1.5,\n",
    " -0.56,\n",
    " -0.6,\n",
    " -0.68,\n",
    " -1.2,\n",
    " 0.53,\n",
    " -1.6,\n",
    " 0.029,\n",
    " 0.02,\n",
    " 0.018,\n",
    " 0.013,\n",
    " 0.012,\n",
    " 0.018,\n",
    " 0.012,\n",
    " 0.012]\n",
    "\n",
    "shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "shutil.copy(rewardfunc, \"GPTrewardfunc.py\")\n",
    "\n",
    "import GPTrewardfunc\n",
    "importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "from GPTrewardfunc import _get_rew\n",
    "GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "model_path = Train(morphology_index, rewardfunc_index, folder_name, stage='fine', total_timesteps=1e6)\n",
    "# model_path = f\"results/Div_m25_r5/fine/SAC_morphology{morphology_index}_rewardfunc{rewardfunc_index}_3000000.0steps\"\n",
    "fitness, _ = Eva(model_path)\n",
    "material = compute_cheetah_volume(parameter)\n",
    "efficiency = fitness / material\n",
    "\n",
    "logging.info(\"eureka morphology 3e6 steps train\\n\")\n",
    "logging.info(f\"fitness:{fitness}\")\n",
    "logging.info(f\"efficiency:{efficiency}\")\n",
    "print(\"eureka morphology 3e6 steps train\\n\")\n",
    "print(f\"fitness:{fitness}\")\n",
    "print(f\"efficiency:{efficiency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
