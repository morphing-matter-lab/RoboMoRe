{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from design import *\n",
    "import importlib\n",
    "import shutil\n",
    "from utils import *\n",
    "from openai import OpenAI\n",
    "from prompts import *\n",
    "import json\n",
    "import numpy as np\n",
    "from gymnasium.envs.robodesign.GPTCheetah import GPTCheetahEnv\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_logs_full(log_dir):\n",
    "    # 找到 .tfevents 文件\n",
    "    event_file = [f for f in os.listdir(log_dir) if f.startswith(\"events.out\")][0]\n",
    "    event_path = os.path.join(log_dir, event_file)\n",
    "\n",
    "    # 加载日志\n",
    "    event_acc = EventAccumulator(event_path)\n",
    "    event_acc.Reload()\n",
    "\n",
    "    # 获取所有 scalar tags\n",
    "    all_tags = event_acc.Tags()[\"scalars\"]\n",
    "\n",
    "    # 筛选 reward 分量\n",
    "    reward_tags = [tag for tag in all_tags if tag.startswith(\"reward/\")]\n",
    "\n",
    "    # 加载所有 reward 分量数据（完整）\n",
    "    data_full = {}\n",
    "    for tag in reward_tags:\n",
    "        events = event_acc.Scalars(tag)\n",
    "        values = [e.value for e in events]\n",
    "        data_full[tag] = values\n",
    "\n",
    "    # 加载 episode length（完整）\n",
    "    ep_len_tag = \"rollout/ep_len_mean\"\n",
    "    if ep_len_tag in all_tags:\n",
    "        events = event_acc.Scalars(ep_len_tag)\n",
    "        values = [e.value for e in events]\n",
    "        data_full[ep_len_tag] = values\n",
    "\n",
    "    return data_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import prompts\n",
    "class DGA:\n",
    "    def __init__(self):\n",
    "        api_key = \"<api_key>\"\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        match = re.search(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def indent_code(self, code):\n",
    "        return \"\\n\".join(line if line.strip() else line for line in code.split(\"\\n\"))\n",
    "\n",
    "    def generate_rewardfunc_eureka(self, rewardfunc_nums, best_message, iteration, folder_name):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a reinforcement learning reward function designer\"},\n",
    "            {\"role\": \"user\", \"content\": rewardfunc_prompts + best_message + zeroshot_rewardfunc_format}\n",
    "        ]\n",
    "\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages, n=rewardfunc_nums\n",
    "        )\n",
    "        \n",
    "        files = []\n",
    "        for i, choice in enumerate(responses.choices):\n",
    "            reward_code = self.extract_code(choice.message.content)\n",
    "            if reward_code:\n",
    "                full_code = \"import numpy as np \\n\" + self.indent_code(reward_code) + \"\\n\"\n",
    "                file_name =  f\"GPTrewardfunc_{i}_{iteration}.py\"\n",
    "                file_path = os.path.join(folder_name, \"env\", file_name)\n",
    "\n",
    "                with open(file_path, \"w\") as fp:\n",
    "                    fp.write(full_code)\n",
    "                files.append(file_path)\n",
    "                print(f\"Saved: {file_path}\")\n",
    "\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"results/eureka\"\n",
    "log_file = os.path.join(folder_name, \"parameters.log\")\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# folder_name = setup_logging(div_flag=True)\n",
    "\n",
    "best_fitness = float('-inf')  \n",
    "best_morphology = None  \n",
    "best_rewardfunc = None  \n",
    "best_reward = None\n",
    "best_material = None\n",
    "best_efficiency = None\n",
    "\n",
    "iterations = 5\n",
    "morphology_nums = 1\n",
    "rewardfunc_nums = 16\n",
    "\n",
    "fitness_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "efficiency_matrix = np.array([[None for _ in range(morphology_nums)] for _ in range(rewardfunc_nums)])\n",
    "fitness_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "morphology_list = [f'results/eureka/assets/GPTCheetah_{i}.xml' for i in range(0,1) ]\n",
    "material_list = [0.021184]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eureka_rewardfunc_prompts = \"\"\"We trained a RL policy using the provided function code and tracked\n",
    "the values of the individual components in the reward function as well as global policy matrics such as fitness function and episode lengths after\n",
    "10000 epochs and the maximum, mean, and minimum values encountered:\n",
    "{reward_reflection}\n",
    "\n",
    "Please carefully analyze the policy feedback and provide a new, improved reward function that can better\n",
    "solve the task. Some helpful tips for analyzing the policy feedback:\n",
    "\n",
    "(1) If the fitness function rates are always zero or negative, then you must rewrite the entire reward function\n",
    "(2) If the values for a certain reward component are near identiacal throughout, then this means RL is not able to optimize this component as it is written. You may \n",
    "consider \n",
    "    (a) Changing its scale or the value of its temperature parameter\n",
    "    (b) Re-writing the reward component\n",
    "    (c) Discarding the reward component\n",
    "(3) If some reward components' magnitude is significanly larger, then you must rescale its value to a proper range\n",
    "Please analyze each existing reward component in the suggested manner above first, and then write the reward function code\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "designer = DGA()\n",
    "best_message = ''\n",
    "\n",
    "for iter in range(1, iterations+1):\n",
    "    reward_reflection = ''\n",
    "    rewardfunc_list = designer.generate_rewardfunc_eureka(rewardfunc_nums, best_message, iter, folder_name)\n",
    "    logging.info(f\"___________________coarse optimization iter{iter}_____________________\")\n",
    "    for i, rewardfunc in enumerate(rewardfunc_list):\n",
    "        for j, morphology in enumerate(morphology_list):\n",
    "\n",
    "            print(i, rewardfunc)\n",
    "            print(j, morphology)\n",
    "\n",
    "            shutil.copy(morphology, \"GPTCheetah.xml\")\n",
    "            shutil.copy(rewardfunc, \"GPTrewardfunc.py\")         \n",
    "\n",
    "            import GPTrewardfunc\n",
    "            importlib.reload(GPTrewardfunc)  # 重新加载模块\n",
    "            from GPTrewardfunc import _get_rew\n",
    "            GPTCheetahEnv._get_rew = _get_rew\n",
    "\n",
    "            env_name = \"GPTCheetahEnv\"\n",
    "            model_path = Train(j,  i, folder_name, total_timesteps=5e5, callback=True, iter=iter)\n",
    "            fitness, reward = Eva(model_path,run_steps=100)\n",
    "\n",
    "            material = material_list[j]\n",
    "            efficiency = fitness/material\n",
    "            fitness_matrix[i][j] = fitness\n",
    "            efficiency_matrix[i][j] = efficiency\n",
    "\n",
    "            logging.info(f\"iteration:{iter}, morphology: {j}, rewardfunc: {i}, material cost: {material} reward: {reward} fitness: {fitness} efficiency: {efficiency}\")\n",
    "\n",
    "    best_efficiency = np.max(efficiency_matrix[:, 0])\n",
    "    best_rewardfunc_index = np.argmax(efficiency_matrix[:, 0])\n",
    "    logs_full = load_logs_full(f\"results/eureka/sac_morphology0_rewardfunc{best_rewardfunc_index}_{iter}/SAC_1\")  \n",
    "    epoch_freq = 100000\n",
    "\n",
    "    for tag, full_values in logs_full.items():\n",
    "        sampled_values = full_values[::epoch_freq]\n",
    "        max_val = max(sampled_values)\n",
    "        mean_val = sum(sampled_values) / len(sampled_values)\n",
    "        min_val = min(sampled_values)\n",
    "        formatted_values = [f\"{v:.2f}\" for v in sampled_values]\n",
    "        reward_reflection +=f\"{tag}: {formatted_values}\" + f\"Max: {max_val:.2f}, Mean: {mean_val:.2f}, Min: {min_val:.2f}\\n\"\n",
    "\n",
    "    with open(best_rewardfunc, 'r') as f:\n",
    "        reward_content = f.read()\n",
    "    best_message += f\"best rewardfunc:{reward_content} \\n\" + f\"best fintess:{best_efficiency}\" + eureka_rewardfunc_prompts.format(reward_reflection=reward_reflection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficiency_matrix = np.array([[3976.6642818633827],\n",
    "       [4751.281561784006],\n",
    "       [7224.448128240247],\n",
    "       [6023.381126781108],\n",
    "       [5764.19702099186],\n",
    "       [6671.181033086907],\n",
    "       [6587.733410099273],\n",
    "       [5417.426909189873],\n",
    "       [6372.382335747838],\n",
    "       [4255.325796926133],\n",
    "       [5247.62963314609],\n",
    "       [5334.098274428634],\n",
    "       [4565.410237548588],\n",
    "       [5198.3884879938605],\n",
    "       [6604.340892256646],\n",
    "       [4453.497254902149]], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    best_rewardfunc_index = np.argmax(efficiency_matrix[:, 0])\n",
    "    best_rewardfunc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_reflection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m min_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(sampled_values)\n\u001b[1;32m     10\u001b[0m formatted_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m sampled_values]\n\u001b[0;32m---> 11\u001b[0m reward_reflection \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reward_reflection' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "logs_full = load_logs_full(f\"results/eureka/sac_morphology0_rewardfunc7/SAC_1\")  \n",
    "# logs_full = load_logs_full(f\"results\\eureka\\\")  \n",
    "epoch_freq = 100000\n",
    "\n",
    "for tag, full_values in logs_full.items():\n",
    "    sampled_values = full_values[::epoch_freq]\n",
    "    max_val = max(sampled_values)\n",
    "    mean_val = sum(sampled_values) / len(sampled_values)\n",
    "    min_val = min(sampled_values)\n",
    "    formatted_values = [f\"{v:.2f}\" for v in sampled_values]\n",
    "    reward_reflection +=f\"{tag}: {formatted_values}\" + f\"Max: {max_val:.2f}, Mean: {mean_val:.2f}, Min: {min_val:.2f}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/eureka\n",
      "results/eureka/assets\n",
      "results/eureka/assets/.ipynb_checkpoints\n",
      "results/eureka/.ipynb_checkpoints\n",
      "results/eureka/env\n",
      "results/eureka/env/.ipynb_checkpoints\n",
      "results/eureka/sac_morphology0_rewardfunc0\n",
      "results/eureka/sac_morphology0_rewardfunc0/SAC_1\n",
      "results/eureka/coarse\n",
      "results/eureka/coarse/.ipynb_checkpoints\n",
      "results/eureka/sac_morphology0_rewardfunc1\n",
      "results/eureka/sac_morphology0_rewardfunc1/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc2\n",
      "results/eureka/sac_morphology0_rewardfunc2/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc3\n",
      "results/eureka/sac_morphology0_rewardfunc3/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc4\n",
      "results/eureka/sac_morphology0_rewardfunc4/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc5\n",
      "results/eureka/sac_morphology0_rewardfunc5/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc6\n",
      "results/eureka/sac_morphology0_rewardfunc6/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc7\n",
      "results/eureka/sac_morphology0_rewardfunc7/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc10\n",
      "results/eureka/sac_morphology0_rewardfunc10/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc11\n",
      "results/eureka/sac_morphology0_rewardfunc11/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc12\n",
      "results/eureka/sac_morphology0_rewardfunc12/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc13\n",
      "results/eureka/sac_morphology0_rewardfunc13/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc14\n",
      "results/eureka/sac_morphology0_rewardfunc14/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc15\n",
      "results/eureka/sac_morphology0_rewardfunc15/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc0_1\n",
      "results/eureka/sac_morphology0_rewardfunc0_1/SAC_1\n",
      "results/eureka/sac_morphology0_rewardfunc0_1/SAC_2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodesign",
   "language": "python",
   "name": "robodesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
